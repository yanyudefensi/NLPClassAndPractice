{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyltp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-b1c12e45c605>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyltp\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSegmentor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPostagger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mParser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNamedEntityRecognizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyltp'"
     ]
    }
   ],
   "source": [
    "from pyltp import Segmentor, Postagger, Parser, NamedEntityRecognizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 分词使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_splitter(sentence):\n",
    "    \"\"\"\n",
    "    分词\n",
    "    :param sentence:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    segmentor = Segmentor()  # 初始化实例\n",
    "    segmentor.load(r\"D:\\MyNLP\\ltp_data_v3.4.0\\cws.model\")  # 加载模型\n",
    "    words = segmentor.segment(sentence)  # 分词\n",
    "    words_list = list(words)\n",
    "    print(\"分词结果：\")\n",
    "    print(words)\n",
    "    for word in words_list:\n",
    "        print(word)\n",
    "    segmentor.release()  # 释放模型\n",
    "    return words_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 词性标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_tag(words):\n",
    "    \"\"\"\n",
    "    词性标注\n",
    "    :param words: 已切分好的词\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    postagger = Postagger()  # 初始化实例\n",
    "    postagger.load(r\"D:\\MyNLP\\ltp_data_v3.4.0\\pos.model\")  # 加载模型\n",
    "    postags = postagger.postag(words)  # 词性标注\n",
    "    print(\"词性标注结果：\")\n",
    "    for word, tag in zip(words, postags):\n",
    "        print(word+':'+tag)\n",
    "    postagger.release()  # 释放模型\n",
    "    return postags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 依存句法分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(words, postags):\n",
    "    \"\"\"\n",
    "    依存句法分析\n",
    "    :param words:\n",
    "    :param postags:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    parser = Parser()  # 初始化实例\n",
    "    parser.load(r\"D:\\MyNLP\\ltp_data_v3.4.0\\parser.model\")  # 加载模型\n",
    "    arcs = parser.parse(words, postags)  # 句法分析\n",
    "    print(\"句法分析结果：\")\n",
    "    print(\"\\t\".join(\"%d:%s\" % (arc.head, arc.relation) for arc in arcs))\n",
    "    parser.release()  # 释放模型\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词结果：\n",
      "<pyltp.VectorOfString object at 0x0000017C95F95F30>\n",
      "我\n",
      "研究\n",
      "的\n",
      "方向\n",
      "是\n",
      "自然\n",
      "语言\n",
      "处理\n",
      "，\n",
      "所以\n",
      "想\n",
      "试试\n",
      "这个\n",
      "模型\n",
      "。\n",
      "词性标注结果：\n",
      "我:r\n",
      "研究:v\n",
      "的:u\n",
      "方向:n\n",
      "是:v\n",
      "自然:n\n",
      "语言:n\n",
      "处理:v\n",
      "，:wp\n",
      "所以:c\n",
      "想:v\n",
      "试试:v\n",
      "这个:r\n",
      "模型:n\n",
      "。:wp\n",
      "句法分析结果：\n",
      "2:SBV\t4:ATT\t2:RAD\t5:SBV\t0:HED\t7:ATT\t8:ATT\t5:VOB\t5:WP\t11:ADV\t5:COO\t11:VOB\t14:ATT\t12:VOB\t5:WP\n"
     ]
    }
   ],
   "source": [
    "words = word_splitter('我研究的方向是自然语言处理，所以想试试这个模型。')\n",
    "tags = word_tag(words)\n",
    "parse(words, tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三元组抽取函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pyltp import Segmentor, Postagger, Parser, NamedEntityRecognizer,SentenceSplitter\n",
    "MODELDIR='D:/LTP/ltp_data'\n",
    "# print (\"正在加载LTP模型... ...\")\n",
    "stopwords = [line.strip() for line in open(r\"D:\\机器学习大project\\nlp部分\\主题模型\\stopwords.txt\", 'r',encoding='utf-8').readlines()]\n",
    "segmentor = Segmentor()\n",
    "segmentor.load(os.path.join(MODELDIR, r\"D:\\MyNLP\\ltp_data_v3.4.0\\cws.model\"))\n",
    "\n",
    "postagger = Postagger()\n",
    "postagger.load(os.path.join(MODELDIR, r\"D:\\MyNLP\\ltp_data_v3.4.0\\pos.model\"))\n",
    "\n",
    "parser = Parser()\n",
    "parser.load(os.path.join(MODELDIR, r\"D:\\MyNLP\\ltp_data_v3.4.0\\parser.model\"))\n",
    "\n",
    "recognizer = NamedEntityRecognizer()\n",
    "recognizer.load(os.path.join(MODELDIR, r\"D:\\MyNLP\\ltp_data_v3.4.0\\ner.model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_parse_child_dict(words, postags, arcs):\n",
    "    \"\"\"\n",
    "    为句子中的每个词语维护一个保存句法依存儿子节点的字典\n",
    "    Args:\n",
    "        words: 分词列表\n",
    "        postags: 词性列表\n",
    "        arcs: 句法依存列表\n",
    "    \"\"\"\n",
    "    child_dict_list = []\n",
    "    for index in range(len(words)):\n",
    "        child_dict = dict()\n",
    "        for arc_index in range(len(arcs)):\n",
    "            if arcs[arc_index].head == index + 1:\n",
    "                if arcs[arc_index].relation in child_dict: #python3删除了has_key方法\n",
    "                    child_dict[arcs[arc_index].relation].append(arc_index)\n",
    "                else:\n",
    "                    child_dict[arcs[arc_index].relation] = []\n",
    "                    child_dict[arcs[arc_index].relation].append(arc_index)\n",
    "        #if child_dict.has_key('SBV'):\n",
    "        #    print words[index],child_dict['SBV']\n",
    "        child_dict_list.append(child_dict)\n",
    "    return child_dict_list\n",
    "\n",
    "def complete_e(words, postags, child_dict_list, word_index):\n",
    "    \"\"\"\n",
    "    完善识别的部分实体\n",
    "    \"\"\"\n",
    "    child_dict = child_dict_list[word_index]\n",
    "    prefix = ''\n",
    "    if 'ATT' in child_dict:\n",
    "        for i in range(len(child_dict['ATT'])):\n",
    "            prefix += complete_e(words, postags, child_dict_list, child_dict['ATT'][i])\n",
    "\n",
    "    postfix = ''\n",
    "    if postags[word_index] == 'v':\n",
    "        if 'VOB' in child_dict:\n",
    "            postfix += complete_e(words, postags, child_dict_list, child_dict['VOB'][0])\n",
    "        if 'SBV' in child_dict:\n",
    "            prefix = complete_e(words, postags, child_dict_list, child_dict['SBV'][0]) + prefix\n",
    "\n",
    "    return prefix + words[word_index] + postfix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fact_triple_extract(sentence):\n",
    "    \"\"\"\n",
    "    对于给定的句子进行事实三元组抽取\n",
    "    Args:\n",
    "        sentence: 要处理的语句\n",
    "    \"\"\"\n",
    "    #print (sentence)\n",
    "    words=[]\n",
    "    cuts = segmentor.segment(sentence)\n",
    "    #print (\"\\t\".join(words))\n",
    "    for word in cuts:\n",
    "        if word not in stopwords:\n",
    "            words.append(word)\n",
    "    postags = postagger.postag(words)\n",
    "    netags = recognizer.recognize(words, postags)\n",
    "    arcs = parser.parse(words, postags)\n",
    "    #print (\"\\t\".join(\"%d:%s\" % (arc.head, arc.relation) for arc in arcs))\n",
    "\n",
    "    child_dict_list = build_parse_child_dict(words, postags, arcs)\n",
    "    for index in range(len(postags)):\n",
    "        # 抽取以谓词为中心的事实三元组\n",
    "        if postags[index] == 'v':\n",
    "            child_dict = child_dict_list[index]\n",
    "            # 主谓宾\n",
    "            if 'SBV' in child_dict and 'VOB' in child_dict:\n",
    "                e1 = complete_e(words, postags, child_dict_list, child_dict['SBV'][0])\n",
    "                r = words[index]\n",
    "                e2 = complete_e(words, postags, child_dict_list, child_dict['VOB'][0])\n",
    "                return [e1, r, e2]\n",
    "\n",
    "            # 定语后置，动宾关系\n",
    "            if arcs[index].relation == 'ATT':\n",
    "                if 'VOB' in child_dict:\n",
    "                    e1 = complete_e(words, postags, child_dict_list, arcs[index].head - 1)\n",
    "                    r = words[index]\n",
    "                    e2 = complete_e(words, postags, child_dict_list, child_dict['VOB'][0])\n",
    "                    temp_string = r+e2\n",
    "                    if temp_string == e1[:len(temp_string)]:\n",
    "                        e1 = e1[len(temp_string):]\n",
    "                    if temp_string not in e1:\n",
    "                        return print(\"定语后置动宾关系\\t(%s, %s, %s)\\n\" % (e1, r, e2))\n",
    "\n",
    "            # 含有介宾关系的主谓动补关系\n",
    "            if 'SBV' in child_dict and 'CMP' in child_dict:\n",
    "                #e1 = words[child_dict['SBV'][0]]\n",
    "                e1 = complete_e(words, postags, child_dict_list, child_dict['SBV'][0])\n",
    "                cmp_index = child_dict['CMP'][0]\n",
    "                r = words[index] + words[cmp_index]\n",
    "                if 'POB' in child_dict_list[cmp_index]:\n",
    "                    e2 = complete_e(words, postags, child_dict_list, child_dict_list[cmp_index]['POB'][0])\n",
    "                    return [e1, r, e2]\n",
    "\n",
    "\n",
    "        # 尝试抽取命名实体有关的三元组\n",
    "        if netags[index][0] == 'S' or netags[index][0] == 'B':\n",
    "            ni = index\n",
    "            if netags[ni][0] == 'B':\n",
    "                while netags[ni][0] != 'E':\n",
    "                    ni += 1\n",
    "                e1 = ''.join(words[index:ni+1])\n",
    "            else:\n",
    "                e1 = words[ni]\n",
    "            if arcs[ni].relation == 'ATT' and postags[arcs[ni].head-1] == 'n' and netags[arcs[ni].head-1] == 'O':\n",
    "                r = complete_e(words, postags, child_dict_list, arcs[ni].head-1)\n",
    "                if e1 in r:\n",
    "                    r = r[(r.index(e1)+len(e1)):]\n",
    "                if arcs[arcs[ni].head-1].relation == 'ATT' and netags[arcs[arcs[ni].head-1].head-1] != 'O':\n",
    "                    e2 = complete_e(words, postags, child_dict_list, arcs[arcs[ni].head-1].head-1)\n",
    "                    mi = arcs[arcs[ni].head-1].head-1\n",
    "                    li = mi\n",
    "                    if netags[mi][0] == 'B':\n",
    "                        while netags[mi][0] != 'E':\n",
    "                            mi += 1\n",
    "                        e = ''.join(words[li+1:mi+1])\n",
    "                        e2 += e\n",
    "                    if r in e2:\n",
    "                        e2 = e2[(e2.index(r)+len(r)):]\n",
    "                    if r+e2 in sentence:\n",
    "                        return [e1, r, e2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(fact_triple_extract('我真的超级喜欢大数据的'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
