{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution_event_id": "c1af154a-374e-4dad-9779-6c4a3474d0d2",
    "last_executed_text": "# Assignment 4",
    "persistent_id": "1cd880f4-f1a5-4a00-82f9-3773b4f68971"
   },
   "source": [
    "# Assignment 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. å¤ä¹ ä¸Šè¯¾å†…å®¹ä»¥åŠå¤ç°è¯¾ç¨‹ä»£ç "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨æœ¬éƒ¨åˆ†ï¼Œä½ éœ€è¦å¤ä¹ ä¸Šè¯¾å†…å®¹å’Œè¯¾ç¨‹ä»£ç åï¼Œè‡ªå·±å¤ç°è¯¾ç¨‹ä»£ç ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. å›ç­”ä¸€ä¸‹ç†è®ºé¢˜ç›®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What does a neuron compute?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸€ä¸ªç¥ç»å…ƒè®¡ç®—ç¤ºä¾‹ï¼š\n",
    "\n",
    "è¾“å…¥ä¸º[x1,x2,x3...],è¿›è¡ŒåŠ å’Œè®¡ç®—ï¼Œå¾—åˆ°[W1X1+B1,W2X2+B2,...](å®é™…æ˜¯ç›´æ¥è¿›è¡ŒçŸ©é˜µè¿ç®—ï¼‰\n",
    "\n",
    "å°†æ‰€å¾—listæ±‚å’Œï¼Œå¸¦å…¥åˆ°æ¿€æ´»å‡½æ•°ğœ(x)è®¡ç®—ï¼ˆéçº¿æ€§å˜æ¢ï¼‰ï¼Œå¹¶è¾“å‡ºå€¼ã€‚\n",
    "\n",
    "ğ‘§=ğ‘¤^ğ‘‡ ğ‘¥+ğ‘\n",
    "ğ‘=ğœ(ğ‘§)\n",
    "ğ‘å³ä¸ºè¾“å‡ºå€¼                                            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  2. Why we use non-linear activation funcitons in neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "éçº¿æ€§æ˜ å°„ä½¿å¾—éšè—å±‚çš„è®¾ç½®æœ‰æ„ä¹‰ï¼š\n",
    "\n",
    "ç»™å‡ºåŒå±‚çº¿æ€§æ˜ å°„ç¥ç»ç½‘ç»œçš„caseï¼š\n",
    "\n",
    "1å±‚ï¼šnode1 node2\n",
    "\n",
    "2å±‚ï¼šnode3\n",
    "\n",
    "node3çš„è¾“å‡º = ğœ(W1X1+B1 + W2X2+B2) = W3(W1X1+B1 + W2X2+B2) + B3 = (W1+W2)W3 = W3W1X1 + W2W3X2 + (W3B1+W3B2+B3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è€Œä¸€å±‚çš„ç¥ç»ç½‘ç»œè¾“å‡ºä¸ºï¼š\n",
    "\n",
    "1å±‚ï¼šnode1 node2\n",
    "\n",
    "è¾“å‡º = W4X1+W5X2 + B4ï¼Œè‹¥W4 = W3W1ï¼ŒW5 = W2W3ï¼ŒB4 = W3B1+W3B2+B3,åˆ™å¯è§çº¿æ€§æ˜ å°„çš„éšè—å±‚æ²¡æœ‰æ„ä¹‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is the 'Logistic Loss' ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Lossæ˜¯äº¤å‰ç†µæŸå¤±åœ¨äºŒåˆ†ç±»é—®é¢˜ä¸‹çš„åº”ç”¨ï¼ŒLogistic Loss = â€”1/m[( âˆ‘ y * log(y)) + ( âˆ‘ (1-y) * log(1-y))],(è®¾æœ‰mä¸ªæ ·æœ¬)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Assume that you are building a binary classifier for detecting if an image containing cats, which activation functions would you recommen using for the output layer ?\n",
    "\n",
    "A. ReLU    \n",
    "B. Leaky ReLU    \n",
    "C. sigmoid    \n",
    "D. tanh  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é€‰Cï¼Œsigmoidå‡½æ•°å¯ä»¥æŠŠä¸Šä¸€å±‚ç¥ç»å…ƒçš„ç»“æœå½’ä¸€åŒ–åˆ°[0,1]åŒºé—´ï¼Œä½¿å¾—åœ¨äºŒåˆ†ç±»é—®é¢˜ä¸­å…·å¤‡è§£é‡Šæ€§æ„ä¹‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Why we don't use zero initialization for all parameters ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœéƒ½ä½¿ç”¨0åˆå§‹åŒ–ï¼Œä¼šå¯¼è‡´ç¥ç»å…ƒä¹‹é—´æ²¡æœ‰å·®å¼‚æ€§ï¼ˆå­¦ä¹ åˆ°çš„ä¸œè¥¿éƒ½æ˜¯ä¸€æ ·çš„ï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Can you implement the softmax function using python ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution_event_id": "a86c8f49-dc42-4094-8374-2f3f79e03810",
    "last_executed_text": "def softmax(X):\n    #softmaxå‡½æ•°\n    return np.exp(X) / np.sum(np.exp(X))\n",
    "persistent_id": "1eed7f4a-251f-46b5-b994-cbed2f5d5ede"
   },
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    #softmaxå‡½æ•°\n",
    "    return np.exp(X) / np.sum(np.exp(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "persistent_id": "cb412d37-1125-489d-868b-e5ea473daee1"
   },
   "source": [
    "### 3.å®è·µé¢˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this practical part, you will build a simple digits recognizer to check if the digit in the image is larger than 5. This assignmnet will guide you step by step to finish your first small project in this course ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1 - Packages  \n",
    "sklearn is a famous package for machine learning.   \n",
    "matplotlib is a common package for vasualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution_event_id": "474a3e21-0134-4878-9919-d6806dabd5c7",
    "last_executed_text": "from sklearn import datasets\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split",
    "persistent_id": "cd5e8289-6c99-4263-a0b9-737ee1fa08e5"
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - Overvie of the dataset  \n",
    "    - a training set has m_train images labeled as 0 if the digit < 5 or 1 if the digit >= 5\n",
    "    - a test set contains m_test images labels as if the digit < 5 or 1 if the digit >= 5\n",
    "    - eah image if of shape (num_px, num_px ). Thus, each image is square(height=num_px and  width = num_px)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution_event_id": "b431a7c8-6961-4574-9993-a868b003911b",
    "last_executed_text": "# Loading the data \ndigits = datasets.load_digits()",
    "persistent_id": "73e4c7a0-2a9c-4d42-857b-03bc36289cc1"
   },
   "outputs": [],
   "source": [
    "# Loading the data \n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution_event_id": "776cbf11-d3e6-4a50-a246-e44f364368d5",
    "last_executed_text": "# Vilizating the data\nfor i in range(1,11):\n    plt.subplot(2,5,i)\n    plt.imshow(digits.data[i-1].reshape([8,8]),cmap=plt.cm.gray_r)\n    plt.text(3,10,str(digits.target[i-1]))\n    plt.xticks([])\n    plt.yticks([])\nplt.show()",
    "persistent_id": "40b6af44-7b70-474f-8251-cb9fb55b9508"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Vilizating the data\n",
    "for i in range(1,11):\n",
    "    plt.subplot(2,5,i)\n",
    "    plt.imshow(digits.data[i-1].reshape([8,8]),cmap=plt.cm.gray_r)\n",
    "    plt.text(3,10,str(digits.target[i-1]))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution_event_id": "e7c89502-d908-486f-bf31-e0d9c14ba394",
    "last_executed_text": "# Split the data into training set and test set \nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.2)",
    "persistent_id": "4c5120f7-b102-4340-ba26-e02a666ab9f6"
   },
   "outputs": [],
   "source": [
    "# Split the data into training set and test set \n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution_event_id": "cb32099c-ee3d-479e-863a-493718f387d3",
    "last_executed_text": "# reformulate the label. \n# If the digit is smaller than 5, the label is 0.\n# If the digit is larger than 5, the label is 1.\n\ny_train[y_train < 5 ] = 0\ny_train[y_train >= 5] = 1\ny_test[y_test < 5] = 0\ny_test[y_test >= 5] = 1",
    "persistent_id": "e2580523-6d94-469d-9bde-b795e40ef9dd"
   },
   "outputs": [],
   "source": [
    "# reformulate the label. \n",
    "# If the digit is smaller than 5, the label is 0.\n",
    "# If the digit is larger than 5, the label is 1.\n",
    "\n",
    "y_train[y_train < 5 ] = 0\n",
    "y_train[y_train >= 5] = 1\n",
    "y_test[y_test < 5] = 0\n",
    "y_test[y_test >= 5] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution_event_id": "e4db12a3-f1aa-4f4b-859d-541b53deb2de",
    "last_executed_text": "print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)",
    "persistent_id": "6d8bcceb-9c81-4f4f-b84a-fb06e0846465"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1437, 64)\n",
      "(360, 64)\n",
      "(1437,)\n",
      "(360,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "persistent_id": "60a980a6-cc74-474b-8028-9a79b7d90547"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 1437)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = X_train.reshape(-1,X_train.shape[0])\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3- Architecture of the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./networks.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution_event_id": "42b9604e-ff35-4048-aa5a-9d709f8fbe23",
    "last_executed_text": "!pwd",
    "persistent_id": "d9c890db-0639-486e-b2f8-a30e2f781a33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/junjiexie/NLPå­¦ä¹ \n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mathematical expression of the algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For one example $x^{(i)}$:   \n",
    " $$ z^{(i)} = w^T * x^{(i)} +b $$   \n",
    " $$ y^{(i)} = a^{(i)} = sigmoid(z^{(i)})$$   \n",
    " $$L(a^{(i)},y^{(i)}) = -y^{(i)} log(a^{(i)})-(1-y^{(i)})log(1-a^{(i)})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total cost over all training examples:\n",
    "$$ J = \\frac{1}{m}\\sum_{i=1}^{m}L(a^{(i)},y^{(i)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4 - Building the algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1- Activation function    \n",
    "###### Exercise:\n",
    "Finish the sigmoid funciton "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution_event_id": "7ad85af6-aeeb-4d36-98bf-3dc4588bd19a",
    "last_executed_text": "import numpy as np\ndef sigmoid(z):\n    '''\n    Compute the sigmoid of z\n    Arguments: z -- a scalar or numpy array of any size.\n    \n    Return:\n    s -- sigmoid(z)\n    '''\n    s = 1./(1 + np.exp(-1 * z))\n    \n    return s",
    "persistent_id": "cb15bbde-4a6d-461a-ae73-8bc399b9d35b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sigmoid(z):\n",
    "    '''\n",
    "    Compute the sigmoid of z\n",
    "    Arguments: z -- a scalar or numpy array of any size.\n",
    "    \n",
    "    Return:\n",
    "    s -- sigmoid(z)\n",
    "    '''\n",
    "    s = 1./(1 + np.exp(-1 * z))\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution_event_id": "9dee9b8d-ff98-4532-aa6e-96d4bc6d2615",
    "last_executed_text": "# Test your code \n# The result should be [0.5 0.88079708]\nprint(\"sigmoid([0,2]) = \" + str(sigmoid(np.array([0,2]))))",
    "persistent_id": "e67aab82-7c95-494e-ac25-7c6832e9691a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid([0,2]) = [0.5        0.88079708]\n"
     ]
    }
   ],
   "source": [
    "# Test your code \n",
    "# The result should be [0.5 0.88079708]\n",
    "print(\"sigmoid([0,2]) = \" + str(sigmoid(np.array([0,2]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1-Initializaing parameters\n",
    "###### Exercise:\n",
    "Finishe the initialize_parameters function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution_event_id": "f8643ac5-fe74-4fa0-a2e9-5897ce36216e",
    "last_executed_text": "# Random innitialize the parameters\n\ndef initialize_parameters(dim):\n    '''\n    Argument: dim -- size of the w vector\n    \n    Returns:\n    w -- initialized vector of shape (dim,1)\n    b -- initializaed scalar\n    '''\n    \n    w = np.random.randn(dim,1)\n    b = 0\n    \n    assert(w.shape == (dim,1))\n    assert(isinstance(b,float) or isinstance(b,int))\n    \n    return w,b",
    "persistent_id": "a95a0c56-ed75-4037-b248-43adcba4e53d"
   },
   "outputs": [],
   "source": [
    "# Random innitialize the parameters\n",
    "\n",
    "def initialize_parameters(dim):\n",
    "    '''\n",
    "    Argument: dim -- size of the w vector\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim,1)\n",
    "    b -- initializaed scalar\n",
    "    '''\n",
    "    \n",
    "    w = np.random.randn(dim,1)\n",
    "    b = 0\n",
    "    \n",
    "    assert(w.shape == (dim,1))\n",
    "    assert(isinstance(b,float) or isinstance(b,int))\n",
    "    \n",
    "    return w,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3-Forward and backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Some mathematical expressions\n",
    "Forward Propagation:   \n",
    ". X    \n",
    ". A = $\\sigma(w^T*X+b) = (a^{(1)},a^{(2)},...,a^{(m)}$   \n",
    ". J = $-\\frac{1}{m} \\sum_{i=1}^{m}y^{(i)}log(a^{(i)}+(1-y^{(i)})log(1-a^{(i)})$       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some derivative: \n",
    "$$\\frac{\\partial{J}}{\\partial{w}} = \\frac{1}{m}X*(A-Y)^T$$   \n",
    "$$\\frac{\\partial{J}}{\\partial{b}} = \\frac{1}{m}\\sum_{i=1}^m(a^{(i)}-y^{(i)}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Exercise:\n",
    "Finish the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution_event_id": "510cd95c-147a-49d0-9087-b414a6cd8fc9",
    "last_executed_text": "def propagate(w,b,X,Y):\n    '''\n    Implement the cost function and its gradient for the propagation\n    \n    Arguments:\n    w - weights\n    b - bias\n    X - data\n    Y - ground truth\n    '''\n    m = X.shape[0]\n    A = sigmoid(np.dot(w.T,X) + b)\n    cost = -1/m * np.sum(Y * np.log(A + 1e-5) + (1-Y) * np.log(1-A + 1e-5)) #æŸå¤±å‡½æ•°è¿™é‡Œæœ€å¥½åŠ ä¸€ä¸ªå°æ•°ï¼Œé˜²æ­¢é™¤0\n#     print(cost)\n    \n    dw = 1/m * np.dot(X,(A-Y).T)\n    db = 1/m * np.sum(A-Y)\n    \n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    cost = np.squeeze(cost)\n    assert(cost.shape == ())\n    \n    grads = {'dw':dw,\n             'db':db}\n    return grads, cost",
    "persistent_id": "d2518a89-1eeb-44fb-95a8-4eab59019f67"
   },
   "outputs": [],
   "source": [
    "def propagate(w,b,X,Y):\n",
    "    '''\n",
    "    Implement the cost function and its gradient for the propagation\n",
    "    \n",
    "    Arguments:\n",
    "    w - weights\n",
    "    b - biaA\n",
    "    X - data\n",
    "    Y - ground truth\n",
    "    '''\n",
    "    m = X.shape[1]\n",
    "    A = sigmoid(np.dot(w.T,X) + b)\n",
    "    cost = -1/m * np.sum(Y * np.log(A) + (1-Y) * np.log(1-A)) #æŸå¤±å‡½æ•°è¿™é‡Œæœ€å¥½åŠ ä¸€ä¸ªå°æ•°ï¼Œé˜²æ­¢é™¤0\n",
    "#     print(cost)\n",
    "\n",
    "    \n",
    "    dw = 1/m * np.dot(X,(A-Y).T)\n",
    "    db = 1/m * np.sum(A-Y)\n",
    "    \n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {'dw':dw,\n",
    "             'db':db}\n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.4 -Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Exercise:\n",
    "Minimizing the cost function using gradient descent.   \n",
    "$$\\theta = \\theta - \\alpha*d\\theta$$ where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution_event_id": "fc3058cb-bc2b-47ef-874c-ca94ce974d4f",
    "last_executed_text": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost=True):\n    '''\n    This function optimize w and b by running a gradient descen algorithm\n    \n    Arguments:\n    w - weights\n    b - bias\n    X - data\n    Y - ground truth\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n    \n    Returns:\n    params - dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n    \n    '''\n    \n    costs = []\n    \n    for i in range(num_iterations):\n        \n        grads, cost = propagate(w,b,X,Y)\n        \n        dw = grads['dw']\n        db = grads['db']\n        \n        w = w - learning_rate * dw\n        b = b - learning_rate * db\n        \n        if i % 100 == 0:\n            costs.append(cost)\n        if print_cost and i % 500 == 0:\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    \n    params = {\"w\":w,\n              \"b\":b}\n    \n    grads = {\"dw\":dw,\n             \"db\":db}\n    \n    return params, grads, costs",
    "persistent_id": "39737492-b457-4dcf-8d9f-76a392488151"
   },
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost=True):\n",
    "    '''\n",
    "    This function optimize w and b by running a gradient descen algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w - weights\n",
    "    b - bias\n",
    "    X - data\n",
    "    Y - ground truth\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params - dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        grads, cost = propagate(w,b,X,Y)\n",
    "        \n",
    "        dw = grads['dw']\n",
    "        db = grads['db']\n",
    "        \n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        if print_cost and i % 500 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\":w,\n",
    "              \"b\":b}\n",
    "    \n",
    "    grads = {\"dw\":dw,\n",
    "             \"db\":db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Exercise\n",
    "The previous function will output the learned w and b. We are able to use w and b to predict the labels for a dataset X. Implement the predict() function.    \n",
    "Two steps to finish this task:   \n",
    "1. Calculate $\\hat{Y} = A = \\sigma(w^T*X+b)$   \n",
    "2. Convert the entries of a into 0 (if activation <= 0.5) or 1 (if activation > 0.5), stores the predictions in a vector Y_prediction. If you wish, you can use an if/else statement in a for loop (though there is also a way to vectorize this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution_event_id": "5615da06-a351-48d3-8ed9-7ca001d87143",
    "last_executed_text": "import matplotlib.pyplot as plt",
    "persistent_id": "a7917915-adf9-4667-88b6-f01c4b8e2de3"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution_event_id": "2a33d897-b529-497b-b1fc-1baae8cab30a",
    "last_executed_text": "def predict(w, b, X):\n    '''\n    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n    \n    Arguments:\n    w -- weights\n    b -- bias \n    X -- data \n    \n    Returns:\n    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n    '''\n    m = X.shape[0]\n    Y_prediction = np.zeros((1,m))\n    w = w.reshape(X.shape[0],1)\n    \n    A = sigmoid(np.dot(w.T,X) + b)\n    \n    for j in range(A.shape[1]):\n        if A[0,j] <= 0.5:\n            Y_prediction[0,j] = 0\n        else:\n            Y_prediction[0,j] = 1\n    \n    assert(Y_prediction.shape == (1,m))\n    \n    return Y_prediction",
    "persistent_id": "afe4201d-351d-44c7-af57-927730e72d92"
   },
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights\n",
    "    b -- bias \n",
    "    X -- data \n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1,m))\n",
    "    w = w.reshape(X.shape[0],1)\n",
    "    \n",
    "    A = sigmoid(np.dot(w.T,X) + b)\n",
    "    \n",
    "    for j in range(A.shape[1]):\n",
    "        if A[0,j] > 0.5:\n",
    "            Y_prediction[0,j] = 1\n",
    "        else:\n",
    "            Y_prediction[0,j] = 0\n",
    "    \n",
    "    assert(Y_prediction.shape == (1,m))\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5- Merge all functions into a model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations !! You have finished all the necessary components for constructing a model. Now, Let's take the challenge to merge all the implemented function into one model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution_event_id": "8270f437-94c8-4399-94bf-307c5017aab1",
    "last_executed_text": "x_train = X_train.reshape(-1,X_train.shape[0])\nX_train.shape",
    "persistent_id": "0a802e33-097a-471d-9936-b407ebf51317"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 1437)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = X_train.reshape(-1,X_train.shape[0])\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "persistent_id": "949dbd20-3ae0-4db2-bc28-baee776b6ee9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "persistent_id": "0850892e-9642-4e4e-bcb2-346e0b356a4e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1437)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.reshape(-1,y_train.shape[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "persistent_id": "63f2e7a7-0f36-4aa2-b9b6-76286dda664f"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution_event_id": "4172ab83-97ef-4ec8-8272-e67a930c9636",
    "last_executed_text": "def model(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate,print_cost=True):\n    \"\"\"\n    Build the logistic regression model by calling all the functions you have implemented.\n    Arguments:\n    X_train - training set\n    Y_train - training label\n    X_test - test set\n    Y_test - test label\n    num_iteration - hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations è¿™é‡Œæˆ‘æƒ³æ”¹æˆ500ï¼Œprintå¤ªå¤šäº†\n    \n    Returns:\n    d - dictionary should contain following information w,b,training_accuracy, test_accuracy,cost\n    eg: d = {\"w\":w,\n             \"b\":b,\n             \"training_accuracy\": traing_accuracy,\n             \"test_accuracy\":test_accuracy,\n             \"cost\":cost}\n    \"\"\"\n    # æ•°æ®ä¿®æ”¹ä¸ºä¸€ç»´\n#     x_train = X_train.reshape(-1,X_train.shape[0])\n#     y_train = Y_train.reshape(-1,Y_train.shape[0])\n#     x_test = X_test.reshape(-1,X_test.shape[0])\n#     y_test = Y_test.reshape(-1,Y_test.shape[0])\n\n    x_train = X_train.flatten()\n    y_train = Y_train.flatten()\n    x_test = X_test.flatten()\n    y_test = Y_test.flatten()\n    \n    # åˆå§‹åŒ–å‚æ•°\n    w,b = initialize_parameters(x_train.shape[0])\n    \n    # params:æ›´æ–°åçš„ç½‘ç»œå‚æ•°\n    # grads:æœ€æ–°çš„æ¢¯åº¦(ä¸‹é™æŸå¤±)\n    # costs:æ¯æ¬¡æ›´æ–°çš„æŸå¤±åˆ—è¡¨\n    params, grads, costs = optimize(w,b,x_train,y_train,num_iterations,learning_rate,print_cost)\n    \n    # è·å–è®­ç»ƒçš„å‚æ•°\n    w = params[\"w\"]\n    b = params[\"b\"]\n    \n    # é¢„æµ‹ç»“æœ\n    y_prediction_train = predict(w,b,x_train)\n    y_prediction_test = predict(w,b,x_test)\n\n    \n    # æ‰“å°å‡†ç¡®ç‡\n#     print(\"training setâ€˜s accuracyï¼š{}\".format(np.sum(y_prediction_train == y_train)/ y_train.shape[1]))\n#     print(\"testing setâ€™s accuracyï¼š{}\".format(np.sum(y_prediction_test == y_test)/ y_test.shape[1]))\n    \n    # æ‰“å°å‡†ç¡®ç‡\n#     plt.plot(range(len(costs),costs))\n    print(\"è®­ç»ƒé›†å‡†ç¡®ç‡: {} \".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"æµ‹è¯•é›†å‡†ç¡®ç‡: {} \".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \n    return costs\n    \n    \n    \n",
    "persistent_id": "736561b1-27f2-4958-a474-6d34622f80bb"
   },
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate,print_cost=True):\n",
    "    \"\"\"\n",
    "    Build the logistic regression model by calling all the functions you have implemented.\n",
    "    Arguments:\n",
    "    X_train - training set\n",
    "    Y_train - training label\n",
    "    X_test - test set\n",
    "    Y_test - test label\n",
    "    num_iteration - hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations è¿™é‡Œæˆ‘æƒ³æ”¹æˆ500ï¼Œprintå¤ªå¤šäº†\n",
    "    \n",
    "    Returns:\n",
    "    d - dictionary should contain following information w,b,training_accuracy, test_accuracy,cost\n",
    "    eg: d = {\"w\":w,\n",
    "             \"b\":b,\n",
    "             \"training_accuracy\": traing_accuracy,\n",
    "             \"test_accuracy\":test_accuracy,\n",
    "             \"cost\":cost}\n",
    "    \"\"\"\n",
    "    # æ•°æ®ä¿®æ”¹ä¸ºä¸€ç»´,å‚æ•°å’Œæ¢¯åº¦å‡½æ•°èƒ½å¤Ÿæ¥å—çš„ç»´åº¦è§„æ ¼ï¼ˆ64ï¼Œsampleä¸ªæ•°ï¼‰\n",
    "    x_train = scale(X_train.reshape(-1,X_train.shape[0])) \n",
    "    y_train = Y_train.reshape(-1,Y_train.shape[0])\n",
    "    x_test = scale(X_test.reshape(-1,X_test.shape[0])) \n",
    "    y_test = Y_test.reshape(-1,Y_test.shape[0])\n",
    "\n",
    "    \n",
    "    # åˆå§‹åŒ–å‚æ•°\n",
    "    w,b = initialize_parameters(x_train.shape[0])\n",
    "    \n",
    "    # params:æ›´æ–°åçš„ç½‘ç»œå‚æ•°\n",
    "    # grads:æœ€æ–°çš„æ¢¯åº¦(ä¸‹é™æŸå¤±)\n",
    "    # costs:æ¯æ¬¡æ›´æ–°çš„æŸå¤±åˆ—è¡¨\n",
    "    params, grads, costs = optimize(w,b,x_train,y_train,num_iterations,learning_rate,print_cost)\n",
    "    \n",
    "    # è·å–è®­ç»ƒçš„å‚æ•°\n",
    "    new_w = params[\"w\"]\n",
    "    new_b = params[\"b\"]\n",
    "    \n",
    "    # é¢„æµ‹ç»“æœ\n",
    "    y_prediction_train = predict(new_w,new_b,x_train)\n",
    "    y_prediction_test = predict(new_w,new_b,x_test)\n",
    "\n",
    "    \n",
    "    # æ‰“å°å‡†ç¡®ç‡\n",
    "    print(\"training setâ€˜s accuracyï¼š{}\".format(np.sum(y_prediction_train == y_train)/ y_train.shape[1]))\n",
    "    print(\"testing setâ€™s accuracyï¼š{}\".format(np.sum(y_prediction_test == y_test)/ y_test.shape[1]))\n",
    "    \n",
    "    # æ‰“å°å‡†ç¡®ç‡\n",
    "#     plt.plot(range(len(costs),costs))\n",
    "#     print(\"è®­ç»ƒé›†å‡†ç¡®ç‡: {} \".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n",
    "#     print(\"æµ‹è¯•é›†å‡†ç¡®ç‡: {} \".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n",
    "    \n",
    "    return costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution_event_id": "9c1b6fb5-ff03-4424-a32b-09752e09a4f5",
    "last_executed_text": "costs_1 = model(X_train, y_train, X_test, y_test, num_iterations=15000, learning_rate=0.005)\ncosts_2 = model(X_train, y_train, X_test, y_test, num_iterations=15000, learning_rate=0.010)",
    "persistent_id": "b175609a-a7d0-4bf8-84b6-23b93d651481"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 4.024492\n",
      "Cost after iteration 500: 2.386687\n",
      "Cost after iteration 1000: 2.128614\n",
      "Cost after iteration 1500: 1.899825\n",
      "Cost after iteration 2000: 1.685784\n",
      "Cost after iteration 2500: 1.488767\n",
      "Cost after iteration 3000: 1.311347\n",
      "Cost after iteration 3500: 1.156076\n",
      "Cost after iteration 4000: 1.025068\n",
      "Cost after iteration 4500: 0.919464\n",
      "Cost after iteration 5000: 0.838812\n",
      "Cost after iteration 5500: 0.780695\n",
      "Cost after iteration 6000: 0.741082\n",
      "Cost after iteration 6500: 0.715313\n",
      "Cost after iteration 7000: 0.699113\n",
      "Cost after iteration 7500: 0.689140\n",
      "Cost after iteration 8000: 0.683061\n",
      "Cost after iteration 8500: 0.679364\n",
      "Cost after iteration 9000: 0.677109\n",
      "Cost after iteration 9500: 0.675727\n",
      "Cost after iteration 10000: 0.674874\n",
      "Cost after iteration 10500: 0.674345\n",
      "Cost after iteration 11000: 0.674014\n",
      "Cost after iteration 11500: 0.673806\n",
      "Cost after iteration 12000: 0.673675\n",
      "Cost after iteration 12500: 0.673591\n",
      "Cost after iteration 13000: 0.673538\n",
      "Cost after iteration 13500: 0.673504\n",
      "Cost after iteration 14000: 0.673482\n",
      "Cost after iteration 14500: 0.673468\n",
      "training setâ€˜s accuracyï¼š0.5838552540013918\n",
      "testing setâ€™s accuracyï¼š0.5194444444444445\n",
      "Cost after iteration 0: 3.882312\n",
      "Cost after iteration 500: 1.893752\n",
      "Cost after iteration 1000: 1.455163\n",
      "Cost after iteration 1500: 1.108213\n",
      "Cost after iteration 2000: 0.875531\n",
      "Cost after iteration 2500: 0.752589\n",
      "Cost after iteration 3000: 0.701459\n",
      "Cost after iteration 3500: 0.683107\n",
      "Cost after iteration 4000: 0.676815\n",
      "Cost after iteration 4500: 0.674645\n",
      "Cost after iteration 5000: 0.673880\n",
      "Cost after iteration 5500: 0.673605\n",
      "Cost after iteration 6000: 0.673503\n",
      "Cost after iteration 6500: 0.673466\n",
      "Cost after iteration 7000: 0.673451\n",
      "Cost after iteration 7500: 0.673446\n",
      "Cost after iteration 8000: 0.673444\n",
      "Cost after iteration 8500: 0.673443\n",
      "Cost after iteration 9000: 0.673442\n",
      "Cost after iteration 9500: 0.673442\n",
      "Cost after iteration 10000: 0.673442\n",
      "Cost after iteration 10500: 0.673442\n",
      "Cost after iteration 11000: 0.673442\n",
      "Cost after iteration 11500: 0.673442\n",
      "Cost after iteration 12000: 0.673442\n",
      "Cost after iteration 12500: 0.673442\n",
      "Cost after iteration 13000: 0.673442\n",
      "Cost after iteration 13500: 0.673442\n",
      "Cost after iteration 14000: 0.673442\n",
      "Cost after iteration 14500: 0.673442\n",
      "training setâ€˜s accuracyï¼š0.581767571329158\n",
      "testing setâ€™s accuracyï¼š0.5138888888888888\n"
     ]
    }
   ],
   "source": [
    "costs_1 = model(X_train, y_train, X_test, y_test, num_iterations=15000, learning_rate=0.005)\n",
    "costs_2 = model(X_train, y_train, X_test, y_test, num_iterations=15000, learning_rate=0.010)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åˆ©ç”¨Kerasåº“éªŒè¯ä¸‹é€»è¾‘å›å½’æ­£å¸¸æ­£ç¡®ç‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "persistent_id": "977f0bf5-d268-4cc6-acd8-3ad120ab9425"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "persistent_id": "6df22da8-7c05-4489-8e72-d6dd248fca1e"
   },
   "outputs": [],
   "source": [
    "x_k_train = scale(X_train.reshape(X_train.shape[0],-1)) \n",
    "y_k_train = y_train.reshape(y_train.shape[0],-1)\n",
    "x_k_test = scale(X_test.reshape(X_test.shape[0],-1)) \n",
    "y_k_test = y_test.reshape(y_test.shape[0],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "persistent_id": "9cda1691-2ca0-4f9b-928f-d2e517cbb599"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1437/1437 [==============================] - 1s 438us/step - loss: 0.7466 - binary_crossentropy: 0.7466\n",
      "Epoch 2/50\n",
      "1437/1437 [==============================] - 0s 20us/step - loss: 0.6812 - binary_crossentropy: 0.6812\n",
      "Epoch 3/50\n",
      "1437/1437 [==============================] - 0s 19us/step - loss: 0.6304 - binary_crossentropy: 0.6304\n",
      "Epoch 4/50\n",
      "1437/1437 [==============================] - 0s 20us/step - loss: 0.5903 - binary_crossentropy: 0.5903\n",
      "Epoch 5/50\n",
      "1437/1437 [==============================] - 0s 20us/step - loss: 0.5575 - binary_crossentropy: 0.5575\n",
      "Epoch 6/50\n",
      "1437/1437 [==============================] - 0s 20us/step - loss: 0.5307 - binary_crossentropy: 0.5307\n",
      "Epoch 7/50\n",
      "1437/1437 [==============================] - 0s 20us/step - loss: 0.5086 - binary_crossentropy: 0.5086\n",
      "Epoch 8/50\n",
      "1437/1437 [==============================] - 0s 21us/step - loss: 0.4892 - binary_crossentropy: 0.4892\n",
      "Epoch 9/50\n",
      "1437/1437 [==============================] - 0s 20us/step - loss: 0.4727 - binary_crossentropy: 0.4727\n",
      "Epoch 10/50\n",
      "1437/1437 [==============================] - 0s 24us/step - loss: 0.4586 - binary_crossentropy: 0.4586\n",
      "Epoch 11/50\n",
      "1437/1437 [==============================] - 0s 20us/step - loss: 0.4460 - binary_crossentropy: 0.4460\n",
      "Epoch 12/50\n",
      "1437/1437 [==============================] - 0s 20us/step - loss: 0.4349 - binary_crossentropy: 0.4349\n",
      "Epoch 13/50\n",
      "1437/1437 [==============================] - 0s 20us/step - loss: 0.4250 - binary_crossentropy: 0.4250\n",
      "Epoch 14/50\n",
      "1437/1437 [==============================] - 0s 20us/step - loss: 0.4162 - binary_crossentropy: 0.4162\n",
      "Epoch 15/50\n",
      "1437/1437 [==============================] - 0s 21us/step - loss: 0.4083 - binary_crossentropy: 0.4083\n",
      "Epoch 16/50\n",
      "1437/1437 [==============================] - 0s 20us/step - loss: 0.4011 - binary_crossentropy: 0.4011\n",
      "Epoch 17/50\n",
      "1437/1437 [==============================] - 0s 20us/step - loss: 0.3946 - binary_crossentropy: 0.3946\n",
      "Epoch 18/50\n",
      "1437/1437 [==============================] - 0s 20us/step - loss: 0.3886 - binary_crossentropy: 0.3886\n",
      "Epoch 19/50\n",
      "1437/1437 [==============================] - 0s 20us/step - loss: 0.3831 - binary_crossentropy: 0.3831\n",
      "Epoch 20/50\n",
      "1437/1437 [==============================] - 0s 20us/step - loss: 0.3779 - binary_crossentropy: 0.3779\n",
      "Epoch 21/50\n",
      "1437/1437 [==============================] - 0s 20us/step - loss: 0.3732 - binary_crossentropy: 0.3732\n",
      "Epoch 22/50\n",
      "1437/1437 [==============================] - 0s 20us/step - loss: 0.3688 - binary_crossentropy: 0.3688\n",
      "Epoch 23/50\n",
      "1437/1437 [==============================] - 0s 21us/step - loss: 0.3646 - binary_crossentropy: 0.3646\n",
      "Epoch 24/50\n",
      "1437/1437 [==============================] - 0s 20us/step - loss: 0.3608 - binary_crossentropy: 0.3608\n",
      "Epoch 25/50\n",
      "1437/1437 [==============================] - 0s 19us/step - loss: 0.3572 - binary_crossentropy: 0.3572\n",
      "Epoch 26/50\n",
      "1437/1437 [==============================] - 0s 19us/step - loss: 0.3538 - binary_crossentropy: 0.3538\n",
      "Epoch 27/50\n",
      "1437/1437 [==============================] - 0s 19us/step - loss: 0.3506 - binary_crossentropy: 0.3506\n",
      "Epoch 28/50\n",
      "1437/1437 [==============================] - 0s 19us/step - loss: 0.3476 - binary_crossentropy: 0.3476\n",
      "Epoch 29/50\n",
      "1437/1437 [==============================] - 0s 20us/step - loss: 0.3448 - binary_crossentropy: 0.3448\n",
      "Epoch 30/50\n",
      "1437/1437 [==============================] - 0s 19us/step - loss: 0.3420 - binary_crossentropy: 0.3420\n",
      "Epoch 31/50\n",
      "1437/1437 [==============================] - 0s 19us/step - loss: 0.3394 - binary_crossentropy: 0.3394\n",
      "Epoch 32/50\n",
      "1437/1437 [==============================] - 0s 19us/step - loss: 0.3370 - binary_crossentropy: 0.3370\n",
      "Epoch 33/50\n",
      "1437/1437 [==============================] - 0s 20us/step - loss: 0.3347 - binary_crossentropy: 0.3347\n",
      "Epoch 34/50\n",
      "1437/1437 [==============================] - 0s 20us/step - loss: 0.3325 - binary_crossentropy: 0.3325\n",
      "Epoch 35/50\n",
      "1437/1437 [==============================] - 0s 20us/step - loss: 0.3304 - binary_crossentropy: 0.3304\n",
      "Epoch 36/50\n",
      "1437/1437 [==============================] - 0s 20us/step - loss: 0.3284 - binary_crossentropy: 0.3284\n",
      "Epoch 37/50\n",
      "1437/1437 [==============================] - 0s 20us/step - loss: 0.3265 - binary_crossentropy: 0.3265\n",
      "Epoch 38/50\n",
      "1437/1437 [==============================] - 0s 20us/step - loss: 0.3246 - binary_crossentropy: 0.3246\n",
      "Epoch 39/50\n",
      "1437/1437 [==============================] - 0s 20us/step - loss: 0.3229 - binary_crossentropy: 0.3229\n",
      "Epoch 40/50\n",
      "1437/1437 [==============================] - 0s 20us/step - loss: 0.3212 - binary_crossentropy: 0.3212\n",
      "Epoch 41/50\n",
      "1437/1437 [==============================] - 0s 19us/step - loss: 0.3195 - binary_crossentropy: 0.3195\n",
      "Epoch 42/50\n",
      "1437/1437 [==============================] - 0s 20us/step - loss: 0.3180 - binary_crossentropy: 0.3180\n",
      "Epoch 43/50\n",
      "1437/1437 [==============================] - 0s 20us/step - loss: 0.3164 - binary_crossentropy: 0.3164\n",
      "Epoch 44/50\n",
      "1437/1437 [==============================] - 0s 20us/step - loss: 0.3150 - binary_crossentropy: 0.3150\n",
      "Epoch 45/50\n",
      "1437/1437 [==============================] - 0s 20us/step - loss: 0.3136 - binary_crossentropy: 0.3136\n",
      "Epoch 46/50\n",
      "1437/1437 [==============================] - 0s 20us/step - loss: 0.3122 - binary_crossentropy: 0.3122\n",
      "Epoch 47/50\n",
      "1437/1437 [==============================] - 0s 19us/step - loss: 0.3110 - binary_crossentropy: 0.3110\n",
      "Epoch 48/50\n",
      "1437/1437 [==============================] - 0s 20us/step - loss: 0.3097 - binary_crossentropy: 0.3097\n",
      "Epoch 49/50\n",
      "1437/1437 [==============================] - 0s 20us/step - loss: 0.3085 - binary_crossentropy: 0.3085\n",
      "Epoch 50/50\n",
      "1437/1437 [==============================] - 0s 19us/step - loss: 0.3073 - binary_crossentropy: 0.3073\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c58352f50>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['binary_crossentropy'])\n",
    "\n",
    "model.fit(x_k_train, y_k_train, epochs=50, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "persistent_id": "b6e55c81-9a58-45cb-9d56-ec2a6d8ea5dd"
   },
   "outputs": [],
   "source": [
    "output = []\n",
    "for i in model.predict(x_k_test):\n",
    "    if i > 0.5:\n",
    "        output.append([1])\n",
    "    else:\n",
    "        output.append([0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keraså®ç°çš„é€»è¾‘å›å½’çš„æ­£ç¡®ç‡æœ‰ç™¾åˆ†ä¹‹88%ï¼Œè¯´æ˜æ‰‹åŠ¨å®ç°å‡ºç°äº†é—®é¢˜ï¼Œä½†æš‚æ—¶æ£€æŸ¥ä¸å‡ºæ¥ï¼Œå¸Œæœ›è€å¸ˆæŒ‡æ­£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "persistent_id": "8071b117-07f6-42c5-963a-850b8ca7d308"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8805555555555555\n"
     ]
    }
   ],
   "source": [
    "output = np.array(output)\n",
    "print(np.sum(output == y_k_test)/ y_k_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.é€‰åšé¢˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on building your first logistic regression model. It is your time to analyze it further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1 Observe the effect of learning rate on the leraning process.   \n",
    "Hits: plot the learning curve with different learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "execution_event_id": "e91fed30-c954-4894-9d61-fd96038e8051",
    "last_executed_text": "%matplotlib widget\n\nfig = plt.figure()\nax = fig.add_subplot(1,1,1)\nax.plot([i*100 for i in range(len(costs_1))], costs_1)\nax.set_xlabel(\"Iteration Times\")\nax.set_ylabel(\"Logistic Loss\")\nax.set_title(\"learning_rate=0.005\")",
    "persistent_id": "5a7025eb-9ff9-4549-af17-da5334c23c45"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "267d570c00c84ad7950e0f41d292aac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'learning_rate=0.005')"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot([i*100 for i in range(len(costs_1))], costs_1)\n",
    "ax.set_xlabel(\"Iteration Times\")\n",
    "ax.set_ylabel(\"Logistic Loss\")\n",
    "ax.set_title(\"learning_rate=0.005\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution_event_id": "55974deb-812d-451e-bd5c-6883bd5f521b",
    "last_executed_text": "fig = plt.figure()\nax = fig.add_subplot(1,1,1)\nax.plot([i*100 for i in range(len(costs_2))], costs_2)\nax.set_xlabel(\"Iteration Times\")\nax.set_ylabel(\"Logistic Loss\")\nax.set_title(\"learning_rate=0.010\")",
    "persistent_id": "d11113f9-daa6-409f-be6d-619265160ef1"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37205d457b3f4d9faf4f1ea4f50858af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'learning_rate=0.010')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot([i*100 for i in range(len(costs_2))], costs_2)\n",
    "ax.set_xlabel(\"Iteration Times\")\n",
    "ax.set_ylabel(\"Logistic Loss\")\n",
    "ax.set_title(\"learning_rate=0.010\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "persistent_id": "8889dcdd-5f21-4ae3-8b3e-a36e69de3b48"
   },
   "source": [
    "å¯ä»¥å‘ç°ä½¿ç”¨å°å­¦ä¹ ç‡ï¼Œæ›´é«˜çš„è¿­ä»£æ¬¡æ•°ï¼Œå¯ä»¥æ›´åŠ æœ‰æ•ˆåœ°é™ä½Logistics Lossï¼Œå½“æŸå¤±ä¸‹é™åˆ°ä¸€å®šç¨‹åº¦ï¼Œå°±ä¼šå‡ºç°æ¢¯åº¦æ›´æ–°è¶Šè¿‡äº†æœ€ä½ç‚¹æƒ…å†µï¼Œä½¿å¾—Lossä¸å†é™ä½"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2 Observe the effect of iteration_num on the test accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution_event_id": "795c1bc0-5190-4337-ac2d-e4564a9e4824",
    "last_executed_text": "costs_3 = model(X_train, y_train, X_test, y_test, num_iterations=5000, learning_rate=0.001)\ncosts_4 = model(X_train, y_train, X_test, y_test, num_iterations=10000, learning_rate=0.001)",
    "persistent_id": "bf483358-d780-4166-8851-b81c338328eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 3.188711\n",
      "Cost after iteration 500: 2.606217\n",
      "Cost after iteration 1000: 2.293976\n",
      "Cost after iteration 1500: 2.142710\n",
      "Cost after iteration 2000: 2.061242\n",
      "Cost after iteration 2500: 2.005380\n",
      "Cost after iteration 3000: 1.958605\n",
      "Cost after iteration 3500: 1.915245\n",
      "Cost after iteration 4000: 1.873422\n",
      "Cost after iteration 4500: 1.832534\n",
      "training setâ€˜s accuracyï¼š0.5149617258176757\n",
      "testing setâ€™s accuracyï¼š0.5138888888888888\n",
      "Cost after iteration 0: 2.357840\n",
      "Cost after iteration 500: 2.231999\n",
      "Cost after iteration 1000: 2.151707\n",
      "Cost after iteration 1500: 2.089928\n",
      "Cost after iteration 2000: 2.035542\n",
      "Cost after iteration 2500: 1.984303\n",
      "Cost after iteration 3000: 1.934669\n",
      "Cost after iteration 3500: 1.886101\n",
      "Cost after iteration 4000: 1.838422\n",
      "Cost after iteration 4500: 1.791580\n",
      "Cost after iteration 5000: 1.745570\n",
      "Cost after iteration 5500: 1.700397\n",
      "Cost after iteration 6000: 1.656074\n",
      "Cost after iteration 6500: 1.612617\n",
      "Cost after iteration 7000: 1.570040\n",
      "Cost after iteration 7500: 1.528361\n",
      "Cost after iteration 8000: 1.487598\n",
      "Cost after iteration 8500: 1.447769\n",
      "Cost after iteration 9000: 1.408894\n",
      "Cost after iteration 9500: 1.370994\n",
      "training setâ€˜s accuracyï¼š0.5247042449547669\n",
      "testing setâ€™s accuracyï¼š0.49166666666666664\n"
     ]
    }
   ],
   "source": [
    "costs_3 = model(X_train, y_train, X_test, y_test, num_iterations=5000, learning_rate=0.001)\n",
    "costs_4 = model(X_train, y_train, X_test, y_test, num_iterations=10000, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "persistent_id": "95e98466-dac4-403d-8e28-4056dc6eac43"
   },
   "source": [
    "è¿™é‡Œå‡ºç°äº†è¿‡æ‹Ÿåˆç°è±¡ã€‚å½“è¿­ä»£æ¬¡æ•°æ›´é«˜æ—¶ï¼Œæ¨¡å‹å¯¹è®­ç»ƒé›†çš„æ‹Ÿåˆç¨‹åº¦æ›´é«˜ï¼Œåœ¨æ¬ æ‹Ÿåˆæ¡ä»¶ä¸‹å‡ºç°æµ‹è¯•é›†å‡†ç¡®ç‡æå‡çš„ç°è±¡ï¼Œè€Œåœ¨è¿‡æ‹Ÿåˆæ¡ä»¶ä¸‹å‡ºç°æµ‹è¯•é›†å‡†ç¡®ç‡ä¸‹é™çš„ç°è±¡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge ! ! !\n",
    "\n",
    "The original data have images labeled 0,1,2,3,4,5,6,7,8,9. In our logistic model, we only detect if the digit in the image is larger or smaller than 5. Now, Let's go for a more challenging problem. Try to use softmax function to build a model to recognize which digit (0,1,2,3,4,5,6,7,8,9) is in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congratulations ! You have completed assigment 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "persistent_id": "b2ecee6e-5e97-4f87-aaca-ab9b6fa5a075"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "history": [
   {
    "cell": {
     "executionCount": 2,
     "executionEventId": "a6ac2a52-d528-4483-83cd-58dda282849e",
     "hasError": false,
     "id": "25695407-914d-4e9a-b753-d1074adedf69",
     "outputs": [],
     "persistentId": "1eed7f4a-251f-46b5-b994-cbed2f5d5ede",
     "text": "def softmax(X):\n    #softmaxå‡½æ•°\n    return np.exp(X) / np.sum(np.exp(X))\n"
    },
    "executionTime": "2020-02-04T04:40:08.896Z"
   },
   {
    "cell": {
     "executionCount": 3,
     "executionEventId": "f7b932bf-3a5e-456e-90ee-4beb89e5997f",
     "hasError": false,
     "id": "25695407-914d-4e9a-b753-d1074adedf69",
     "outputs": [],
     "persistentId": "1eed7f4a-251f-46b5-b994-cbed2f5d5ede",
     "text": "def softmax(X):\n    #softmaxå‡½æ•°\n    return np.exp(X) / np.sum(np.exp(X))\n"
    },
    "executionTime": "2020-02-04T04:40:16.465Z"
   },
   {
    "cell": {
     "executionCount": 4,
     "executionEventId": "eb92fd4b-e867-4db9-b190-7616efab138b",
     "hasError": false,
     "id": "b36c4333-53c9-4f17-8a00-cd3ce030d077",
     "outputs": [],
     "persistentId": "cd5e8289-6c99-4263-a0b9-737ee1fa08e5",
     "text": "from sklearn import datasets\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split"
    },
    "executionTime": "2020-02-04T04:40:22.189Z"
   },
   {
    "cell": {
     "executionCount": 5,
     "executionEventId": "05807cdf-bdee-47f9-a64e-b2f1ad0b2371",
     "hasError": false,
     "id": "21f347fd-05d9-4e05-80f0-2064e9aaee68",
     "outputs": [],
     "persistentId": "73e4c7a0-2a9c-4d42-857b-03bc36289cc1",
     "text": "# Loading the data \ndigits = datasets.load_digits()"
    },
    "executionTime": "2020-02-04T04:40:26.028Z"
   },
   {
    "cell": {
     "executionCount": 6,
     "executionEventId": "de053f42-f6d1-4f35-abb7-da30d352b645",
     "hasError": false,
     "id": "b0574d79-9de3-4adb-98bd-0d6abceb4c5f",
     "outputs": [
      {
       "data": {
        "image/png": "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\n",
        "text/plain": "<Figure size 432x288 with 10 Axes>"
       },
       "metadata": {},
       "output_type": "display_data"
      }
     ],
     "persistentId": "40b6af44-7b70-474f-8251-cb9fb55b9508",
     "text": "# Vilizating the data\nfor i in range(1,11):\n    plt.subplot(2,5,i)\n    plt.imshow(digits.data[i-1].reshape([8,8]),cmap=plt.cm.gray_r)\n    plt.text(3,10,str(digits.target[i-1]))\n    plt.xticks([])\n    plt.yticks([])\nplt.show()"
    },
    "executionTime": "2020-02-04T04:40:26.640Z"
   },
   {
    "cell": {
     "executionCount": 7,
     "executionEventId": "c55bffce-cb11-4b60-9c9b-dff7c35b547d",
     "hasError": false,
     "id": "3d3ecbb2-8a49-4008-a62b-e2c48d8d3f3e",
     "outputs": [],
     "persistentId": "4c5120f7-b102-4340-ba26-e02a666ab9f6",
     "text": "# Split the data into training set and test set \nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.25)"
    },
    "executionTime": "2020-02-04T04:40:27.297Z"
   },
   {
    "cell": {
     "executionCount": 8,
     "executionEventId": "2518003c-5d8b-48dc-8b72-2b74a8e3f1ea",
     "hasError": false,
     "id": "88d37c78-04ea-4ceb-836f-ba6c68d1b792",
     "outputs": [],
     "persistentId": "e2580523-6d94-469d-9bde-b795e40ef9dd",
     "text": "# reformulate the label. \n# If the digit is smaller than 5, the label is 0.\n# If the digit is larger than 5, the label is 1.\n\ny_train[y_train < 5 ] = 0\ny_train[y_train >= 5] = 1\ny_test[y_test < 5] = 0\ny_test[y_test >= 5] = 1"
    },
    "executionTime": "2020-02-04T04:40:27.888Z"
   },
   {
    "cell": {
     "executionCount": 9,
     "executionEventId": "5df6bb27-7797-4607-b1ba-c806dfb7cbf8",
     "hasError": false,
     "id": "0402c7fd-ee85-463d-a198-7a6578292ac4",
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "(1347, 64)\n(450, 64)\n(1347,)\n(450,)\n"
      }
     ],
     "persistentId": "6d8bcceb-9c81-4f4f-b84a-fb06e0846465",
     "text": "print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)"
    },
    "executionTime": "2020-02-04T04:40:28.587Z"
   },
   {
    "cell": {
     "executionCount": 10,
     "executionEventId": "a83bd1d9-85e8-43ec-9f85-d2bd03a0b70f",
     "hasError": false,
     "id": "6209e1bf-eb43-4fe4-ad6d-efe9f43f4e48",
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "/Users/junjiexie/NLPå­¦ä¹ \n"
      }
     ],
     "persistentId": "d9c890db-0639-486e-b2f8-a30e2f781a33",
     "text": "!pwd"
    },
    "executionTime": "2020-02-04T04:40:30.691Z"
   },
   {
    "cell": {
     "executionCount": 11,
     "executionEventId": "c2a446ab-bd02-4658-8882-e855d908c96d",
     "hasError": false,
     "id": "b491624c-2615-4642-be33-f30c39b82169",
     "outputs": [],
     "persistentId": "cb15bbde-4a6d-461a-ae73-8bc399b9d35b",
     "text": "import numpy as np\ndef sigmoid(z):\n    '''\n    Compute the sigmoid of z\n    Arguments: z -- a scalar or numpy array of any size.\n    \n    Return:\n    s -- sigmoid(z)\n    '''\n    s = 1./(1 + np.exp(-1 * z))\n    \n    return s"
    },
    "executionTime": "2020-02-04T04:40:34.320Z"
   },
   {
    "cell": {
     "executionCount": 12,
     "executionEventId": "c0fc3189-31cf-4ff4-8d5c-498b3c90d7c6",
     "hasError": false,
     "id": "106585ed-1460-4ae3-88c2-02ef8b5cef89",
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "sigmoid([0,2]) = [0.5        0.88079708]\n"
      }
     ],
     "persistentId": "e67aab82-7c95-494e-ac25-7c6832e9691a",
     "text": "# Test your code \n# The result should be [0.5 0.88079708]\nprint(\"sigmoid([0,2]) = \" + str(sigmoid(np.array([0,2]))))"
    },
    "executionTime": "2020-02-04T04:40:35.487Z"
   },
   {
    "cell": {
     "executionCount": 13,
     "executionEventId": "3c8918e7-a897-429d-bc2e-be0367d7a918",
     "hasError": false,
     "id": "78cedb5d-5d89-4417-8005-0373eb1a9094",
     "outputs": [],
     "persistentId": "a95a0c56-ed75-4037-b248-43adcba4e53d",
     "text": "# Random innitialize the parameters\n\ndef initialize_parameters(dim):\n    '''\n    Argument: dim -- size of the w vector\n    \n    Returns:\n    w -- initialized vector of shape (dim,1)\n    b -- initializaed scalar\n    '''\n    \n    w = np.random.randn(dim,1)\n    b = np.zeros(1)\n    \n    assert(w.shape == (dim,1))\n    assert(isinstance(b,float) or isinstance(b,int))\n    \n    return w,b"
    },
    "executionTime": "2020-02-04T04:40:38.011Z"
   },
   {
    "cell": {
     "executionCount": 14,
     "executionEventId": "a9eae8b3-cf86-431b-bea3-176d9e682072",
     "hasError": false,
     "id": "2e14e518-c694-442e-8483-58f4a46d56e4",
     "outputs": [],
     "persistentId": "d2518a89-1eeb-44fb-95a8-4eab59019f67",
     "text": "def propagate(w,b,X,Y):\n    '''\n    Implement the cost function and its gradient for the propagation\n    \n    Arguments:\n    w - weights\n    b - bias\n    X - data\n    Y - ground truth\n    '''\n    m = X.shape[1]\n    A = np.dot(w.T,X) + b\n    cost = -1/m * np.sum(Y * np.log(A) + (1-Y) * np.log(1-A))\n    \n    dw = np.mean(X * (A-Y).T)\n    db = 1/m * np.sum(A-Y)\n    \n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    cost = np.squeeze(cost)\n    assert(cost.shape == ())\n    \n    grads = {'dw':dw,\n             'db':db}\n    return grads, cost"
    },
    "executionTime": "2020-02-04T04:40:40.911Z"
   },
   {
    "cell": {
     "executionCount": 15,
     "executionEventId": "20616889-72d5-4c67-ab70-0ec0c96b2146",
     "hasError": false,
     "id": "f0685016-4bda-4ca9-9976-ae5f4793ece6",
     "outputs": [],
     "persistentId": "39737492-b457-4dcf-8d9f-76a392488151",
     "text": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost=False):\n    '''\n    This function optimize w and b by running a gradient descen algorithm\n    \n    Arguments:\n    w - weights\n    b - bias\n    X - data\n    Y - ground truth\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n    \n    Returns:\n    params - dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n    \n    '''\n    \n    costs = []\n    \n    for i in range(num_iterations):\n        \n        grads, cost = propagate(w,b,X,Y)\n        \n        dw = grads['dw']\n        db = grads['db']\n        \n        w = w - learning_rate * dw\n        b = b - learning_rate * db\n        \n        if i % 100 == 0:\n            costs.append(cost)\n        if print_cost and i % 100 == 0:\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    \n    params = {\"w\":w,\n              \"b\":b}\n    \n    grads = {\"dw\":dw,\n             \"db\":db}\n    \n    return params, grads, costs"
    },
    "executionTime": "2020-02-04T04:40:42.891Z"
   },
   {
    "cell": {
     "executionCount": 16,
     "executionEventId": "11115419-e10d-4bc2-991e-ad1ae2cc25b4",
     "hasError": false,
     "id": "3dab0397-2b00-4236-ad61-4e8886ec359c",
     "outputs": [],
     "persistentId": "afe4201d-351d-44c7-af57-927730e72d92",
     "text": "def predict(w, b, X):\n    '''\n    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n    \n    Arguments:\n    w -- weights\n    b -- bias \n    X -- data \n    \n    Returns:\n    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n    '''\n    m = X.shape[1]\n    Y_prediction = np.zeros((1,m))\n    w = w.reshape(X.shape[0],1)\n    \n    A = sigmoid(np.dot(w.T,X) + b)\n    \n    for i in range(A.shape[i]):\n        if A[0,i] <= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n    \n    assert(Y_prediction.shape == (1,m))\n    \n    return Y_prediction"
    },
    "executionTime": "2020-02-04T04:40:44.113Z"
   },
   {
    "cell": {
     "executionCount": 17,
     "executionEventId": "b531abe1-4277-4324-ba5e-b8f46c22f346",
     "hasError": true,
     "id": "611bea7f-db28-443e-b57e-464cc0492891",
     "outputs": [
      {
       "ename": "SyntaxError",
       "evalue": "invalid syntax (<ipython-input-17-a4bd750dd235>, line 22)",
       "output_type": "error",
       "traceback": [
        "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-a4bd750dd235>\"\u001b[0;36m, line \u001b[0;32m22\u001b[0m\n\u001b[0;31m    x_train = X_train.reshape(-1.X_train.shape[0])\u001b[0m\n\u001b[0m                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
       ]
      }
     ],
     "persistentId": "736561b1-27f2-4958-a474-6d34622f80bb",
     "text": "def model(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate,print_cost):\n    \"\"\"\n    Build the logistic regression model by calling all the functions you have implemented.\n    Arguments:\n    X_train - training set\n    Y_train - training label\n    X_test - test set\n    Y_test - test label\n    num_iteration - hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n    \n    Returns:\n    d - dictionary should contain following information w,b,training_accuracy, test_accuracy,cost\n    eg: d = {\"w\":w,\n             \"b\":b,\n             \"training_accuracy\": traing_accuracy,\n             \"test_accuracy\":test_accuracy,\n             \"cost\":cost}\n    \"\"\"\n    # æ•°æ®ä¿®æ”¹ä¸ºä¸€ç»´\n    x_train = X_train.reshape(-1.X_train.shape[0])\n    y_train = Y_train.reshape(-1.Y_train.shape[0])\n    x_test = X_test.reshape(-1.X_test.shape[0])\n    y_test = Y_test.reshape(-1.Y_test.shape[0])\n    \n    # åˆå§‹åŒ–å‚æ•°\n    w,b = initialize_parameters(x_train.shape[0])\n    \n    # params:æ›´æ–°åçš„ç½‘ç»œå‚æ•°\n    # grads:æœ€æ–°çš„æ¢¯åº¦(ä¸‹é™æŸå¤±)\n    # costs:æ¯æ¬¡æ›´æ–°çš„æŸå¤±åˆ—è¡¨\n    params, grads, costs = optimize(w,b,x_train,y_train,num_iterations,learning_rate)\n    \n    w = params[\"w\"]\n    b = params[\"b\"]\n    \n    "
    },
    "executionTime": "2020-02-04T04:40:45.880Z"
   },
   {
    "cell": {
     "executionCount": 18,
     "executionEventId": "04a34507-a74b-4229-886a-a246944a3186",
     "hasError": false,
     "id": "611bea7f-db28-443e-b57e-464cc0492891",
     "outputs": [],
     "persistentId": "736561b1-27f2-4958-a474-6d34622f80bb",
     "text": "def model(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate,print_cost):\n    \"\"\"\n    Build the logistic regression model by calling all the functions you have implemented.\n    Arguments:\n    X_train - training set\n    Y_train - training label\n    X_test - test set\n    Y_test - test label\n    num_iteration - hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n    \n    Returns:\n    d - dictionary should contain following information w,b,training_accuracy, test_accuracy,cost\n    eg: d = {\"w\":w,\n             \"b\":b,\n             \"training_accuracy\": traing_accuracy,\n             \"test_accuracy\":test_accuracy,\n             \"cost\":cost}\n    \"\"\"\n    # æ•°æ®ä¿®æ”¹ä¸ºä¸€ç»´\n    x_train = X_train.reshape(-1,X_train.shape[0])\n    y_train = Y_train.reshape(-1,Y_train.shape[0])\n    x_test = X_test.reshape(-1,X_test.shape[0])\n    y_test = Y_test.reshape(-1,Y_test.shape[0])\n    \n    # åˆå§‹åŒ–å‚æ•°\n    w,b = initialize_parameters(x_train.shape[0])\n    \n    # params:æ›´æ–°åçš„ç½‘ç»œå‚æ•°\n    # grads:æœ€æ–°çš„æ¢¯åº¦(ä¸‹é™æŸå¤±)\n    # costs:æ¯æ¬¡æ›´æ–°çš„æŸå¤±åˆ—è¡¨\n    params, grads, costs = optimize(w,b,x_train,y_train,num_iterations,learning_rate)\n    \n    # è·å–è®­ç»ƒçš„å‚æ•°\n    w = params[\"w\"]\n    b = params[\"b\"]\n    \n    # é¢„æµ‹ç»“æœ\n    y_prediction_train = predict(w,b,x_train)\n    y_prediction_test = predict(w,b,x_test)\n    \n    # æ‰“å°å‡†ç¡®ç‡ï¼ŒæŸå¤±\n    print(\"Logistic Loss is ()\".format(np.mean(np.abs(costs))))\n    print(\"è®­ç»ƒé›†å‡†ç¡®ç‡ï¼š()\".format(100 - np.mean(np.abs(y_prediction_train - y_train))))\n    print(\"æµ‹è¯•é›†å‡†ç¡®ç‡ï¼š()\".format(100 - np.mean(np.abs(y_prediction_test - y_test))))\n    \n    \n    \n    "
    },
    "executionTime": "2020-02-04T04:51:20.927Z"
   },
   {
    "cell": {
     "executionCount": 19,
     "executionEventId": "c880470e-7c62-49c1-84b4-33a6aa11df3e",
     "hasError": true,
     "id": "1fde88e1-df7e-49c2-ae1f-e396411d30e3",
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "model() missing 1 required positional argument: 'print_cost'",
       "output_type": "error",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
        "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-19-aae697adc03c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mTypeError\u001b[0m: model() missing 1 required positional argument: 'print_cost'"
       ]
      }
     ],
     "persistentId": "b175609a-a7d0-4bf8-84b6-23b93d651481",
     "text": "if __name__ == '__main__':\n    model(X_train, y_train, X_test, y_test, num_iterations=500, learning_rate=0.01)\n"
    },
    "executionTime": "2020-02-04T04:52:51.111Z"
   },
   {
    "cell": {
     "executionCount": 20,
     "executionEventId": "50c541f4-fb83-4e9f-a781-6285dcb81beb",
     "hasError": false,
     "id": "611bea7f-db28-443e-b57e-464cc0492891",
     "outputs": [],
     "persistentId": "736561b1-27f2-4958-a474-6d34622f80bb",
     "text": "def model(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate,print_cost=True):\n    \"\"\"\n    Build the logistic regression model by calling all the functions you have implemented.\n    Arguments:\n    X_train - training set\n    Y_train - training label\n    X_test - test set\n    Y_test - test label\n    num_iteration - hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n    \n    Returns:\n    d - dictionary should contain following information w,b,training_accuracy, test_accuracy,cost\n    eg: d = {\"w\":w,\n             \"b\":b,\n             \"training_accuracy\": traing_accuracy,\n             \"test_accuracy\":test_accuracy,\n             \"cost\":cost}\n    \"\"\"\n    # æ•°æ®ä¿®æ”¹ä¸ºä¸€ç»´\n    x_train = X_train.reshape(-1,X_train.shape[0])\n    y_train = Y_train.reshape(-1,Y_train.shape[0])\n    x_test = X_test.reshape(-1,X_test.shape[0])\n    y_test = Y_test.reshape(-1,Y_test.shape[0])\n    \n    # åˆå§‹åŒ–å‚æ•°\n    w,b = initialize_parameters(x_train.shape[0])\n    \n    # params:æ›´æ–°åçš„ç½‘ç»œå‚æ•°\n    # grads:æœ€æ–°çš„æ¢¯åº¦(ä¸‹é™æŸå¤±)\n    # costs:æ¯æ¬¡æ›´æ–°çš„æŸå¤±åˆ—è¡¨\n    params, grads, costs = optimize(w,b,x_train,y_train,num_iterations,learning_rate,print_cost)\n    \n    # è·å–è®­ç»ƒçš„å‚æ•°\n    w = params[\"w\"]\n    b = params[\"b\"]\n    \n    # é¢„æµ‹ç»“æœ\n    y_prediction_train = predict(w,b,x_train)\n    y_prediction_test = predict(w,b,x_test)\n    \n    # æ‰“å°å‡†ç¡®ç‡ï¼ŒæŸå¤±\n#     print(\"Logistic Loss is ()\".format(np.mean(np.abs(costs))))\n    print(\"è®­ç»ƒé›†å‡†ç¡®ç‡ï¼š()\".format(100 - np.mean(np.abs(y_prediction_train - y_train))))\n    print(\"æµ‹è¯•é›†å‡†ç¡®ç‡ï¼š()\".format(100 - np.mean(np.abs(y_prediction_test - y_test))))\n    \n    \n    \n    "
    },
    "executionTime": "2020-02-04T04:57:36.399Z"
   },
   {
    "cell": {
     "executionCount": 21,
     "executionEventId": "4fc3832b-f50c-4dee-97c3-09e2a352425d",
     "hasError": true,
     "id": "1fde88e1-df7e-49c2-ae1f-e396411d30e3",
     "outputs": [
      {
       "ename": "AssertionError",
       "evalue": "",
       "output_type": "error",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
        "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-21-aae697adc03c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-20-ffa59aeaaf86>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate, print_cost)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# åˆå§‹åŒ–å‚æ•°\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# params:æ›´æ–°åçš„ç½‘ç»œå‚æ•°\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-13-04f2f7614805>\u001b[0m in \u001b[0;36minitialize_parameters\u001b[0;34m(dim)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mAssertionError\u001b[0m: "
       ]
      }
     ],
     "persistentId": "b175609a-a7d0-4bf8-84b6-23b93d651481",
     "text": "if __name__ == '__main__':\n    model(X_train, y_train, X_test, y_test, num_iterations=500, learning_rate=0.01)\n"
    },
    "executionTime": "2020-02-04T04:57:37.324Z"
   },
   {
    "cell": {
     "executionCount": 22,
     "executionEventId": "e8a9a6fd-8383-4af9-8763-cf7bad8cc3f7",
     "hasError": false,
     "id": "78cedb5d-5d89-4417-8005-0373eb1a9094",
     "outputs": [],
     "persistentId": "a95a0c56-ed75-4037-b248-43adcba4e53d",
     "text": "# Random innitialize the parameters\n\ndef initialize_parameters(dim):\n    '''\n    Argument: dim -- size of the w vector\n    \n    Returns:\n    w -- initialized vector of shape (dim,1)\n    b -- initializaed scalar\n    '''\n    \n    w = np.random.randn(dim,1)\n    b = 0\n    \n    assert(w.shape == (dim,1))\n    assert(isinstance(b,float) or isinstance(b,int))\n    \n    return w,b"
    },
    "executionTime": "2020-02-04T04:59:29.806Z"
   },
   {
    "cell": {
     "executionCount": 23,
     "executionEventId": "3440aab7-3844-42f6-b379-ec5f7d675dd7",
     "hasError": true,
     "id": "1fde88e1-df7e-49c2-ae1f-e396411d30e3",
     "outputs": [
      {
       "name": "stderr",
       "output_type": "stream",
       "text": "/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in log\n  del sys.path[0]\n"
      },
      {
       "ename": "ValueError",
       "evalue": "operands could not be broadcast together with shapes (64,1347) (1347,1) ",
       "output_type": "error",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
        "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-23-aae697adc03c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-20-ffa59aeaaf86>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate, print_cost)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;31m# grads:æœ€æ–°çš„æ¢¯åº¦(ä¸‹é™æŸå¤±)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# costs:æ¯æ¬¡æ›´æ–°çš„æŸå¤±åˆ—è¡¨\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprint_cost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# è·å–è®­ç»ƒçš„å‚æ•°\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-15-6ea5f4fa1941>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(w, b, X, Y, num_iterations, learning_rate, print_cost)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mdw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dw'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-14-7eab6d652f07>\u001b[0m in \u001b[0;36mpropagate\u001b[0;34m(w, b, X, Y)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mdw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (64,1347) (1347,1) "
       ]
      }
     ],
     "persistentId": "b175609a-a7d0-4bf8-84b6-23b93d651481",
     "text": "if __name__ == '__main__':\n    model(X_train, y_train, X_test, y_test, num_iterations=500, learning_rate=0.01)\n"
    },
    "executionTime": "2020-02-04T04:59:43.087Z"
   },
   {
    "cell": {
     "executionCount": 24,
     "executionEventId": "05e5fec7-6808-402a-8b99-6a90c8b42534",
     "hasError": false,
     "id": "2e14e518-c694-442e-8483-58f4a46d56e4",
     "outputs": [],
     "persistentId": "d2518a89-1eeb-44fb-95a8-4eab59019f67",
     "text": "def propagate(w,b,X,Y):\n    '''\n    Implement the cost function and its gradient for the propagation\n    \n    Arguments:\n    w - weights\n    b - bias\n    X - data\n    Y - ground truth\n    '''\n    m = X.shape[1]\n    A = np.dot(w.T,X) + b\n    cost = -1/m * np.sum(Y * np.log(A) + (1-Y) * np.log(1-A))\n    \n    dw = 1/m * np.dot(X,(A-Y).T)\n    db = 1/m * np.sum(A-Y)\n    \n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    cost = np.squeeze(cost)\n    assert(cost.shape == ())\n    \n    grads = {'dw':dw,\n             'db':db}\n    return grads, cost"
    },
    "executionTime": "2020-02-04T05:05:14.260Z"
   },
   {
    "cell": {
     "executionCount": 25,
     "executionEventId": "6c75c074-078a-4e1c-af73-c8fac9d32c36",
     "hasError": true,
     "id": "1fde88e1-df7e-49c2-ae1f-e396411d30e3",
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "Cost after iteration 0: nan\nCost after iteration 100: nan\nCost after iteration 200: nan\nCost after iteration 300: nan\nCost after iteration 400: nan\n"
      },
      {
       "name": "stderr",
       "output_type": "stream",
       "text": "/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in log\n  del sys.path[0]\n"
      },
      {
       "ename": "UnboundLocalError",
       "evalue": "local variable 'i' referenced before assignment",
       "output_type": "error",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
        "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-25-aae697adc03c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-20-ffa59aeaaf86>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate, print_cost)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# é¢„æµ‹ç»“æœ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0my_prediction_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0my_prediction_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-16-f600b82b691f>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(w, b, X)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mY_prediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'i' referenced before assignment"
       ]
      }
     ],
     "persistentId": "b175609a-a7d0-4bf8-84b6-23b93d651481",
     "text": "if __name__ == '__main__':\n    model(X_train, y_train, X_test, y_test, num_iterations=500, learning_rate=0.01)\n"
    },
    "executionTime": "2020-02-04T05:05:19.979Z"
   },
   {
    "cell": {
     "executionCount": 26,
     "executionEventId": "d68e8780-805a-43be-8a16-f13d1195d1cd",
     "hasError": false,
     "id": "3dab0397-2b00-4236-ad61-4e8886ec359c",
     "outputs": [],
     "persistentId": "afe4201d-351d-44c7-af57-927730e72d92",
     "text": "def predict(w, b, X):\n    '''\n    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n    \n    Arguments:\n    w -- weights\n    b -- bias \n    X -- data \n    \n    Returns:\n    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n    '''\n    m = X.shape[1]\n    Y_prediction = np.zeros((1,m))\n    w = w.reshape(X.shape[0],1)\n    \n    A = sigmoid(np.dot(w.T,X) + b)\n    \n    for nubmer in range(A.shape[1]):\n        if A[0,number] <= 0.5:\n            Y_prediction[0,number] = 0\n        else:\n            Y_prediction[0,nubmer] = 1\n    \n    assert(Y_prediction.shape == (1,m))\n    \n    return Y_prediction"
    },
    "executionTime": "2020-02-04T05:07:33.424Z"
   },
   {
    "cell": {
     "executionCount": 27,
     "executionEventId": "ceff56c8-9b01-42d5-960e-3c903712ba27",
     "hasError": true,
     "id": "1fde88e1-df7e-49c2-ae1f-e396411d30e3",
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "Cost after iteration 0: nan\nCost after iteration 100: nan\nCost after iteration 200: nan\nCost after iteration 300: nan\nCost after iteration 400: nan\n"
      },
      {
       "name": "stderr",
       "output_type": "stream",
       "text": "/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in log\n  del sys.path[0]\n/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:90: RuntimeWarning: overflow encountered in reduce\n  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
      },
      {
       "ename": "NameError",
       "evalue": "name 'number' is not defined",
       "output_type": "error",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
        "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-27-aae697adc03c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-20-ffa59aeaaf86>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate, print_cost)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# é¢„æµ‹ç»“æœ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0my_prediction_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0my_prediction_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-26-8ebdf7643f31>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(w, b, X)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mnubmer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mY_prediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'number' is not defined"
       ]
      }
     ],
     "persistentId": "b175609a-a7d0-4bf8-84b6-23b93d651481",
     "text": "if __name__ == '__main__':\n    model(X_train, y_train, X_test, y_test, num_iterations=500, learning_rate=0.01)\n"
    },
    "executionTime": "2020-02-04T05:07:37.006Z"
   },
   {
    "cell": {
     "executionCount": 28,
     "executionEventId": "d4c9ea8f-55f9-4ed9-92f4-9d71d003ce70",
     "hasError": false,
     "id": "3dab0397-2b00-4236-ad61-4e8886ec359c",
     "outputs": [],
     "persistentId": "afe4201d-351d-44c7-af57-927730e72d92",
     "text": "def predict(w, b, X):\n    '''\n    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n    \n    Arguments:\n    w -- weights\n    b -- bias \n    X -- data \n    \n    Returns:\n    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n    '''\n    m = X.shape[1]\n    Y_prediction = np.zeros((1,m))\n    w = w.reshape(X.shape[0],1)\n    \n    A = sigmoid(np.dot(w.T,X) + b)\n    \n    for j in range(A.shape[1]):\n        if A[0,j] <= 0.5:\n            Y_prediction[0,j] = 0\n        else:\n            Y_prediction[0,j] = 1\n    \n    assert(Y_prediction.shape == (1,m))\n    \n    return Y_prediction"
    },
    "executionTime": "2020-02-04T05:09:23.367Z"
   },
   {
    "cell": {
     "executionCount": 29,
     "executionEventId": "55cb157f-8665-47a3-b32f-2587fa5d5134",
     "hasError": false,
     "id": "1fde88e1-df7e-49c2-ae1f-e396411d30e3",
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "Cost after iteration 0: nan\nCost after iteration 100: nan\nCost after iteration 200: nan\nCost after iteration 300: nan\nCost after iteration 400: nan\nè®­ç»ƒé›†å‡†ç¡®ç‡ï¼š()\næµ‹è¯•é›†å‡†ç¡®ç‡ï¼š()\n"
      },
      {
       "name": "stderr",
       "output_type": "stream",
       "text": "/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in log\n  del sys.path[0]\n"
      }
     ],
     "persistentId": "b175609a-a7d0-4bf8-84b6-23b93d651481",
     "text": "if __name__ == '__main__':\n    model(X_train, y_train, X_test, y_test, num_iterations=500, learning_rate=0.01)\n"
    },
    "executionTime": "2020-02-04T05:09:27.520Z"
   },
   {
    "cell": {
     "executionCount": 30,
     "executionEventId": "f5312430-95b9-4a02-883f-a6a3407233b6",
     "hasError": false,
     "id": "2e14e518-c694-442e-8483-58f4a46d56e4",
     "outputs": [],
     "persistentId": "d2518a89-1eeb-44fb-95a8-4eab59019f67",
     "text": "def propagate(w,b,X,Y):\n    '''\n    Implement the cost function and its gradient for the propagation\n    \n    Arguments:\n    w - weights\n    b - bias\n    X - data\n    Y - ground truth\n    '''\n    m = X.shape[1]\n    A = np.dot(w.T,X) + b\n    cost = -1/m * np.sum(Y * np.log(A) + (1-Y) * np.log(1-A))\n    print(cost)\n    \n    dw = 1/m * np.dot(X,(A-Y).T)\n    db = 1/m * np.sum(A-Y)\n    \n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    cost = np.squeeze(cost)\n    assert(cost.shape == ())\n    \n    grads = {'dw':dw,\n             'db':db}\n    return grads, cost"
    },
    "executionTime": "2020-02-04T05:10:42.884Z"
   },
   {
    "cell": {
     "executionCount": 31,
     "executionEventId": "1a10d82c-35e3-42d5-8388-e920ad26cab9",
     "hasError": false,
     "id": "1fde88e1-df7e-49c2-ae1f-e396411d30e3",
     "outputs": [
      {
       "name": "stderr",
       "output_type": "stream",
       "text": "/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in log\n  del sys.path[0]\n"
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "nan\nCost after iteration 0: nan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nCost after iteration 100: nan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nCost after iteration 200: nan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nCost after iteration 300: nan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nCost after iteration 400: nan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nè®­ç»ƒé›†å‡†ç¡®ç‡ï¼š()\næµ‹è¯•é›†å‡†ç¡®ç‡ï¼š()\n"
      }
     ],
     "persistentId": "b175609a-a7d0-4bf8-84b6-23b93d651481",
     "text": "if __name__ == '__main__':\n    model(X_train, y_train, X_test, y_test, num_iterations=500, learning_rate=0.01)\n"
    },
    "executionTime": "2020-02-04T05:10:47.572Z"
   },
   {
    "cell": {
     "executionCount": 32,
     "executionEventId": "13bb7f11-f9b4-4212-9e27-1335d9a92470",
     "hasError": false,
     "id": "2e14e518-c694-442e-8483-58f4a46d56e4",
     "outputs": [],
     "persistentId": "d2518a89-1eeb-44fb-95a8-4eab59019f67",
     "text": "def propagate(w,b,X,Y):\n    '''\n    Implement the cost function and its gradient for the propagation\n    \n    Arguments:\n    w - weights\n    b - bias\n    X - data\n    Y - ground truth\n    '''\n    m = X.shape[1]\n    A = sigmoid(np.dot(w.T,X) + b)\n    cost = -1/m * np.sum(Y * np.log(A) + (1-Y) * np.log(1-A))\n    print(cost)\n    \n    dw = 1/m * np.dot(X,(A-Y).T)\n    db = 1/m * np.sum(A-Y)\n    \n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    cost = np.squeeze(cost)\n    assert(cost.shape == ())\n    \n    grads = {'dw':dw,\n             'db':db}\n    return grads, cost"
    },
    "executionTime": "2020-02-04T05:11:53.058Z"
   },
   {
    "cell": {
     "executionCount": 33,
     "executionEventId": "15019086-48df-48bd-a1b9-ff92a0f31f3e",
     "hasError": false,
     "id": "1fde88e1-df7e-49c2-ae1f-e396411d30e3",
     "outputs": [
      {
       "name": "stderr",
       "output_type": "stream",
       "text": "/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: divide by zero encountered in log\n  del sys.path[0]\n/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in multiply\n  del sys.path[0]\n"
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "nan\nCost after iteration 0: nan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nCost after iteration 100: nan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nCost after iteration 200: nan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\ninf\n5.099880901198564\n5.066417227286787\n5.033621761562315\n5.0005601753057425\n4.967621029660082\n4.934347473122306\n4.9015480373910965\n4.868612187206234\n4.835832561954557\n4.80319327792216\n4.770702032038947\n4.738197244282929\n4.705809400056728\n4.673508036625702\n4.641260297186556\n4.609150565401165\n4.577097946949511\n4.545153145241026\n4.513279983060799\n4.481504882252783\n4.449791807162622\n4.418175987637901\n4.386635873148688\n4.355188003056857\n4.323819573001792\n4.292530000238455\n4.261331113952129\n4.230213015153637\n4.199178041271867\n4.168227845430019\n4.137360268629704\n4.1065766995043536\n4.075877023641475\n4.045262236266419\n4.014731679772767\n3.984285932771752\n3.953925658412551\n3.9236512412025153\n3.8934627922901095\n3.863361135927882\n3.8333469366519903\n3.8034205001844734\n3.773582552369322\n3.743833797327371\n3.7141749395357344\n3.684606736129307\n3.655129892527464\n3.6257452811624105\n3.596453770185594\n3.56725618365215\n3.5381534681719553\n3.509146555459204\n3.480236398249497\n3.451423978383484\n3.422710290577911\n3.3940963573214114\n3.36558319465361\n3.337171829482139\n3.3088632905172566\n3.280658601226439\nCost after iteration 300: 3.280659\n3.252558773706241\n3.22456480420836\n3.1966776755839597\n3.1688983402367112\n3.141227726747236\n3.1136667263187925\n3.0862161995359383\n3.0588769639444697\n3.031649801153828\n3.0045354468233243\n2.977534596044568\n2.950647901482944\n2.923875972806431\n2.897219380165613\n2.8706786557770436\n2.8442542957399506\n2.8179467642065386\n2.7917564965010815\n2.765683902998769\n2.7397293734393613\n2.7138932810614924\n2.6881759871829978\n2.6625778451934896\n2.637099205024366\n2.6117404170697984\n2.5865018361765837\n2.5613838252246732\n2.536386758346097\n2.511511024052068\n2.486757027946885\n2.4621251949920393\n2.43761597168528\n2.413229827694104\n2.3889672573593264\n2.3648287808207917\n2.3408149448958375\n2.3169263237641595\n2.2931635193660442\n2.269527161661363\n2.2460179087029006\n2.222636446569107\n2.199383489169841\n2.176259777982334\n2.1532660816964135\n2.1304031958259086\n2.10767194228721\n2.0850731689848656\n2.0626077494153345\n2.0402765823071376\n2.0180805913286393\n1.9960207248678536\n1.9740979559106555\n1.952313282028835\n1.930667725487024\n1.9091623334798618\n1.887798178501985\n1.8665763588529516\n1.8454979992710228\n1.8245642516892349\n1.8037762961004702\n1.7831353415134152\n1.7626426269797286\n1.7422994226680433\n1.722107030958447\n1.7020667875300062\n1.6821800624128078\n1.662448260977546\n1.6428728248360132\n1.6234552326294778\n1.6041970006839241\n1.585099683516083\n1.5661648741771348\n1.5473942044259037\n1.5287893447270129\n1.5103520040734837\n1.4920839296361281\n1.473986906244623\n1.4560627557067276\n1.438313335972417\n1.4207405401495217\n1.4033462953758915\n1.3861325615508449\n1.3691013299256982\n1.3522546215494677\n1.335594485562039\n1.3191229973229104\n1.3028422563597486\n1.2867543841174118\n1.270861521485012\n1.2551658260763083\n1.239669469237289\n1.224374632754292\n1.2092835052365472\n1.1943982781484674\n1.179721141469456\n1.1652542789622056\n1.1509998630343918\n1.1369600491831446\n1.1231369700165708\n1.1095327288517374\nCost after iteration 400: 1.109533\n1.0961493928938069\n1.0829889860063022\n1.0700534810877163\n1.0573447920747925\n1.0448647655977832\n1.0326151723178376\n1.0205976979813636\n1.0088139342308484\n0.9972653692161545\n0.9859533780548171\n0.9748792131943433\n0.9640439947339502\n0.9534487007675498\n0.9430941578140516\n0.9329810314051392\n0.9231098169044782\n0.9134808306357444\n0.9040942013997841\n0.8949498624635017\n0.8860475441045748\n0.8773867667966878\n0.8689668351194964\n0.8607868324758736\n0.852845616696022\n0.8451418166036664\n0.8376738296137077\n0.8304398204234057\n0.8234377208503422\n0.8166652308602163\n0.8101198208159979\n0.8037987349673041\n0.7976989961852916\n0.7918174119340805\n0.7861505814550833\n0.7806949041258823\n0.7754465889407741\n0.7704016650462212\n0.7655559932512148\n0.7609052784209333\n0.7564450826507862\n0.7521708391105891\n0.7480778664378502\n0.7441613835618692\n0.7404165248196307\n0.7368383552650094\n0.7334218859942759\n0.7301620895720632\n0.7270539154845865\n0.7240923082141676\n0.7212722370126112\n0.7185888011987985\n0.7160377395949407\n0.7136182808810879\n0.7113493045050585\n0.709362876303118\n0.7084501485622241\n0.7132529464436445\n0.7503809960225379\n0.946887223534221\n1.5425176483273193\n1.7982558587051052\n1.771809296223731\n1.6911118298889207\n1.8096013194463325\n1.6625939004849135\n1.8110697510398197\n1.654398241711752\n1.8052139953089847\n1.6504785592725024\n1.7981684557667592\n1.647310365156215\n1.7911044832088499\n1.6442084145578768\n1.78419661698865\n1.641057156737812\n1.7774398462306986\n1.6378467651679143\n1.7708076295652988\n1.634584617104046\n1.7642791001785034\n1.6312795619571216\n1.7578415629409168\n1.6279396256612366\n1.7514888435858644\n1.624572026289497\n1.7452193597962247\n1.6211834829260352\n1.7390345724126344\n1.6177804718411746\n1.7329378523262302\n1.6143693891701727\n1.726933684083554\n1.6109566341493806\n1.7210271191836843\n1.6075486341252883\n1.7152234068174774\n1.6041518301204745\n1.709527747410488\n1.6007726376845524\nè®­ç»ƒé›†å‡†ç¡®ç‡ï¼š()\næµ‹è¯•é›†å‡†ç¡®ç‡ï¼š()\n"
      }
     ],
     "persistentId": "b175609a-a7d0-4bf8-84b6-23b93d651481",
     "text": "if __name__ == '__main__':\n    model(X_train, y_train, X_test, y_test, num_iterations=500, learning_rate=0.01)\n"
    },
    "executionTime": "2020-02-04T05:12:01.973Z"
   },
   {
    "cell": {
     "executionCount": 34,
     "executionEventId": "763bb2af-35fd-4ac3-9a67-fefb6d75ab86",
     "hasError": false,
     "id": "2e14e518-c694-442e-8483-58f4a46d56e4",
     "outputs": [],
     "persistentId": "d2518a89-1eeb-44fb-95a8-4eab59019f67",
     "text": "def propagate(w,b,X,Y):\n    '''\n    Implement the cost function and its gradient for the propagation\n    \n    Arguments:\n    w - weights\n    b - bias\n    X - data\n    Y - ground truth\n    '''\n    m = X.shape[1]\n    A = sigmoid(np.dot(w.T,X) + b)\n    cost = -1/m * np.sum(Y * np.log(A) + (1-Y) * np.log(1-A))\n#     print(cost)\n    \n    dw = 1/m * np.dot(X,(A-Y).T)\n    db = 1/m * np.sum(A-Y)\n    \n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    cost = np.squeeze(cost)\n    assert(cost.shape == ())\n    \n    grads = {'dw':dw,\n             'db':db}\n    return grads, cost"
    },
    "executionTime": "2020-02-04T05:12:56.357Z"
   },
   {
    "cell": {
     "executionCount": 35,
     "executionEventId": "29f89346-774e-43b1-a505-2531323c2fa4",
     "hasError": false,
     "id": "f0685016-4bda-4ca9-9976-ae5f4793ece6",
     "outputs": [],
     "persistentId": "39737492-b457-4dcf-8d9f-76a392488151",
     "text": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost=True):\n    '''\n    This function optimize w and b by running a gradient descen algorithm\n    \n    Arguments:\n    w - weights\n    b - bias\n    X - data\n    Y - ground truth\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n    \n    Returns:\n    params - dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n    \n    '''\n    \n    costs = []\n    \n    for i in range(num_iterations):\n        \n        grads, cost = propagate(w,b,X,Y)\n        \n        dw = grads['dw']\n        db = grads['db']\n        \n        w = w - learning_rate * dw\n        b = b - learning_rate * db\n        \n        if i % 100 == 0:\n            costs.append(cost)\n        if print_cost and i % 100 == 0:\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    \n    params = {\"w\":w,\n              \"b\":b}\n    \n    grads = {\"dw\":dw,\n             \"db\":db}\n    \n    return params, grads, costs"
    },
    "executionTime": "2020-02-04T05:12:59.183Z"
   },
   {
    "cell": {
     "executionCount": 36,
     "executionEventId": "47e09653-83f6-42c4-939e-2a159c585792",
     "hasError": false,
     "id": "3dab0397-2b00-4236-ad61-4e8886ec359c",
     "outputs": [],
     "persistentId": "afe4201d-351d-44c7-af57-927730e72d92",
     "text": "def predict(w, b, X):\n    '''\n    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n    \n    Arguments:\n    w -- weights\n    b -- bias \n    X -- data \n    \n    Returns:\n    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n    '''\n    m = X.shape[1]\n    Y_prediction = np.zeros((1,m))\n    w = w.reshape(X.shape[0],1)\n    \n    A = sigmoid(np.dot(w.T,X) + b)\n    \n    for j in range(A.shape[1]):\n        if A[0,j] <= 0.5:\n            Y_prediction[0,j] = 0\n        else:\n            Y_prediction[0,j] = 1\n    \n    assert(Y_prediction.shape == (1,m))\n    \n    return Y_prediction"
    },
    "executionTime": "2020-02-04T05:13:02.029Z"
   },
   {
    "cell": {
     "executionCount": 37,
     "executionEventId": "bc6a019e-3826-4a92-9703-53c6fde538ce",
     "hasError": false,
     "id": "611bea7f-db28-443e-b57e-464cc0492891",
     "outputs": [],
     "persistentId": "736561b1-27f2-4958-a474-6d34622f80bb",
     "text": "def model(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate,print_cost=True):\n    \"\"\"\n    Build the logistic regression model by calling all the functions you have implemented.\n    Arguments:\n    X_train - training set\n    Y_train - training label\n    X_test - test set\n    Y_test - test label\n    num_iteration - hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n    \n    Returns:\n    d - dictionary should contain following information w,b,training_accuracy, test_accuracy,cost\n    eg: d = {\"w\":w,\n             \"b\":b,\n             \"training_accuracy\": traing_accuracy,\n             \"test_accuracy\":test_accuracy,\n             \"cost\":cost}\n    \"\"\"\n    # æ•°æ®ä¿®æ”¹ä¸ºä¸€ç»´\n    x_train = X_train.reshape(-1,X_train.shape[0])\n    y_train = Y_train.reshape(-1,Y_train.shape[0])\n    x_test = X_test.reshape(-1,X_test.shape[0])\n    y_test = Y_test.reshape(-1,Y_test.shape[0])\n    \n    # åˆå§‹åŒ–å‚æ•°\n    w,b = initialize_parameters(x_train.shape[0])\n    \n    # params:æ›´æ–°åçš„ç½‘ç»œå‚æ•°\n    # grads:æœ€æ–°çš„æ¢¯åº¦(ä¸‹é™æŸå¤±)\n    # costs:æ¯æ¬¡æ›´æ–°çš„æŸå¤±åˆ—è¡¨\n    params, grads, costs = optimize(w,b,x_train,y_train,num_iterations,learning_rate,print_cost)\n    \n    # è·å–è®­ç»ƒçš„å‚æ•°\n    w = params[\"w\"]\n    b = params[\"b\"]\n    \n    # é¢„æµ‹ç»“æœ\n    y_prediction_train = predict(w,b,x_train)\n    y_prediction_test = predict(w,b,x_test)\n    \n    # æ‰“å°å‡†ç¡®ç‡ï¼ŒæŸå¤±\n#     print(\"Logistic Loss is ()\".format(np.mean(np.abs(costs))))\n    print(\"è®­ç»ƒé›†å‡†ç¡®ç‡ï¼š()\".format(100 - np.mean(np.abs(y_prediction_train - y_train))))\n    print(\"æµ‹è¯•é›†å‡†ç¡®ç‡ï¼š()\".format(100 - np.mean(np.abs(y_prediction_test - y_test))))\n    \n    \n    \n    "
    },
    "executionTime": "2020-02-04T05:13:05.458Z"
   },
   {
    "cell": {
     "executionCount": 38,
     "executionEventId": "55bb2233-02f9-4cd5-807d-8e3752818810",
     "hasError": false,
     "id": "1fde88e1-df7e-49c2-ae1f-e396411d30e3",
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "Cost after iteration 0: nan\nCost after iteration 100: nan\nCost after iteration 200: nan\nCost after iteration 300: nan\nCost after iteration 400: 2.657163\nè®­ç»ƒé›†å‡†ç¡®ç‡ï¼š()\næµ‹è¯•é›†å‡†ç¡®ç‡ï¼š()\n"
      },
      {
       "name": "stderr",
       "output_type": "stream",
       "text": "/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: divide by zero encountered in log\n  del sys.path[0]\n/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in multiply\n  del sys.path[0]\n"
      }
     ],
     "persistentId": "b175609a-a7d0-4bf8-84b6-23b93d651481",
     "text": "if __name__ == '__main__':\n    model(X_train, y_train, X_test, y_test, num_iterations=500, learning_rate=0.01)\n"
    },
    "executionTime": "2020-02-04T05:13:06.876Z"
   },
   {
    "cell": {
     "executionCount": 39,
     "executionEventId": "85e96fd4-d287-41c0-b9d2-cc8dd33ae8bf",
     "hasError": false,
     "id": "1fde88e1-df7e-49c2-ae1f-e396411d30e3",
     "outputs": [
      {
       "name": "stderr",
       "output_type": "stream",
       "text": "/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: divide by zero encountered in log\n  del sys.path[0]\n/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in multiply\n  del sys.path[0]\n"
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "Cost after iteration 0: nan\nCost after iteration 100: nan\nCost after iteration 200: nan\nCost after iteration 300: nan\nCost after iteration 400: nan\nCost after iteration 500: 1.941211\nCost after iteration 600: 0.754729\nCost after iteration 700: 1.542411\nCost after iteration 800: 1.503588\nCost after iteration 900: 1.500523\nCost after iteration 1000: 1.500161\nCost after iteration 1100: 1.500106\nCost after iteration 1200: 1.500102\nCost after iteration 1300: 1.500108\nCost after iteration 1400: 1.500113\nCost after iteration 1500: 1.500118\nCost after iteration 1600: 1.500122\nCost after iteration 1700: 1.500126\nCost after iteration 1800: 1.500129\nCost after iteration 1900: 1.500132\nCost after iteration 2000: 1.500136\nCost after iteration 2100: 1.500139\nCost after iteration 2200: 1.500142\nCost after iteration 2300: 1.500146\nCost after iteration 2400: 1.500149\nCost after iteration 2500: 1.500152\nCost after iteration 2600: 1.500155\nCost after iteration 2700: 1.500159\nCost after iteration 2800: 1.500162\nCost after iteration 2900: 1.500165\nCost after iteration 3000: 1.500168\nCost after iteration 3100: 1.500172\nCost after iteration 3200: 1.500175\nCost after iteration 3300: 1.500178\nCost after iteration 3400: 1.500181\nCost after iteration 3500: 1.500185\nCost after iteration 3600: 1.500188\nCost after iteration 3700: 1.500191\nCost after iteration 3800: 1.500194\nCost after iteration 3900: 1.500198\nCost after iteration 4000: 1.500201\nCost after iteration 4100: 1.500204\nCost after iteration 4200: 1.500207\nCost after iteration 4300: 1.500210\nCost after iteration 4400: 1.500214\nCost after iteration 4500: 1.500217\nCost after iteration 4600: 1.500220\nCost after iteration 4700: 1.500223\nCost after iteration 4800: 1.500226\nCost after iteration 4900: 1.500230\nè®­ç»ƒé›†å‡†ç¡®ç‡ï¼š()\næµ‹è¯•é›†å‡†ç¡®ç‡ï¼š()\n"
      }
     ],
     "persistentId": "b175609a-a7d0-4bf8-84b6-23b93d651481",
     "text": "if __name__ == '__main__':\n    model(X_train, y_train, X_test, y_test, num_iterations=5000, learning_rate=0.01)\n"
    },
    "executionTime": "2020-02-04T05:13:39.530Z"
   },
   {
    "cell": {
     "executionCount": 40,
     "executionEventId": "da09a8e9-9e5a-4bbc-abf0-8ccc81c6fde2",
     "hasError": false,
     "id": "611bea7f-db28-443e-b57e-464cc0492891",
     "outputs": [],
     "persistentId": "736561b1-27f2-4958-a474-6d34622f80bb",
     "text": "def model(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate,print_cost=True):\n    \"\"\"\n    Build the logistic regression model by calling all the functions you have implemented.\n    Arguments:\n    X_train - training set\n    Y_train - training label\n    X_test - test set\n    Y_test - test label\n    num_iteration - hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n    \n    Returns:\n    d - dictionary should contain following information w,b,training_accuracy, test_accuracy,cost\n    eg: d = {\"w\":w,\n             \"b\":b,\n             \"training_accuracy\": traing_accuracy,\n             \"test_accuracy\":test_accuracy,\n             \"cost\":cost}\n    \"\"\"\n    # æ•°æ®ä¿®æ”¹ä¸ºä¸€ç»´\n    x_train = X_train.reshape(-1,X_train.shape[0])\n    y_train = Y_train.reshape(-1,Y_train.shape[0])\n    x_test = X_test.reshape(-1,X_test.shape[0])\n    y_test = Y_test.reshape(-1,Y_test.shape[0])\n    \n    # åˆå§‹åŒ–å‚æ•°\n    w,b = initialize_parameters(x_train.shape[0])\n    \n    # params:æ›´æ–°åçš„ç½‘ç»œå‚æ•°\n    # grads:æœ€æ–°çš„æ¢¯åº¦(ä¸‹é™æŸå¤±)\n    # costs:æ¯æ¬¡æ›´æ–°çš„æŸå¤±åˆ—è¡¨\n    params, grads, costs = optimize(w,b,x_train,y_train,num_iterations,learning_rate,print_cost)\n    \n    # è·å–è®­ç»ƒçš„å‚æ•°\n    w = params[\"w\"]\n    b = params[\"b\"]\n    \n    # é¢„æµ‹ç»“æœ\n    y_prediction_train = predict(w,b,x_train)\n    y_prediction_test = predict(w,b,x_test)\n    print(y_prediction_train)\n    \n    # æ‰“å°å‡†ç¡®ç‡ï¼ŒæŸå¤±\n#     print(\"Logistic Loss is ()\".format(np.mean(np.abs(costs))))\n#     print(\"è®­ç»ƒé›†å‡†ç¡®ç‡ï¼š()\".format(100 - np.mean(np.abs(y_prediction_train - y_train))))\n#     print(\"æµ‹è¯•é›†å‡†ç¡®ç‡ï¼š()\".format(100 - np.mean(np.abs(y_prediction_test - y_test))))\n    \n    \n    \n    "
    },
    "executionTime": "2020-02-04T05:14:23.099Z"
   },
   {
    "cell": {
     "executionCount": 41,
     "executionEventId": "dbd209fc-be7e-4634-849b-216c63469d10",
     "hasError": false,
     "id": "1fde88e1-df7e-49c2-ae1f-e396411d30e3",
     "outputs": [
      {
       "name": "stderr",
       "output_type": "stream",
       "text": "/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: divide by zero encountered in log\n  del sys.path[0]\n/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in multiply\n  del sys.path[0]\n"
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "Cost after iteration 0: nan\nCost after iteration 100: nan\nCost after iteration 200: nan\nCost after iteration 300: nan\nCost after iteration 400: 1.503430\nCost after iteration 500: 1.788924\nCost after iteration 600: 1.584540\nCost after iteration 700: 1.560191\nCost after iteration 800: 1.557274\nCost after iteration 900: 1.556602\nCost after iteration 1000: 1.556397\nCost after iteration 1100: 1.556325\nCost after iteration 1200: 1.556298\nCost after iteration 1300: 1.556285\nCost after iteration 1400: 1.556278\nCost after iteration 1500: 1.556273\nCost after iteration 1600: 1.556269\nCost after iteration 1700: 1.556265\nCost after iteration 1800: 1.556261\nCost after iteration 1900: 1.556257\nCost after iteration 2000: 1.556253\nCost after iteration 2100: 1.556249\nCost after iteration 2200: 1.556246\nCost after iteration 2300: 1.556242\nCost after iteration 2400: 1.556238\nCost after iteration 2500: 1.556234\nCost after iteration 2600: 1.556230\nCost after iteration 2700: 1.556227\nCost after iteration 2800: 1.556223\nCost after iteration 2900: 1.556219\nCost after iteration 3000: 1.556215\nCost after iteration 3100: 1.556211\nCost after iteration 3200: 1.556208\nCost after iteration 3300: 1.556204\nCost after iteration 3400: 1.556200\nCost after iteration 3500: 1.556196\nCost after iteration 3600: 1.556192\nCost after iteration 3700: 1.556189\nCost after iteration 3800: 1.556185\nCost after iteration 3900: 1.556181\nCost after iteration 4000: 1.556177\nCost after iteration 4100: 1.556174\nCost after iteration 4200: 1.556170\nCost after iteration 4300: 1.556166\nCost after iteration 4400: 1.556162\nCost after iteration 4500: 1.556158\nCost after iteration 4600: 1.556155\nCost after iteration 4700: 1.556151\nCost after iteration 4800: 1.556147\nCost after iteration 4900: 1.556143\n[[1. 1. 1. ... 1. 1. 1.]]\n"
      }
     ],
     "persistentId": "b175609a-a7d0-4bf8-84b6-23b93d651481",
     "text": "if __name__ == '__main__':\n    model(X_train, y_train, X_test, y_test, num_iterations=5000, learning_rate=0.01)\n"
    },
    "executionTime": "2020-02-04T05:14:24.190Z"
   },
   {
    "cell": {
     "executionCount": 42,
     "executionEventId": "9826c304-d396-4bd1-ae29-569707983a67",
     "hasError": false,
     "id": "1fde88e1-df7e-49c2-ae1f-e396411d30e3",
     "outputs": [
      {
       "name": "stderr",
       "output_type": "stream",
       "text": "/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: divide by zero encountered in log\n  del sys.path[0]\n/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in multiply\n  del sys.path[0]\n"
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "Cost after iteration 0: nan\nCost after iteration 100: 15.832886\nCost after iteration 200: 15.168856\nCost after iteration 300: 15.050150\nCost after iteration 400: 14.924186\nCost after iteration 500: 14.879597\nCost after iteration 600: 14.875267\nCost after iteration 700: 14.878194\nCost after iteration 800: 14.881734\nCost after iteration 900: 14.884907\nCost after iteration 1000: 14.887420\nCost after iteration 1100: 14.889289\nCost after iteration 1200: 14.890538\nCost after iteration 1300: 14.891490\nCost after iteration 1400: 14.892061\nCost after iteration 1500: 14.892439\nCost after iteration 1600: 14.892686\nCost after iteration 1700: 14.892848\nCost after iteration 1800: 14.892953\nCost after iteration 1900: 14.893022\nCost after iteration 2000: 14.893069\nCost after iteration 2100: 14.893102\nCost after iteration 2200: 14.893128\nCost after iteration 2300: 14.893149\nCost after iteration 2400: 14.893168\nCost after iteration 2500: 14.893187\nCost after iteration 2600: 14.893206\nCost after iteration 2700: 14.893225\nCost after iteration 2800: 14.893246\nCost after iteration 2900: 14.893267\nCost after iteration 3000: 14.893289\nCost after iteration 3100: 14.893311\nCost after iteration 3200: 14.893335\nCost after iteration 3300: 14.893358\nCost after iteration 3400: 14.893382\nCost after iteration 3500: 14.893407\nCost after iteration 3600: 14.893432\nCost after iteration 3700: 14.893457\nCost after iteration 3800: 14.893482\nCost after iteration 3900: 14.893508\nCost after iteration 4000: 14.893533\nCost after iteration 4100: 14.893559\nCost after iteration 4200: 14.893585\nCost after iteration 4300: 14.893611\nCost after iteration 4400: 14.893636\nCost after iteration 4500: 14.893662\nCost after iteration 4600: 14.893688\nCost after iteration 4700: 14.893714\nCost after iteration 4800: 14.893740\nCost after iteration 4900: 14.893766\n[[0. 0. 0. ... 0. 0. 0.]]\n"
      }
     ],
     "persistentId": "b175609a-a7d0-4bf8-84b6-23b93d651481",
     "text": "if __name__ == '__main__':\n    model(X_train, y_train, X_test, y_test, num_iterations=5000, learning_rate=0.1)\n"
    },
    "executionTime": "2020-02-04T05:17:28.535Z"
   },
   {
    "cell": {
     "executionCount": 43,
     "executionEventId": "271aa840-084a-488a-b101-9d70eb60cc2d",
     "hasError": false,
     "id": "1fde88e1-df7e-49c2-ae1f-e396411d30e3",
     "outputs": [
      {
       "name": "stderr",
       "output_type": "stream",
       "text": "/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: divide by zero encountered in log\n  del sys.path[0]\n/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in multiply\n  del sys.path[0]\n"
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "Cost after iteration 0: nan\nCost after iteration 100: nan\nCost after iteration 200: nan\nCost after iteration 300: nan\nCost after iteration 400: nan\nCost after iteration 500: nan\nCost after iteration 600: nan\nCost after iteration 700: nan\nCost after iteration 800: nan\nCost after iteration 900: nan\nCost after iteration 1000: nan\nCost after iteration 1100: nan\nCost after iteration 1200: nan\nCost after iteration 1300: nan\nCost after iteration 1400: nan\nCost after iteration 1500: nan\nCost after iteration 1600: nan\nCost after iteration 1700: nan\nCost after iteration 1800: nan\nCost after iteration 1900: nan\nCost after iteration 2000: nan\nCost after iteration 2100: nan\nCost after iteration 2200: nan\nCost after iteration 2300: nan\nCost after iteration 2400: nan\nCost after iteration 2500: nan\nCost after iteration 2600: nan\nCost after iteration 2700: nan\nCost after iteration 2800: nan\nCost after iteration 2900: nan\nCost after iteration 3000: nan\nCost after iteration 3100: nan\nCost after iteration 3200: nan\nCost after iteration 3300: nan\nCost after iteration 3400: nan\nCost after iteration 3500: nan\nCost after iteration 3600: nan\nCost after iteration 3700: nan\nCost after iteration 3800: nan\nCost after iteration 3900: nan\nCost after iteration 4000: nan\nCost after iteration 4100: nan\nCost after iteration 4200: nan\nCost after iteration 4300: nan\nCost after iteration 4400: nan\nCost after iteration 4500: nan\nCost after iteration 4600: nan\nCost after iteration 4700: nan\nCost after iteration 4800: nan\nCost after iteration 4900: nan\n[[1. 1. 1. ... 1. 1. 1.]]\n"
      }
     ],
     "persistentId": "b175609a-a7d0-4bf8-84b6-23b93d651481",
     "text": "if __name__ == '__main__':\n    model(X_train, y_train, X_test, y_test, num_iterations=5000, learning_rate=0.5)\n"
    },
    "executionTime": "2020-02-04T05:17:34.697Z"
   },
   {
    "cell": {
     "executionCount": 44,
     "executionEventId": "f469da19-c507-4575-84bb-f85478ed5fd0",
     "hasError": false,
     "id": "1fde88e1-df7e-49c2-ae1f-e396411d30e3",
     "outputs": [
      {
       "name": "stderr",
       "output_type": "stream",
       "text": "/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: divide by zero encountered in log\n  del sys.path[0]\n/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in multiply\n  del sys.path[0]\n"
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "Cost after iteration 0: nan\nCost after iteration 100: nan\nCost after iteration 200: nan\nCost after iteration 300: nan\nCost after iteration 400: nan\nCost after iteration 500: nan\nCost after iteration 600: nan\nCost after iteration 700: 4.063871\nCost after iteration 800: 2.774811\nCost after iteration 900: 1.709297\nCost after iteration 1000: 0.979613\nCost after iteration 1100: 0.704909\nCost after iteration 1200: 0.673904\nCost after iteration 1300: 0.672385\nCost after iteration 1400: 0.672316\nCost after iteration 1500: 0.672313\nCost after iteration 1600: 0.672312\nCost after iteration 1700: 0.672312\nCost after iteration 1800: 0.672312\nCost after iteration 1900: 0.672311\nCost after iteration 2000: 0.672311\nCost after iteration 2100: 0.672311\nCost after iteration 2200: 0.672311\nCost after iteration 2300: 0.672310\nCost after iteration 2400: 0.672310\nCost after iteration 2500: 0.672310\nCost after iteration 2600: 0.672310\nCost after iteration 2700: 0.672309\nCost after iteration 2800: 0.672309\nCost after iteration 2900: 0.672309\nCost after iteration 3000: 0.672309\nCost after iteration 3100: 0.672308\nCost after iteration 3200: 0.672308\nCost after iteration 3300: 0.672308\nCost after iteration 3400: 0.672308\nCost after iteration 3500: 0.672307\nCost after iteration 3600: 0.672307\nCost after iteration 3700: 0.672307\nCost after iteration 3800: 0.672307\nCost after iteration 3900: 0.672306\nCost after iteration 4000: 0.672306\nCost after iteration 4100: 0.672306\nCost after iteration 4200: 0.672306\nCost after iteration 4300: 0.672305\nCost after iteration 4400: 0.672305\nCost after iteration 4500: 0.672305\nCost after iteration 4600: 0.672305\nCost after iteration 4700: 0.672304\nCost after iteration 4800: 0.672304\nCost after iteration 4900: 0.672304\n[[0. 0. 0. ... 1. 0. 1.]]\n"
      }
     ],
     "persistentId": "b175609a-a7d0-4bf8-84b6-23b93d651481",
     "text": "if __name__ == '__main__':\n    model(X_train, y_train, X_test, y_test, num_iterations=5000, learning_rate=0.005)\n"
    },
    "executionTime": "2020-02-04T05:17:43.681Z"
   },
   {
    "cell": {
     "executionCount": 45,
     "executionEventId": "776b32da-5d8d-41d9-81e2-06fc99f693b7",
     "hasError": false,
     "id": "611bea7f-db28-443e-b57e-464cc0492891",
     "outputs": [],
     "persistentId": "736561b1-27f2-4958-a474-6d34622f80bb",
     "text": "def model(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate,print_cost=True):\n    \"\"\"\n    Build the logistic regression model by calling all the functions you have implemented.\n    Arguments:\n    X_train - training set\n    Y_train - training label\n    X_test - test set\n    Y_test - test label\n    num_iteration - hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n    \n    Returns:\n    d - dictionary should contain following information w,b,training_accuracy, test_accuracy,cost\n    eg: d = {\"w\":w,\n             \"b\":b,\n             \"training_accuracy\": traing_accuracy,\n             \"test_accuracy\":test_accuracy,\n             \"cost\":cost}\n    \"\"\"\n    # æ•°æ®ä¿®æ”¹ä¸ºä¸€ç»´\n    x_train = X_train.reshape(-1,X_train.shape[0])\n    y_train = Y_train.reshape(-1,Y_train.shape[0])\n    x_test = X_test.reshape(-1,X_test.shape[0])\n    y_test = Y_test.reshape(-1,Y_test.shape[0])\n    \n    # åˆå§‹åŒ–å‚æ•°\n    w,b = initialize_parameters(x_train.shape[0])\n    \n    # params:æ›´æ–°åçš„ç½‘ç»œå‚æ•°\n    # grads:æœ€æ–°çš„æ¢¯åº¦(ä¸‹é™æŸå¤±)\n    # costs:æ¯æ¬¡æ›´æ–°çš„æŸå¤±åˆ—è¡¨\n    params, grads, costs = optimize(w,b,x_train,y_train,num_iterations,learning_rate,print_cost)\n    \n    # è·å–è®­ç»ƒçš„å‚æ•°\n    w = params[\"w\"]\n    b = params[\"b\"]\n    \n    # é¢„æµ‹ç»“æœ\n    y_prediction_train = predict(w,b,x_train)\n    y_prediction_test = predict(w,b,x_test)\n    print(y_prediction_train)\n    \n    # æ‰“å°å‡†ç¡®ç‡ï¼ŒæŸå¤±\n#     print(\"Logistic Loss is ()\".format(np.mean(np.abs(costs))))\n    print(\"è®­ç»ƒé›†å‡†ç¡®ç‡ï¼š()\".format(100 - np.mean(np.abs(y_prediction_train - y_train))))\n    print(\"æµ‹è¯•é›†å‡†ç¡®ç‡ï¼š()\".format(100 - np.mean(np.abs(y_prediction_test - y_test))))\n    \n    \n    \n    "
    },
    "executionTime": "2020-02-04T05:18:08.037Z"
   },
   {
    "cell": {
     "executionCount": 46,
     "executionEventId": "03c72f3c-2320-49a7-b636-5ac4df52178d",
     "hasError": false,
     "id": "1fde88e1-df7e-49c2-ae1f-e396411d30e3",
     "outputs": [
      {
       "name": "stderr",
       "output_type": "stream",
       "text": "/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: divide by zero encountered in log\n  del sys.path[0]\n/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in multiply\n  del sys.path[0]\n"
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "Cost after iteration 0: nan\nCost after iteration 100: nan\nCost after iteration 200: nan\nCost after iteration 300: nan\nCost after iteration 400: nan\nCost after iteration 500: nan\nCost after iteration 600: nan\nCost after iteration 700: nan\nCost after iteration 800: 3.326833\nCost after iteration 900: 2.284166\nCost after iteration 1000: 1.400701\nCost after iteration 1100: 0.836438\nCost after iteration 1200: 0.684540\nCost after iteration 1300: 0.672888\nCost after iteration 1400: 0.672341\nCost after iteration 1500: 0.672314\nCost after iteration 1600: 0.672313\nCost after iteration 1700: 0.672312\nCost after iteration 1800: 0.672312\nCost after iteration 1900: 0.672312\nCost after iteration 2000: 0.672311\nCost after iteration 2100: 0.672311\nCost after iteration 2200: 0.672311\nCost after iteration 2300: 0.672311\nCost after iteration 2400: 0.672310\nCost after iteration 2500: 0.672310\nCost after iteration 2600: 0.672310\nCost after iteration 2700: 0.672310\nCost after iteration 2800: 0.672309\nCost after iteration 2900: 0.672309\nCost after iteration 3000: 0.672309\nCost after iteration 3100: 0.672309\nCost after iteration 3200: 0.672308\nCost after iteration 3300: 0.672308\nCost after iteration 3400: 0.672308\nCost after iteration 3500: 0.672308\nCost after iteration 3600: 0.672307\nCost after iteration 3700: 0.672307\nCost after iteration 3800: 0.672307\nCost after iteration 3900: 0.672307\nCost after iteration 4000: 0.672306\nCost after iteration 4100: 0.672306\nCost after iteration 4200: 0.672306\nCost after iteration 4300: 0.672306\nCost after iteration 4400: 0.672305\nCost after iteration 4500: 0.672305\nCost after iteration 4600: 0.672305\nCost after iteration 4700: 0.672305\nCost after iteration 4800: 0.672304\nCost after iteration 4900: 0.672304\n[[0. 0. 0. ... 1. 0. 1.]]\nè®­ç»ƒé›†å‡†ç¡®ç‡ï¼š()\næµ‹è¯•é›†å‡†ç¡®ç‡ï¼š()\n"
      }
     ],
     "persistentId": "b175609a-a7d0-4bf8-84b6-23b93d651481",
     "text": "if __name__ == '__main__':\n    model(X_train, y_train, X_test, y_test, num_iterations=5000, learning_rate=0.005)\n"
    },
    "executionTime": "2020-02-04T05:18:09.053Z"
   },
   {
    "cell": {
     "executionCount": 47,
     "executionEventId": "7496c1fc-6a8d-41fc-8824-1acbcd9e1ac7",
     "hasError": false,
     "id": "611bea7f-db28-443e-b57e-464cc0492891",
     "outputs": [],
     "persistentId": "736561b1-27f2-4958-a474-6d34622f80bb",
     "text": "def model(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate,print_cost=True):\n    \"\"\"\n    Build the logistic regression model by calling all the functions you have implemented.\n    Arguments:\n    X_train - training set\n    Y_train - training label\n    X_test - test set\n    Y_test - test label\n    num_iteration - hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n    \n    Returns:\n    d - dictionary should contain following information w,b,training_accuracy, test_accuracy,cost\n    eg: d = {\"w\":w,\n             \"b\":b,\n             \"training_accuracy\": traing_accuracy,\n             \"test_accuracy\":test_accuracy,\n             \"cost\":cost}\n    \"\"\"\n    # æ•°æ®ä¿®æ”¹ä¸ºä¸€ç»´\n    x_train = X_train.reshape(-1,X_train.shape[0])\n    y_train = Y_train.reshape(-1,Y_train.shape[0])\n    x_test = X_test.reshape(-1,X_test.shape[0])\n    y_test = Y_test.reshape(-1,Y_test.shape[0])\n    \n    # åˆå§‹åŒ–å‚æ•°\n    w,b = initialize_parameters(x_train.shape[0])\n    \n    # params:æ›´æ–°åçš„ç½‘ç»œå‚æ•°\n    # grads:æœ€æ–°çš„æ¢¯åº¦(ä¸‹é™æŸå¤±)\n    # costs:æ¯æ¬¡æ›´æ–°çš„æŸå¤±åˆ—è¡¨\n    params, grads, costs = optimize(w,b,x_train,y_train,num_iterations,learning_rate,print_cost)\n    \n    # è·å–è®­ç»ƒçš„å‚æ•°\n    w = params[\"w\"]\n    b = params[\"b\"]\n    \n    # é¢„æµ‹ç»“æœ\n    y_prediction_train = predict(w,b,x_train)\n    y_prediction_test = predict(w,b,x_test)\n#     print(y_prediction_train)\n    \n    print(np.sum(y_prediction_train == y_train))\n    \n    # æ‰“å°å‡†ç¡®ç‡ï¼ŒæŸå¤±\n#     print(\"Logistic Loss is ()\".format(np.mean(np.abs(costs))))\n#     print(\"è®­ç»ƒé›†å‡†ç¡®ç‡ï¼š()\".format(100 - np.mean(np.abs(y_prediction_train - y_train))))\n#     print(\"æµ‹è¯•é›†å‡†ç¡®ç‡ï¼š()\".format(100 - np.mean(np.abs(y_prediction_test - y_test))))\n    \n    \n    \n    "
    },
    "executionTime": "2020-02-04T05:19:24.171Z"
   },
   {
    "cell": {
     "executionCount": 48,
     "executionEventId": "76e5118a-a76e-4dd3-8af7-dbb670ca92eb",
     "hasError": false,
     "id": "1fde88e1-df7e-49c2-ae1f-e396411d30e3",
     "outputs": [
      {
       "name": "stderr",
       "output_type": "stream",
       "text": "/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: divide by zero encountered in log\n  del sys.path[0]\n/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in multiply\n  del sys.path[0]\n"
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "Cost after iteration 0: nan\nCost after iteration 100: nan\nCost after iteration 200: nan\nCost after iteration 300: nan\nCost after iteration 400: nan\nCost after iteration 500: nan\nCost after iteration 600: nan\nCost after iteration 700: inf\nCost after iteration 800: 3.595031\nCost after iteration 900: 2.418356\nCost after iteration 1000: 1.413759\nCost after iteration 1100: 0.792260\nCost after iteration 1200: 0.677276\nCost after iteration 1300: 0.672496\nCost after iteration 1400: 0.672327\nCost after iteration 1500: 0.672320\nCost after iteration 1600: 0.672320\nCost after iteration 1700: 0.672319\nCost after iteration 1800: 0.672319\nCost after iteration 1900: 0.672319\nCost after iteration 2000: 0.672319\nCost after iteration 2100: 0.672318\nCost after iteration 2200: 0.672318\nCost after iteration 2300: 0.672318\nCost after iteration 2400: 0.672318\nCost after iteration 2500: 0.672317\nCost after iteration 2600: 0.672317\nCost after iteration 2700: 0.672317\nCost after iteration 2800: 0.672316\nCost after iteration 2900: 0.672316\nCost after iteration 3000: 0.672316\nCost after iteration 3100: 0.672316\nCost after iteration 3200: 0.672315\nCost after iteration 3300: 0.672315\nCost after iteration 3400: 0.672315\nCost after iteration 3500: 0.672315\nCost after iteration 3600: 0.672314\nCost after iteration 3700: 0.672314\nCost after iteration 3800: 0.672314\nCost after iteration 3900: 0.672313\nCost after iteration 4000: 0.672313\nCost after iteration 4100: 0.672313\nCost after iteration 4200: 0.672313\nCost after iteration 4300: 0.672312\nCost after iteration 4400: 0.672312\nCost after iteration 4500: 0.672312\nCost after iteration 4600: 0.672312\nCost after iteration 4700: 0.672311\nCost after iteration 4800: 0.672311\nCost after iteration 4900: 0.672311\n778\n"
      }
     ],
     "persistentId": "b175609a-a7d0-4bf8-84b6-23b93d651481",
     "text": "if __name__ == '__main__':\n    model(X_train, y_train, X_test, y_test, num_iterations=5000, learning_rate=0.005)\n"
    },
    "executionTime": "2020-02-04T05:19:25.143Z"
   },
   {
    "cell": {
     "executionCount": 49,
     "executionEventId": "1889bea4-0413-467e-ba92-5060044edc49",
     "hasError": false,
     "id": "611bea7f-db28-443e-b57e-464cc0492891",
     "outputs": [],
     "persistentId": "736561b1-27f2-4958-a474-6d34622f80bb",
     "text": "def model(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate,print_cost=True):\n    \"\"\"\n    Build the logistic regression model by calling all the functions you have implemented.\n    Arguments:\n    X_train - training set\n    Y_train - training label\n    X_test - test set\n    Y_test - test label\n    num_iteration - hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n    \n    Returns:\n    d - dictionary should contain following information w,b,training_accuracy, test_accuracy,cost\n    eg: d = {\"w\":w,\n             \"b\":b,\n             \"training_accuracy\": traing_accuracy,\n             \"test_accuracy\":test_accuracy,\n             \"cost\":cost}\n    \"\"\"\n    # æ•°æ®ä¿®æ”¹ä¸ºä¸€ç»´\n    x_train = X_train.reshape(-1,X_train.shape[0])\n    y_train = Y_train.reshape(-1,Y_train.shape[0])\n    x_test = X_test.reshape(-1,X_test.shape[0])\n    y_test = Y_test.reshape(-1,Y_test.shape[0])\n    \n    # åˆå§‹åŒ–å‚æ•°\n    w,b = initialize_parameters(x_train.shape[0])\n    \n    # params:æ›´æ–°åçš„ç½‘ç»œå‚æ•°\n    # grads:æœ€æ–°çš„æ¢¯åº¦(ä¸‹é™æŸå¤±)\n    # costs:æ¯æ¬¡æ›´æ–°çš„æŸå¤±åˆ—è¡¨\n    params, grads, costs = optimize(w,b,x_train,y_train,num_iterations,learning_rate,print_cost)\n    \n    # è·å–è®­ç»ƒçš„å‚æ•°\n    w = params[\"w\"]\n    b = params[\"b\"]\n    \n    # é¢„æµ‹ç»“æœ\n    y_prediction_train = predict(w,b,x_train)\n    y_prediction_test = predict(w,b,x_test)\n#     print(y_prediction_train)\n    \n    print(np.sum(y_prediction_train == y_train)/ len(y_train))\n    \n    # æ‰“å°å‡†ç¡®ç‡ï¼ŒæŸå¤±\n#     print(\"Logistic Loss is ()\".format(np.mean(np.abs(costs))))\n#     print(\"è®­ç»ƒé›†å‡†ç¡®ç‡ï¼š()\".format(100 - np.mean(np.abs(y_prediction_train - y_train))))\n#     print(\"æµ‹è¯•é›†å‡†ç¡®ç‡ï¼š()\".format(100 - np.mean(np.abs(y_prediction_test - y_test))))\n    \n    \n    \n    "
    },
    "executionTime": "2020-02-04T05:20:09.157Z"
   },
   {
    "cell": {
     "executionCount": 50,
     "executionEventId": "0cfca6e6-2189-4d3e-8489-e2f8f54fe577",
     "hasError": false,
     "id": "1fde88e1-df7e-49c2-ae1f-e396411d30e3",
     "outputs": [
      {
       "name": "stderr",
       "output_type": "stream",
       "text": "/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: divide by zero encountered in log\n  del sys.path[0]\n/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in multiply\n  del sys.path[0]\n"
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "Cost after iteration 0: nan\nCost after iteration 100: nan\nCost after iteration 200: nan\nCost after iteration 300: nan\nCost after iteration 400: nan\nCost after iteration 500: 3.508420\nCost after iteration 600: 2.370409\nCost after iteration 700: 1.474854\nCost after iteration 800: 0.886012\nCost after iteration 900: 0.691177\nCost after iteration 1000: 0.673220\nCost after iteration 1100: 0.672342\nCost after iteration 1200: 0.672299\nCost after iteration 1300: 0.672297\nCost after iteration 1400: 0.672296\nCost after iteration 1500: 0.672296\nCost after iteration 1600: 0.672296\nCost after iteration 1700: 0.672296\nCost after iteration 1800: 0.672296\nCost after iteration 1900: 0.672295\nCost after iteration 2000: 0.672295\nCost after iteration 2100: 0.672295\nCost after iteration 2200: 0.672295\nCost after iteration 2300: 0.672294\nCost after iteration 2400: 0.672294\nCost after iteration 2500: 0.672294\nCost after iteration 2600: 0.672294\nCost after iteration 2700: 0.672294\nCost after iteration 2800: 0.672293\nCost after iteration 2900: 0.672293\nCost after iteration 3000: 0.672293\nCost after iteration 3100: 0.672293\nCost after iteration 3200: 0.672293\nCost after iteration 3300: 0.672292\nCost after iteration 3400: 0.672292\nCost after iteration 3500: 0.672292\nCost after iteration 3600: 0.672292\nCost after iteration 3700: 0.672291\nCost after iteration 3800: 0.672291\nCost after iteration 3900: 0.672291\nCost after iteration 4000: 0.672291\nCost after iteration 4100: 0.672291\nCost after iteration 4200: 0.672290\nCost after iteration 4300: 0.672290\nCost after iteration 4400: 0.672290\nCost after iteration 4500: 0.672290\nCost after iteration 4600: 0.672290\nCost after iteration 4700: 0.672289\nCost after iteration 4800: 0.672289\nCost after iteration 4900: 0.672289\n779.0\n"
      }
     ],
     "persistentId": "b175609a-a7d0-4bf8-84b6-23b93d651481",
     "text": "if __name__ == '__main__':\n    model(X_train, y_train, X_test, y_test, num_iterations=5000, learning_rate=0.005)\n"
    },
    "executionTime": "2020-02-04T05:20:10.198Z"
   },
   {
    "cell": {
     "executionCount": 51,
     "executionEventId": "e7f4c1ef-8a8b-4929-acba-5743ab803e60",
     "hasError": false,
     "id": "611bea7f-db28-443e-b57e-464cc0492891",
     "outputs": [],
     "persistentId": "736561b1-27f2-4958-a474-6d34622f80bb",
     "text": "def model(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate,print_cost=True):\n    \"\"\"\n    Build the logistic regression model by calling all the functions you have implemented.\n    Arguments:\n    X_train - training set\n    Y_train - training label\n    X_test - test set\n    Y_test - test label\n    num_iteration - hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n    \n    Returns:\n    d - dictionary should contain following information w,b,training_accuracy, test_accuracy,cost\n    eg: d = {\"w\":w,\n             \"b\":b,\n             \"training_accuracy\": traing_accuracy,\n             \"test_accuracy\":test_accuracy,\n             \"cost\":cost}\n    \"\"\"\n    # æ•°æ®ä¿®æ”¹ä¸ºä¸€ç»´\n    x_train = X_train.reshape(-1,X_train.shape[0])\n    y_train = Y_train.reshape(-1,Y_train.shape[0])\n    x_test = X_test.reshape(-1,X_test.shape[0])\n    y_test = Y_test.reshape(-1,Y_test.shape[0])\n    \n    # åˆå§‹åŒ–å‚æ•°\n    w,b = initialize_parameters(x_train.shape[0])\n    \n    # params:æ›´æ–°åçš„ç½‘ç»œå‚æ•°\n    # grads:æœ€æ–°çš„æ¢¯åº¦(ä¸‹é™æŸå¤±)\n    # costs:æ¯æ¬¡æ›´æ–°çš„æŸå¤±åˆ—è¡¨\n    params, grads, costs = optimize(w,b,x_train,y_train,num_iterations,learning_rate,print_cost)\n    \n    # è·å–è®­ç»ƒçš„å‚æ•°\n    w = params[\"w\"]\n    b = params[\"b\"]\n    \n    # é¢„æµ‹ç»“æœ\n    y_prediction_train = predict(w,b,x_train)\n    y_prediction_test = predict(w,b,x_test)\n#     print(y_prediction_train)\n    \n    print(np.sum(y_prediction_train == y_train)/ len(y_train))\n    \n    # æ‰“å°å‡†ç¡®ç‡ï¼ŒæŸå¤±\n#     print(\"Logistic Loss is ()\".format(np.mean(np.abs(costs))))\n#     print(\"è®­ç»ƒé›†å‡†ç¡®ç‡ï¼š()\".format(100 - np.mean(np.abs(y_prediction_train - y_train))))\n#     print(\"æµ‹è¯•é›†å‡†ç¡®ç‡ï¼š()\".format(100 - np.mean(np.abs(y_prediction_test - y_test))))\n    \n    \n    \n    "
    },
    "executionTime": "2020-02-04T05:20:44.289Z"
   },
   {
    "cell": {
     "executionCount": 52,
     "executionEventId": "0f89ad59-d2f5-4a57-9d90-4d8d23da50cc",
     "hasError": false,
     "id": "1fde88e1-df7e-49c2-ae1f-e396411d30e3",
     "outputs": [
      {
       "name": "stderr",
       "output_type": "stream",
       "text": "/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: divide by zero encountered in log\n  del sys.path[0]\n/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in multiply\n  del sys.path[0]\n"
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "Cost after iteration 0: nan\nCost after iteration 100: nan\nCost after iteration 200: nan\nCost after iteration 300: nan\nCost after iteration 400: nan\nCost after iteration 500: nan\nCost after iteration 600: nan\nCost after iteration 700: nan\nCost after iteration 800: 2.702912\nCost after iteration 900: 1.643708\nCost after iteration 1000: 0.910413\nCost after iteration 1100: 0.688425\nCost after iteration 1200: 0.672913\nCost after iteration 1300: 0.672306\nCost after iteration 1400: 0.672281\nCost after iteration 1500: 0.672280\nCost after iteration 1600: 0.672279\nCost after iteration 1700: 0.672279\nCost after iteration 1800: 0.672279\nCost after iteration 1900: 0.672279\nCost after iteration 2000: 0.672279\nCost after iteration 2100: 0.672278\nCost after iteration 2200: 0.672278\nCost after iteration 2300: 0.672278\nCost after iteration 2400: 0.672278\nCost after iteration 2500: 0.672278\nCost after iteration 2600: 0.672278\nCost after iteration 2700: 0.672277\nCost after iteration 2800: 0.672277\nCost after iteration 2900: 0.672277\nCost after iteration 3000: 0.672277\nCost after iteration 3100: 0.672277\nCost after iteration 3200: 0.672277\nCost after iteration 3300: 0.672276\nCost after iteration 3400: 0.672276\nCost after iteration 3500: 0.672276\nCost after iteration 3600: 0.672276\nCost after iteration 3700: 0.672276\nCost after iteration 3800: 0.672276\nCost after iteration 3900: 0.672275\nCost after iteration 4000: 0.672275\nCost after iteration 4100: 0.672275\nCost after iteration 4200: 0.672275\nCost after iteration 4300: 0.672275\nCost after iteration 4400: 0.672275\nCost after iteration 4500: 0.672274\nCost after iteration 4600: 0.672274\nCost after iteration 4700: 0.672274\nCost after iteration 4800: 0.672274\nCost after iteration 4900: 0.672274\n780.0\n"
      }
     ],
     "persistentId": "b175609a-a7d0-4bf8-84b6-23b93d651481",
     "text": "if __name__ == '__main__':\n    model(X_train, y_train, X_test, y_test, num_iterations=5000, learning_rate=0.005)\n"
    },
    "executionTime": "2020-02-04T05:20:45.767Z"
   },
   {
    "cell": {
     "executionCount": 53,
     "executionEventId": "a39f007d-b668-4abf-8e75-074e267306dc",
     "hasError": false,
     "id": "611bea7f-db28-443e-b57e-464cc0492891",
     "outputs": [],
     "persistentId": "736561b1-27f2-4958-a474-6d34622f80bb",
     "text": "def model(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate,print_cost=True):\n    \"\"\"\n    Build the logistic regression model by calling all the functions you have implemented.\n    Arguments:\n    X_train - training set\n    Y_train - training label\n    X_test - test set\n    Y_test - test label\n    num_iteration - hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n    \n    Returns:\n    d - dictionary should contain following information w,b,training_accuracy, test_accuracy,cost\n    eg: d = {\"w\":w,\n             \"b\":b,\n             \"training_accuracy\": traing_accuracy,\n             \"test_accuracy\":test_accuracy,\n             \"cost\":cost}\n    \"\"\"\n    # æ•°æ®ä¿®æ”¹ä¸ºä¸€ç»´\n    x_train = X_train.reshape(-1,X_train.shape[0])\n    y_train = Y_train.reshape(-1,Y_train.shape[0])\n    x_test = X_test.reshape(-1,X_test.shape[0])\n    y_test = Y_test.reshape(-1,Y_test.shape[0])\n    \n    # åˆå§‹åŒ–å‚æ•°\n    w,b = initialize_parameters(x_train.shape[0])\n    \n    # params:æ›´æ–°åçš„ç½‘ç»œå‚æ•°\n    # grads:æœ€æ–°çš„æ¢¯åº¦(ä¸‹é™æŸå¤±)\n    # costs:æ¯æ¬¡æ›´æ–°çš„æŸå¤±åˆ—è¡¨\n    params, grads, costs = optimize(w,b,x_train,y_train,num_iterations,learning_rate,print_cost)\n    \n    # è·å–è®­ç»ƒçš„å‚æ•°\n    w = params[\"w\"]\n    b = params[\"b\"]\n    \n    # é¢„æµ‹ç»“æœ\n    y_prediction_train = predict(w,b,x_train)\n    y_prediction_test = predict(w,b,x_test)\n#     print(y_prediction_train)\n    \n    count_right = np.sum(y_prediction_train == y_train)\n    accuracy = count_right / 100\n    print(accuracy)\n    \n    # æ‰“å°å‡†ç¡®ç‡ï¼ŒæŸå¤±\n#     print(\"Logistic Loss is ()\".format(np.mean(np.abs(costs))))\n#     print(\"è®­ç»ƒé›†å‡†ç¡®ç‡ï¼š()\".format(100 - np.mean(np.abs(y_prediction_train - y_train))))\n#     print(\"æµ‹è¯•é›†å‡†ç¡®ç‡ï¼š()\".format(100 - np.mean(np.abs(y_prediction_test - y_test))))\n    \n    \n    \n    "
    },
    "executionTime": "2020-02-04T05:21:49.987Z"
   },
   {
    "cell": {
     "executionCount": 54,
     "executionEventId": "fe5b9541-9fae-43ba-9dc0-b7f77749512a",
     "hasError": false,
     "id": "1fde88e1-df7e-49c2-ae1f-e396411d30e3",
     "outputs": [
      {
       "name": "stderr",
       "output_type": "stream",
       "text": "/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: divide by zero encountered in log\n  del sys.path[0]\n/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in multiply\n  del sys.path[0]\n"
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "Cost after iteration 0: nan\nCost after iteration 100: nan\nCost after iteration 200: nan\nCost after iteration 300: nan\nCost after iteration 400: nan\nCost after iteration 500: nan\nCost after iteration 600: nan\nCost after iteration 700: nan\nCost after iteration 800: 2.320940\nCost after iteration 900: 1.491825\nCost after iteration 1000: 0.901324\nCost after iteration 1100: 0.692798\nCost after iteration 1200: 0.673290\nCost after iteration 1300: 0.672359\nCost after iteration 1400: 0.672313\nCost after iteration 1500: 0.672311\nCost after iteration 1600: 0.672310\nCost after iteration 1700: 0.672310\nCost after iteration 1800: 0.672310\nCost after iteration 1900: 0.672310\nCost after iteration 2000: 0.672309\nCost after iteration 2100: 0.672309\nCost after iteration 2200: 0.672309\nCost after iteration 2300: 0.672309\nCost after iteration 2400: 0.672308\nCost after iteration 2500: 0.672308\nCost after iteration 2600: 0.672308\nCost after iteration 2700: 0.672307\nCost after iteration 2800: 0.672307\nCost after iteration 2900: 0.672307\nCost after iteration 3000: 0.672307\nCost after iteration 3100: 0.672307\nCost after iteration 3200: 0.672306\nCost after iteration 3300: 0.672306\nCost after iteration 3400: 0.672306\nCost after iteration 3500: 0.672306\nCost after iteration 3600: 0.672305\nCost after iteration 3700: 0.672305\nCost after iteration 3800: 0.672305\nCost after iteration 3900: 0.672305\nCost after iteration 4000: 0.672304\nCost after iteration 4100: 0.672304\nCost after iteration 4200: 0.672304\nCost after iteration 4300: 0.672304\nCost after iteration 4400: 0.672303\nCost after iteration 4500: 0.672303\nCost after iteration 4600: 0.672303\nCost after iteration 4700: 0.672303\nCost after iteration 4800: 0.672302\nCost after iteration 4900: 0.672302\n7.78\n"
      }
     ],
     "persistentId": "b175609a-a7d0-4bf8-84b6-23b93d651481",
     "text": "if __name__ == '__main__':\n    model(X_train, y_train, X_test, y_test, num_iterations=5000, learning_rate=0.005)\n"
    },
    "executionTime": "2020-02-04T05:21:50.998Z"
   },
   {
    "cell": {
     "executionCount": 55,
     "executionEventId": "f556b59d-4076-4304-a0fb-c218a82bcc59",
     "hasError": false,
     "id": "611bea7f-db28-443e-b57e-464cc0492891",
     "outputs": [],
     "persistentId": "736561b1-27f2-4958-a474-6d34622f80bb",
     "text": "def model(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate,print_cost=True):\n    \"\"\"\n    Build the logistic regression model by calling all the functions you have implemented.\n    Arguments:\n    X_train - training set\n    Y_train - training label\n    X_test - test set\n    Y_test - test label\n    num_iteration - hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n    \n    Returns:\n    d - dictionary should contain following information w,b,training_accuracy, test_accuracy,cost\n    eg: d = {\"w\":w,\n             \"b\":b,\n             \"training_accuracy\": traing_accuracy,\n             \"test_accuracy\":test_accuracy,\n             \"cost\":cost}\n    \"\"\"\n    # æ•°æ®ä¿®æ”¹ä¸ºä¸€ç»´\n    x_train = X_train.reshape(-1,X_train.shape[0])\n    y_train = Y_train.reshape(-1,Y_train.shape[0])\n    x_test = X_test.reshape(-1,X_test.shape[0])\n    y_test = Y_test.reshape(-1,Y_test.shape[0])\n    \n    # åˆå§‹åŒ–å‚æ•°\n    w,b = initialize_parameters(x_train.shape[0])\n    \n    # params:æ›´æ–°åçš„ç½‘ç»œå‚æ•°\n    # grads:æœ€æ–°çš„æ¢¯åº¦(ä¸‹é™æŸå¤±)\n    # costs:æ¯æ¬¡æ›´æ–°çš„æŸå¤±åˆ—è¡¨\n    params, grads, costs = optimize(w,b,x_train,y_train,num_iterations,learning_rate,print_cost)\n    \n    # è·å–è®­ç»ƒçš„å‚æ•°\n    w = params[\"w\"]\n    b = params[\"b\"]\n    \n    # é¢„æµ‹ç»“æœ\n    y_prediction_train = predict(w,b,x_train)\n    y_prediction_test = predict(w,b,x_test)\n#     print(y_prediction_train)\n    \n    count_right = np.sum(y_prediction_train == y_train)\n    accuracy = count_right / y_train.shape[0]\n    print(accuracy)\n    \n    # æ‰“å°å‡†ç¡®ç‡ï¼ŒæŸå¤±\n#     print(\"Logistic Loss is ()\".format(np.mean(np.abs(costs))))\n#     print(\"è®­ç»ƒé›†å‡†ç¡®ç‡ï¼š()\".format(100 - np.mean(np.abs(y_prediction_train - y_train))))\n#     print(\"æµ‹è¯•é›†å‡†ç¡®ç‡ï¼š()\".format(100 - np.mean(np.abs(y_prediction_test - y_test))))\n    \n    \n    \n    "
    },
    "executionTime": "2020-02-04T05:22:10.649Z"
   },
   {
    "cell": {
     "executionCount": 56,
     "executionEventId": "a13436d6-cf23-4104-a4af-017a05c2f9c6",
     "hasError": false,
     "id": "1fde88e1-df7e-49c2-ae1f-e396411d30e3",
     "outputs": [
      {
       "name": "stderr",
       "output_type": "stream",
       "text": "/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: divide by zero encountered in log\n  del sys.path[0]\n/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in multiply\n  del sys.path[0]\n"
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "Cost after iteration 0: nan\nCost after iteration 100: nan\nCost after iteration 200: nan\nCost after iteration 300: nan\nCost after iteration 400: nan\nCost after iteration 500: nan\nCost after iteration 600: inf\nCost after iteration 700: 2.656683\nCost after iteration 800: 1.582503\nCost after iteration 900: 0.883899\nCost after iteration 1000: 0.686537\nCost after iteration 1100: 0.672882\nCost after iteration 1200: 0.672332\nCost after iteration 1300: 0.672309\nCost after iteration 1400: 0.672308\nCost after iteration 1500: 0.672308\nCost after iteration 1600: 0.672307\nCost after iteration 1700: 0.672307\nCost after iteration 1800: 0.672307\nCost after iteration 1900: 0.672307\nCost after iteration 2000: 0.672306\nCost after iteration 2100: 0.672306\nCost after iteration 2200: 0.672306\nCost after iteration 2300: 0.672306\nCost after iteration 2400: 0.672305\nCost after iteration 2500: 0.672305\nCost after iteration 2600: 0.672305\nCost after iteration 2700: 0.672305\nCost after iteration 2800: 0.672304\nCost after iteration 2900: 0.672304\nCost after iteration 3000: 0.672304\nCost after iteration 3100: 0.672304\nCost after iteration 3200: 0.672304\nCost after iteration 3300: 0.672303\nCost after iteration 3400: 0.672303\nCost after iteration 3500: 0.672303\nCost after iteration 3600: 0.672303\nCost after iteration 3700: 0.672302\nCost after iteration 3800: 0.672302\nCost after iteration 3900: 0.672302\nCost after iteration 4000: 0.672302\nCost after iteration 4100: 0.672301\nCost after iteration 4200: 0.672301\nCost after iteration 4300: 0.672301\nCost after iteration 4400: 0.672301\nCost after iteration 4500: 0.672300\nCost after iteration 4600: 0.672300\nCost after iteration 4700: 0.672300\nCost after iteration 4800: 0.672300\nCost after iteration 4900: 0.672300\n777.0\n"
      }
     ],
     "persistentId": "b175609a-a7d0-4bf8-84b6-23b93d651481",
     "text": "if __name__ == '__main__':\n    model(X_train, y_train, X_test, y_test, num_iterations=5000, learning_rate=0.005)\n"
    },
    "executionTime": "2020-02-04T05:22:11.614Z"
   },
   {
    "cell": {
     "executionCount": 57,
     "executionEventId": "b33a649f-335d-4c31-98e0-68ebcaf9f76c",
     "hasError": false,
     "id": "611bea7f-db28-443e-b57e-464cc0492891",
     "outputs": [],
     "persistentId": "736561b1-27f2-4958-a474-6d34622f80bb",
     "text": "def model(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate,print_cost=True):\n    \"\"\"\n    Build the logistic regression model by calling all the functions you have implemented.\n    Arguments:\n    X_train - training set\n    Y_train - training label\n    X_test - test set\n    Y_test - test label\n    num_iteration - hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n    \n    Returns:\n    d - dictionary should contain following information w,b,training_accuracy, test_accuracy,cost\n    eg: d = {\"w\":w,\n             \"b\":b,\n             \"training_accuracy\": traing_accuracy,\n             \"test_accuracy\":test_accuracy,\n             \"cost\":cost}\n    \"\"\"\n    # æ•°æ®ä¿®æ”¹ä¸ºä¸€ç»´\n    x_train = X_train.reshape(-1,X_train.shape[0])\n    y_train = Y_train.reshape(-1,Y_train.shape[0])\n    x_test = X_test.reshape(-1,X_test.shape[0])\n    y_test = Y_test.reshape(-1,Y_test.shape[0])\n    \n    # åˆå§‹åŒ–å‚æ•°\n    w,b = initialize_parameters(x_train.shape[0])\n    \n    # params:æ›´æ–°åçš„ç½‘ç»œå‚æ•°\n    # grads:æœ€æ–°çš„æ¢¯åº¦(ä¸‹é™æŸå¤±)\n    # costs:æ¯æ¬¡æ›´æ–°çš„æŸå¤±åˆ—è¡¨\n    params, grads, costs = optimize(w,b,x_train,y_train,num_iterations,learning_rate,print_cost)\n    \n    # è·å–è®­ç»ƒçš„å‚æ•°\n    w = params[\"w\"]\n    b = params[\"b\"]\n    \n    # é¢„æµ‹ç»“æœ\n    y_prediction_train = predict(w,b,x_train)\n    y_prediction_test = predict(w,b,x_test)\n#     print(y_prediction_train)\n    \n    count_right = np.sum(y_prediction_train == y_train)\n    accuracy = count_right / y_train.shape[1]\n    print(accuracy)\n    \n    # æ‰“å°å‡†ç¡®ç‡ï¼ŒæŸå¤±\n#     print(\"Logistic Loss is ()\".format(np.mean(np.abs(costs))))\n#     print(\"è®­ç»ƒé›†å‡†ç¡®ç‡ï¼š()\".format(100 - np.mean(np.abs(y_prediction_train - y_train))))\n#     print(\"æµ‹è¯•é›†å‡†ç¡®ç‡ï¼š()\".format(100 - np.mean(np.abs(y_prediction_test - y_test))))\n    \n    \n    \n    "
    },
    "executionTime": "2020-02-04T05:22:19.938Z"
   },
   {
    "cell": {
     "executionCount": 58,
     "executionEventId": "6bdeada8-f296-4dba-80cc-7b412995480e",
     "hasError": false,
     "id": "1fde88e1-df7e-49c2-ae1f-e396411d30e3",
     "outputs": [
      {
       "name": "stderr",
       "output_type": "stream",
       "text": "/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: divide by zero encountered in log\n  del sys.path[0]\n/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in multiply\n  del sys.path[0]\n"
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "Cost after iteration 0: nan\nCost after iteration 100: nan\nCost after iteration 200: nan\nCost after iteration 300: nan\nCost after iteration 400: nan\nCost after iteration 500: nan\nCost after iteration 600: nan\nCost after iteration 700: inf\nCost after iteration 800: 2.592047\nCost after iteration 900: 1.659818\nCost after iteration 1000: 0.975653\nCost after iteration 1100: 0.705014\nCost after iteration 1200: 0.674090\nCost after iteration 1300: 0.672416\nCost after iteration 1400: 0.672327\nCost after iteration 1500: 0.672321\nCost after iteration 1600: 0.672321\nCost after iteration 1700: 0.672320\nCost after iteration 1800: 0.672320\nCost after iteration 1900: 0.672320\nCost after iteration 2000: 0.672320\nCost after iteration 2100: 0.672319\nCost after iteration 2200: 0.672319\nCost after iteration 2300: 0.672319\nCost after iteration 2400: 0.672319\nCost after iteration 2500: 0.672318\nCost after iteration 2600: 0.672318\nCost after iteration 2700: 0.672318\nCost after iteration 2800: 0.672317\nCost after iteration 2900: 0.672317\nCost after iteration 3000: 0.672317\nCost after iteration 3100: 0.672317\nCost after iteration 3200: 0.672316\nCost after iteration 3300: 0.672316\nCost after iteration 3400: 0.672316\nCost after iteration 3500: 0.672315\nCost after iteration 3600: 0.672315\nCost after iteration 3700: 0.672315\nCost after iteration 3800: 0.672315\nCost after iteration 3900: 0.672314\nCost after iteration 4000: 0.672314\nCost after iteration 4100: 0.672314\nCost after iteration 4200: 0.672314\nCost after iteration 4300: 0.672313\nCost after iteration 4400: 0.672313\nCost after iteration 4500: 0.672313\nCost after iteration 4600: 0.672313\nCost after iteration 4700: 0.672312\nCost after iteration 4800: 0.672312\nCost after iteration 4900: 0.672312\n0.5783221974758723\n"
      }
     ],
     "persistentId": "b175609a-a7d0-4bf8-84b6-23b93d651481",
     "text": "if __name__ == '__main__':\n    model(X_train, y_train, X_test, y_test, num_iterations=5000, learning_rate=0.005)\n"
    },
    "executionTime": "2020-02-04T05:22:20.887Z"
   },
   {
    "cell": {
     "executionCount": 59,
     "executionEventId": "305bed2e-bbe1-48f7-855b-5fcd0dc96e73",
     "hasError": false,
     "id": "611bea7f-db28-443e-b57e-464cc0492891",
     "outputs": [],
     "persistentId": "736561b1-27f2-4958-a474-6d34622f80bb",
     "text": "def model(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate,print_cost=True):\n    \"\"\"\n    Build the logistic regression model by calling all the functions you have implemented.\n    Arguments:\n    X_train - training set\n    Y_train - training label\n    X_test - test set\n    Y_test - test label\n    num_iteration - hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n    \n    Returns:\n    d - dictionary should contain following information w,b,training_accuracy, test_accuracy,cost\n    eg: d = {\"w\":w,\n             \"b\":b,\n             \"training_accuracy\": traing_accuracy,\n             \"test_accuracy\":test_accuracy,\n             \"cost\":cost}\n    \"\"\"\n    # æ•°æ®ä¿®æ”¹ä¸ºä¸€ç»´\n    x_train = X_train.reshape(-1,X_train.shape[0])\n    y_train = Y_train.reshape(-1,Y_train.shape[0])\n    x_test = X_test.reshape(-1,X_test.shape[0])\n    y_test = Y_test.reshape(-1,Y_test.shape[0])\n    \n    # åˆå§‹åŒ–å‚æ•°\n    w,b = initialize_parameters(x_train.shape[0])\n    \n    # params:æ›´æ–°åçš„ç½‘ç»œå‚æ•°\n    # grads:æœ€æ–°çš„æ¢¯åº¦(ä¸‹é™æŸå¤±)\n    # costs:æ¯æ¬¡æ›´æ–°çš„æŸå¤±åˆ—è¡¨\n    params, grads, costs = optimize(w,b,x_train,y_train,num_iterations,learning_rate,print_cost)\n    \n    # è·å–è®­ç»ƒçš„å‚æ•°\n    w = params[\"w\"]\n    b = params[\"b\"]\n    \n    # é¢„æµ‹ç»“æœ\n    y_prediction_train = predict(w,b,x_train)\n    y_prediction_test = predict(w,b,x_test)\n    \n#     count_right = np.sum(y_prediction_train == y_train)\n#     accuracy = count_right / y_train.shape[1]\n#     print(accuracy)\n\n    \n    # æ‰“å°å‡†ç¡®ç‡ï¼ŒæŸå¤±\n    print(\"training setâ€˜s accuracyï¼š()\".format(np.sum(y_prediction_train == y_train)/ y_train.shape[1]))\n    print(\"testing setâ€™s accuracyï¼š()\".format(np.sum(y_prediction_test == y_test)/ y_test.shape[1]))\n    \n    \n    \n    "
    },
    "executionTime": "2020-02-04T05:26:01.965Z"
   },
   {
    "cell": {
     "executionCount": 60,
     "executionEventId": "33a71ce2-cf41-484b-8392-8dbcf42694b8",
     "hasError": false,
     "id": "1fde88e1-df7e-49c2-ae1f-e396411d30e3",
     "outputs": [
      {
       "name": "stderr",
       "output_type": "stream",
       "text": "/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: divide by zero encountered in log\n  del sys.path[0]\n/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in multiply\n  del sys.path[0]\n"
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "Cost after iteration 0: nan\nCost after iteration 100: nan\nCost after iteration 200: nan\nCost after iteration 300: nan\nCost after iteration 400: nan\nCost after iteration 500: nan\nCost after iteration 600: nan\nCost after iteration 700: nan\nCost after iteration 800: nan\nCost after iteration 900: 2.945424\nCost after iteration 1000: 2.023290\nCost after iteration 1100: 1.267144\nCost after iteration 1200: 0.801561\nCost after iteration 1300: 0.681491\nCost after iteration 1400: 0.672717\nCost after iteration 1500: 0.672317\nCost after iteration 1600: 0.672299\nCost after iteration 1700: 0.672298\nCost after iteration 1800: 0.672298\nCost after iteration 1900: 0.672297\nCost after iteration 2000: 0.672297\nCost after iteration 2100: 0.672297\nCost after iteration 2200: 0.672297\nCost after iteration 2300: 0.672296\nCost after iteration 2400: 0.672296\nCost after iteration 2500: 0.672296\nCost after iteration 2600: 0.672296\nCost after iteration 2700: 0.672296\nCost after iteration 2800: 0.672295\nCost after iteration 2900: 0.672295\nCost after iteration 3000: 0.672295\nCost after iteration 3100: 0.672295\nCost after iteration 3200: 0.672295\nCost after iteration 3300: 0.672294\nCost after iteration 3400: 0.672294\nCost after iteration 3500: 0.672294\nCost after iteration 3600: 0.672294\nCost after iteration 3700: 0.672293\nCost after iteration 3800: 0.672293\nCost after iteration 3900: 0.672293\nCost after iteration 4000: 0.672293\nCost after iteration 4100: 0.672293\nCost after iteration 4200: 0.672292\nCost after iteration 4300: 0.672292\nCost after iteration 4400: 0.672292\nCost after iteration 4500: 0.672292\nCost after iteration 4600: 0.672292\nCost after iteration 4700: 0.672291\nCost after iteration 4800: 0.672291\nCost after iteration 4900: 0.672291\ntraining setâ€˜s accuracyï¼š()\ntesting setâ€™s accuracyï¼š()\n"
      }
     ],
     "persistentId": "b175609a-a7d0-4bf8-84b6-23b93d651481",
     "text": "if __name__ == '__main__':\n    model(X_train, y_train, X_test, y_test, num_iterations=5000, learning_rate=0.005)\n"
    },
    "executionTime": "2020-02-04T05:26:03.438Z"
   },
   {
    "cell": {
     "executionCount": 61,
     "executionEventId": "807a0bb5-ae04-462c-b9df-54e1dda41748",
     "hasError": false,
     "id": "611bea7f-db28-443e-b57e-464cc0492891",
     "outputs": [],
     "persistentId": "736561b1-27f2-4958-a474-6d34622f80bb",
     "text": "def model(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate,print_cost=True):\n    \"\"\"\n    Build the logistic regression model by calling all the functions you have implemented.\n    Arguments:\n    X_train - training set\n    Y_train - training label\n    X_test - test set\n    Y_test - test label\n    num_iteration - hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n    \n    Returns:\n    d - dictionary should contain following information w,b,training_accuracy, test_accuracy,cost\n    eg: d = {\"w\":w,\n             \"b\":b,\n             \"training_accuracy\": traing_accuracy,\n             \"test_accuracy\":test_accuracy,\n             \"cost\":cost}\n    \"\"\"\n    # æ•°æ®ä¿®æ”¹ä¸ºä¸€ç»´\n    x_train = X_train.reshape(-1,X_train.shape[0])\n    y_train = Y_train.reshape(-1,Y_train.shape[0])\n    x_test = X_test.reshape(-1,X_test.shape[0])\n    y_test = Y_test.reshape(-1,Y_test.shape[0])\n    \n    # åˆå§‹åŒ–å‚æ•°\n    w,b = initialize_parameters(x_train.shape[0])\n    \n    # params:æ›´æ–°åçš„ç½‘ç»œå‚æ•°\n    # grads:æœ€æ–°çš„æ¢¯åº¦(ä¸‹é™æŸå¤±)\n    # costs:æ¯æ¬¡æ›´æ–°çš„æŸå¤±åˆ—è¡¨\n    params, grads, costs = optimize(w,b,x_train,y_train,num_iterations,learning_rate,print_cost)\n    \n    # è·å–è®­ç»ƒçš„å‚æ•°\n    w = params[\"w\"]\n    b = params[\"b\"]\n    \n    # é¢„æµ‹ç»“æœ\n    y_prediction_train = predict(w,b,x_train)\n    y_prediction_test = predict(w,b,x_test)\n    \n#     count_right = np.sum(y_prediction_train == y_train)\n#     accuracy = count_right / y_train.shape[1]\n#     print(accuracy)\n\n    \n    # æ‰“å°å‡†ç¡®ç‡ï¼ŒæŸå¤±\n    print(\"training setâ€˜s accuracyï¼š{}\".format(np.sum(y_prediction_train == y_train)/ y_train.shape[1]))\n    print(\"testing setâ€™s accuracyï¼š{}\".format(np.sum(y_prediction_test == y_test)/ y_test.shape[1]))\n    \n    \n    \n    "
    },
    "executionTime": "2020-02-04T05:26:53.857Z"
   },
   {
    "cell": {
     "executionCount": 62,
     "executionEventId": "216bdda7-a925-4e04-b096-6037b482b575",
     "hasError": false,
     "id": "1fde88e1-df7e-49c2-ae1f-e396411d30e3",
     "outputs": [
      {
       "name": "stderr",
       "output_type": "stream",
       "text": "/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: divide by zero encountered in log\n  del sys.path[0]\n/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in multiply\n  del sys.path[0]\n"
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "Cost after iteration 0: nan\nCost after iteration 100: nan\nCost after iteration 200: nan\nCost after iteration 300: nan\nCost after iteration 400: nan\nCost after iteration 500: nan\nCost after iteration 600: nan\nCost after iteration 700: 3.056728\nCost after iteration 800: 2.073709\nCost after iteration 900: 1.268307\nCost after iteration 1000: 0.780128\nCost after iteration 1100: 0.678150\nCost after iteration 1200: 0.672553\nCost after iteration 1300: 0.672312\nCost after iteration 1400: 0.672301\nCost after iteration 1500: 0.672300\nCost after iteration 1600: 0.672299\nCost after iteration 1700: 0.672299\nCost after iteration 1800: 0.672299\nCost after iteration 1900: 0.672299\nCost after iteration 2000: 0.672299\nCost after iteration 2100: 0.672298\nCost after iteration 2200: 0.672298\nCost after iteration 2300: 0.672298\nCost after iteration 2400: 0.672298\nCost after iteration 2500: 0.672297\nCost after iteration 2600: 0.672297\nCost after iteration 2700: 0.672297\nCost after iteration 2800: 0.672297\nCost after iteration 2900: 0.672297\nCost after iteration 3000: 0.672296\nCost after iteration 3100: 0.672296\nCost after iteration 3200: 0.672296\nCost after iteration 3300: 0.672296\nCost after iteration 3400: 0.672295\nCost after iteration 3500: 0.672295\nCost after iteration 3600: 0.672295\nCost after iteration 3700: 0.672295\nCost after iteration 3800: 0.672295\nCost after iteration 3900: 0.672294\nCost after iteration 4000: 0.672294\nCost after iteration 4100: 0.672294\nCost after iteration 4200: 0.672294\nCost after iteration 4300: 0.672294\nCost after iteration 4400: 0.672293\nCost after iteration 4500: 0.672293\nCost after iteration 4600: 0.672293\nCost after iteration 4700: 0.672293\nCost after iteration 4800: 0.672292\nCost after iteration 4900: 0.672292\ntraining setâ€˜s accuracyï¼š0.5775798069784707\ntesting setâ€™s accuracyï¼š0.49777777777777776\n"
      }
     ],
     "persistentId": "b175609a-a7d0-4bf8-84b6-23b93d651481",
     "text": "if __name__ == '__main__':\n    model(X_train, y_train, X_test, y_test, num_iterations=5000, learning_rate=0.005)\n"
    },
    "executionTime": "2020-02-04T05:26:59.184Z"
   },
   {
    "cell": {
     "executionCount": 63,
     "executionEventId": "f21d6803-7f9c-4eda-96a0-0c2cd5cad03e",
     "hasError": false,
     "id": "1fde88e1-df7e-49c2-ae1f-e396411d30e3",
     "outputs": [
      {
       "name": "stderr",
       "output_type": "stream",
       "text": "/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: divide by zero encountered in log\n  del sys.path[0]\n/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in multiply\n  del sys.path[0]\n"
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "Cost after iteration 0: nan\nCost after iteration 100: nan\nCost after iteration 200: nan\nCost after iteration 300: nan\nCost after iteration 400: nan\nCost after iteration 500: nan\nCost after iteration 600: nan\nCost after iteration 700: nan\nCost after iteration 800: nan\nCost after iteration 900: nan\nCost after iteration 1000: nan\nCost after iteration 1100: nan\nCost after iteration 1200: nan\nCost after iteration 1300: nan\nCost after iteration 1400: nan\nCost after iteration 1500: nan\nCost after iteration 1600: nan\nCost after iteration 1700: nan\nCost after iteration 1800: nan\nCost after iteration 1900: nan\nCost after iteration 2000: nan\nCost after iteration 2100: nan\nCost after iteration 2200: nan\nCost after iteration 2300: nan\nCost after iteration 2400: nan\nCost after iteration 2500: nan\nCost after iteration 2600: nan\nCost after iteration 2700: nan\nCost after iteration 2800: nan\nCost after iteration 2900: nan\nCost after iteration 3000: inf\nCost after iteration 3100: inf\nCost after iteration 3200: inf\nCost after iteration 3300: inf\nCost after iteration 3400: inf\nCost after iteration 3500: inf\nCost after iteration 3600: 3.367273\nCost after iteration 3700: 3.134754\nCost after iteration 3800: 2.909105\nCost after iteration 3900: 2.690670\nCost after iteration 4000: 2.480086\nCost after iteration 4100: 2.277973\nCost after iteration 4200: 2.084871\nCost after iteration 4300: 1.901214\nCost after iteration 4400: 1.727374\nCost after iteration 4500: 1.563807\nCost after iteration 4600: 1.411187\nCost after iteration 4700: 1.270487\nCost after iteration 4800: 1.142967\nCost after iteration 4900: 1.030113\nCost after iteration 5000: 0.933457\nCost after iteration 5100: 0.854210\nCost after iteration 5200: 0.792730\nCost after iteration 5300: 0.748026\nCost after iteration 5400: 0.717691\nCost after iteration 5500: 0.698433\nCost after iteration 5600: 0.686889\nCost after iteration 5700: 0.680272\nCost after iteration 5800: 0.676597\nCost after iteration 5900: 0.674596\nCost after iteration 6000: 0.673521\nCost after iteration 6100: 0.672946\nCost after iteration 6200: 0.672640\nCost after iteration 6300: 0.672477\nCost after iteration 6400: 0.672390\nCost after iteration 6500: 0.672343\nCost after iteration 6600: 0.672318\nCost after iteration 6700: 0.672305\nCost after iteration 6800: 0.672297\nCost after iteration 6900: 0.672293\nCost after iteration 7000: 0.672291\nCost after iteration 7100: 0.672290\nCost after iteration 7200: 0.672289\nCost after iteration 7300: 0.672289\nCost after iteration 7400: 0.672289\nCost after iteration 7500: 0.672289\nCost after iteration 7600: 0.672289\nCost after iteration 7700: 0.672288\nCost after iteration 7800: 0.672288\nCost after iteration 7900: 0.672288\nCost after iteration 8000: 0.672288\nCost after iteration 8100: 0.672288\nCost after iteration 8200: 0.672288\nCost after iteration 8300: 0.672288\nCost after iteration 8400: 0.672288\nCost after iteration 8500: 0.672288\nCost after iteration 8600: 0.672288\nCost after iteration 8700: 0.672288\nCost after iteration 8800: 0.672288\nCost after iteration 8900: 0.672288\nCost after iteration 9000: 0.672288\nCost after iteration 9100: 0.672288\nCost after iteration 9200: 0.672288\nCost after iteration 9300: 0.672288\nCost after iteration 9400: 0.672288\nCost after iteration 9500: 0.672288\nCost after iteration 9600: 0.672288\nCost after iteration 9700: 0.672288\nCost after iteration 9800: 0.672288\nCost after iteration 9900: 0.672288\nCost after iteration 10000: 0.672288\nCost after iteration 10100: 0.672287\nCost after iteration 10200: 0.672287\nCost after iteration 10300: 0.672287\nCost after iteration 10400: 0.672287\nCost after iteration 10500: 0.672287\nCost after iteration 10600: 0.672287\nCost after iteration 10700: 0.672287\nCost after iteration 10800: 0.672287\nCost after iteration 10900: 0.672287\nCost after iteration 11000: 0.672287\nCost after iteration 11100: 0.672287\nCost after iteration 11200: 0.672287\nCost after iteration 11300: 0.672287\nCost after iteration 11400: 0.672287\nCost after iteration 11500: 0.672287\nCost after iteration 11600: 0.672287\nCost after iteration 11700: 0.672287\nCost after iteration 11800: 0.672287\nCost after iteration 11900: 0.672287\nCost after iteration 12000: 0.672287\nCost after iteration 12100: 0.672287\nCost after iteration 12200: 0.672287\nCost after iteration 12300: 0.672287\nCost after iteration 12400: 0.672287\nCost after iteration 12500: 0.672287\nCost after iteration 12600: 0.672286\nCost after iteration 12700: 0.672286\nCost after iteration 12800: 0.672286\nCost after iteration 12900: 0.672286\nCost after iteration 13000: 0.672286\nCost after iteration 13100: 0.672286\nCost after iteration 13200: 0.672286\nCost after iteration 13300: 0.672286\nCost after iteration 13400: 0.672286\nCost after iteration 13500: 0.672286\nCost after iteration 13600: 0.672286\nCost after iteration 13700: 0.672286\nCost after iteration 13800: 0.672286\nCost after iteration 13900: 0.672286\nCost after iteration 14000: 0.672286\nCost after iteration 14100: 0.672286\nCost after iteration 14200: 0.672286\nCost after iteration 14300: 0.672286\nCost after iteration 14400: 0.672286\nCost after iteration 14500: 0.672286\nCost after iteration 14600: 0.672286\nCost after iteration 14700: 0.672286\nCost after iteration 14800: 0.672286\nCost after iteration 14900: 0.672286\nCost after iteration 15000: 0.672286\nCost after iteration 15100: 0.672286\nCost after iteration 15200: 0.672285\nCost after iteration 15300: 0.672285\nCost after iteration 15400: 0.672285\nCost after iteration 15500: 0.672285\nCost after iteration 15600: 0.672285\nCost after iteration 15700: 0.672285\nCost after iteration 15800: 0.672285\nCost after iteration 15900: 0.672285\nCost after iteration 16000: 0.672285\nCost after iteration 16100: 0.672285\nCost after iteration 16200: 0.672285\nCost after iteration 16300: 0.672285\nCost after iteration 16400: 0.672285\nCost after iteration 16500: 0.672285\nCost after iteration 16600: 0.672285\nCost after iteration 16700: 0.672285\nCost after iteration 16800: 0.672285\nCost after iteration 16900: 0.672285\nCost after iteration 17000: 0.672285\nCost after iteration 17100: 0.672285\nCost after iteration 17200: 0.672285\nCost after iteration 17300: 0.672285\nCost after iteration 17400: 0.672285\nCost after iteration 17500: 0.672285\nCost after iteration 17600: 0.672285\nCost after iteration 17700: 0.672285\nCost after iteration 17800: 0.672284\nCost after iteration 17900: 0.672284\nCost after iteration 18000: 0.672284\nCost after iteration 18100: 0.672284\nCost after iteration 18200: 0.672284\nCost after iteration 18300: 0.672284\nCost after iteration 18400: 0.672284\nCost after iteration 18500: 0.672284\nCost after iteration 18600: 0.672284\nCost after iteration 18700: 0.672284\nCost after iteration 18800: 0.672284\nCost after iteration 18900: 0.672284\nCost after iteration 19000: 0.672284\nCost after iteration 19100: 0.672284\nCost after iteration 19200: 0.672284\nCost after iteration 19300: 0.672284\nCost after iteration 19400: 0.672284\nCost after iteration 19500: 0.672284\nCost after iteration 19600: 0.672284\nCost after iteration 19700: 0.672284\nCost after iteration 19800: 0.672284\nCost after iteration 19900: 0.672284\nCost after iteration 20000: 0.672284\nCost after iteration 20100: 0.672284\nCost after iteration 20200: 0.672284\nCost after iteration 20300: 0.672284\nCost after iteration 20400: 0.672284\nCost after iteration 20500: 0.672283\nCost after iteration 20600: 0.672283\nCost after iteration 20700: 0.672283\nCost after iteration 20800: 0.672283\nCost after iteration 20900: 0.672283\nCost after iteration 21000: 0.672283\nCost after iteration 21100: 0.672283\nCost after iteration 21200: 0.672283\nCost after iteration 21300: 0.672283\nCost after iteration 21400: 0.672283\nCost after iteration 21500: 0.672283\nCost after iteration 21600: 0.672283\nCost after iteration 21700: 0.672283\nCost after iteration 21800: 0.672283\nCost after iteration 21900: 0.672283\nCost after iteration 22000: 0.672283\nCost after iteration 22100: 0.672283\nCost after iteration 22200: 0.672283\nCost after iteration 22300: 0.672283\nCost after iteration 22400: 0.672283\nCost after iteration 22500: 0.672283\nCost after iteration 22600: 0.672283\nCost after iteration 22700: 0.672283\nCost after iteration 22800: 0.672283\nCost after iteration 22900: 0.672283\nCost after iteration 23000: 0.672283\nCost after iteration 23100: 0.672283\nCost after iteration 23200: 0.672282\nCost after iteration 23300: 0.672282\nCost after iteration 23400: 0.672282\nCost after iteration 23500: 0.672282\nCost after iteration 23600: 0.672282\nCost after iteration 23700: 0.672282\nCost after iteration 23800: 0.672282\nCost after iteration 23900: 0.672282\nCost after iteration 24000: 0.672282\nCost after iteration 24100: 0.672282\nCost after iteration 24200: 0.672282\nCost after iteration 24300: 0.672282\nCost after iteration 24400: 0.672282\nCost after iteration 24500: 0.672282\nCost after iteration 24600: 0.672282\nCost after iteration 24700: 0.672282\nCost after iteration 24800: 0.672282\nCost after iteration 24900: 0.672282\nCost after iteration 25000: 0.672282\nCost after iteration 25100: 0.672282\nCost after iteration 25200: 0.672282\nCost after iteration 25300: 0.672282\nCost after iteration 25400: 0.672282\nCost after iteration 25500: 0.672282\nCost after iteration 25600: 0.672282\nCost after iteration 25700: 0.672282\nCost after iteration 25800: 0.672282\nCost after iteration 25900: 0.672281\nCost after iteration 26000: 0.672281\nCost after iteration 26100: 0.672281\nCost after iteration 26200: 0.672281\nCost after iteration 26300: 0.672281\nCost after iteration 26400: 0.672281\nCost after iteration 26500: 0.672281\nCost after iteration 26600: 0.672281\nCost after iteration 26700: 0.672281\nCost after iteration 26800: 0.672281\nCost after iteration 26900: 0.672281\nCost after iteration 27000: 0.672281\nCost after iteration 27100: 0.672281\nCost after iteration 27200: 0.672281\nCost after iteration 27300: 0.672281\nCost after iteration 27400: 0.672281\nCost after iteration 27500: 0.672281\nCost after iteration 27600: 0.672281\nCost after iteration 27700: 0.672281\nCost after iteration 27800: 0.672281\nCost after iteration 27900: 0.672281\nCost after iteration 28000: 0.672281\nCost after iteration 28100: 0.672281\nCost after iteration 28200: 0.672281\nCost after iteration 28300: 0.672281\nCost after iteration 28400: 0.672281\nCost after iteration 28500: 0.672281\nCost after iteration 28600: 0.672281\nCost after iteration 28700: 0.672280\nCost after iteration 28800: 0.672280\nCost after iteration 28900: 0.672280\nCost after iteration 29000: 0.672280\nCost after iteration 29100: 0.672280\nCost after iteration 29200: 0.672280\nCost after iteration 29300: 0.672280\nCost after iteration 29400: 0.672280\nCost after iteration 29500: 0.672280\nCost after iteration 29600: 0.672280\nCost after iteration 29700: 0.672280\nCost after iteration 29800: 0.672280\nCost after iteration 29900: 0.672280\nCost after iteration 30000: 0.672280\nCost after iteration 30100: 0.672280\nCost after iteration 30200: 0.672280\nCost after iteration 30300: 0.672280\nCost after iteration 30400: 0.672280\nCost after iteration 30500: 0.672280\nCost after iteration 30600: 0.672280\nCost after iteration 30700: 0.672280\nCost after iteration 30800: 0.672280\nCost after iteration 30900: 0.672280\nCost after iteration 31000: 0.672280\nCost after iteration 31100: 0.672280\nCost after iteration 31200: 0.672280\nCost after iteration 31300: 0.672280\nCost after iteration 31400: 0.672280\nCost after iteration 31500: 0.672279\nCost after iteration 31600: 0.672279\nCost after iteration 31700: 0.672279\nCost after iteration 31800: 0.672279\nCost after iteration 31900: 0.672279\nCost after iteration 32000: 0.672279\nCost after iteration 32100: 0.672279\nCost after iteration 32200: 0.672279\nCost after iteration 32300: 0.672279\nCost after iteration 32400: 0.672279\nCost after iteration 32500: 0.672279\nCost after iteration 32600: 0.672279\nCost after iteration 32700: 0.672279\nCost after iteration 32800: 0.672279\nCost after iteration 32900: 0.672279\nCost after iteration 33000: 0.672279\nCost after iteration 33100: 0.672279\nCost after iteration 33200: 0.672279\nCost after iteration 33300: 0.672279\nCost after iteration 33400: 0.672279\nCost after iteration 33500: 0.672279\nCost after iteration 33600: 0.672279\nCost after iteration 33700: 0.672279\nCost after iteration 33800: 0.672279\nCost after iteration 33900: 0.672279\nCost after iteration 34000: 0.672279\nCost after iteration 34100: 0.672279\nCost after iteration 34200: 0.672279\nCost after iteration 34300: 0.672279\nCost after iteration 34400: 0.672278\nCost after iteration 34500: 0.672278\nCost after iteration 34600: 0.672278\nCost after iteration 34700: 0.672278\nCost after iteration 34800: 0.672278\nCost after iteration 34900: 0.672278\nCost after iteration 35000: 0.672278\nCost after iteration 35100: 0.672278\nCost after iteration 35200: 0.672278\nCost after iteration 35300: 0.672278\nCost after iteration 35400: 0.672278\nCost after iteration 35500: 0.672278\nCost after iteration 35600: 0.672278\nCost after iteration 35700: 0.672278\nCost after iteration 35800: 0.672278\nCost after iteration 35900: 0.672278\nCost after iteration 36000: 0.672278\nCost after iteration 36100: 0.672278\nCost after iteration 36200: 0.672278\nCost after iteration 36300: 0.672278\nCost after iteration 36400: 0.672278\nCost after iteration 36500: 0.672278\nCost after iteration 36600: 0.672278\nCost after iteration 36700: 0.672278\nCost after iteration 36800: 0.672278\nCost after iteration 36900: 0.672278\nCost after iteration 37000: 0.672278\nCost after iteration 37100: 0.672278\nCost after iteration 37200: 0.672278\nCost after iteration 37300: 0.672277\nCost after iteration 37400: 0.672277\nCost after iteration 37500: 0.672277\nCost after iteration 37600: 0.672277\nCost after iteration 37700: 0.672277\nCost after iteration 37800: 0.672277\nCost after iteration 37900: 0.672277\nCost after iteration 38000: 0.672277\nCost after iteration 38100: 0.672277\nCost after iteration 38200: 0.672277\nCost after iteration 38300: 0.672277\nCost after iteration 38400: 0.672277\nCost after iteration 38500: 0.672277\nCost after iteration 38600: 0.672277\nCost after iteration 38700: 0.672277\nCost after iteration 38800: 0.672277\nCost after iteration 38900: 0.672277\nCost after iteration 39000: 0.672277\nCost after iteration 39100: 0.672277\nCost after iteration 39200: 0.672277\nCost after iteration 39300: 0.672277\nCost after iteration 39400: 0.672277\nCost after iteration 39500: 0.672277\nCost after iteration 39600: 0.672277\nCost after iteration 39700: 0.672277\nCost after iteration 39800: 0.672277\nCost after iteration 39900: 0.672277\nCost after iteration 40000: 0.672277\nCost after iteration 40100: 0.672277\nCost after iteration 40200: 0.672276\nCost after iteration 40300: 0.672276\nCost after iteration 40400: 0.672276\nCost after iteration 40500: 0.672276\nCost after iteration 40600: 0.672276\nCost after iteration 40700: 0.672276\nCost after iteration 40800: 0.672276\nCost after iteration 40900: 0.672276\nCost after iteration 41000: 0.672276\nCost after iteration 41100: 0.672276\nCost after iteration 41200: 0.672276\nCost after iteration 41300: 0.672276\nCost after iteration 41400: 0.672276\nCost after iteration 41500: 0.672276\nCost after iteration 41600: 0.672276\nCost after iteration 41700: 0.672276\nCost after iteration 41800: 0.672276\nCost after iteration 41900: 0.672276\nCost after iteration 42000: 0.672276\nCost after iteration 42100: 0.672276\nCost after iteration 42200: 0.672276\nCost after iteration 42300: 0.672276\nCost after iteration 42400: 0.672276\nCost after iteration 42500: 0.672276\nCost after iteration 42600: 0.672276\nCost after iteration 42700: 0.672276\nCost after iteration 42800: 0.672276\nCost after iteration 42900: 0.672276\nCost after iteration 43000: 0.672276\nCost after iteration 43100: 0.672276\nCost after iteration 43200: 0.672275\nCost after iteration 43300: 0.672275\nCost after iteration 43400: 0.672275\nCost after iteration 43500: 0.672275\nCost after iteration 43600: 0.672275\nCost after iteration 43700: 0.672275\nCost after iteration 43800: 0.672275\nCost after iteration 43900: 0.672275\nCost after iteration 44000: 0.672275\nCost after iteration 44100: 0.672275\nCost after iteration 44200: 0.672275\nCost after iteration 44300: 0.672275\nCost after iteration 44400: 0.672275\nCost after iteration 44500: 0.672275\nCost after iteration 44600: 0.672275\nCost after iteration 44700: 0.672275\nCost after iteration 44800: 0.672275\nCost after iteration 44900: 0.672275\nCost after iteration 45000: 0.672275\nCost after iteration 45100: 0.672275\nCost after iteration 45200: 0.672275\nCost after iteration 45300: 0.672275\nCost after iteration 45400: 0.672275\nCost after iteration 45500: 0.672275\nCost after iteration 45600: 0.672275\nCost after iteration 45700: 0.672275\nCost after iteration 45800: 0.672275\nCost after iteration 45900: 0.672275\nCost after iteration 46000: 0.672275\nCost after iteration 46100: 0.672275\nCost after iteration 46200: 0.672275\nCost after iteration 46300: 0.672274\nCost after iteration 46400: 0.672274\nCost after iteration 46500: 0.672274\nCost after iteration 46600: 0.672274\nCost after iteration 46700: 0.672274\nCost after iteration 46800: 0.672274\nCost after iteration 46900: 0.672274\nCost after iteration 47000: 0.672274\nCost after iteration 47100: 0.672274\nCost after iteration 47200: 0.672274\nCost after iteration 47300: 0.672274\nCost after iteration 47400: 0.672274\nCost after iteration 47500: 0.672274\nCost after iteration 47600: 0.672274\nCost after iteration 47700: 0.672274\nCost after iteration 47800: 0.672274\nCost after iteration 47900: 0.672274\nCost after iteration 48000: 0.672274\nCost after iteration 48100: 0.672274\nCost after iteration 48200: 0.672274\nCost after iteration 48300: 0.672274\nCost after iteration 48400: 0.672274\nCost after iteration 48500: 0.672274\nCost after iteration 48600: 0.672274\nCost after iteration 48700: 0.672274\nCost after iteration 48800: 0.672274\nCost after iteration 48900: 0.672274\nCost after iteration 49000: 0.672274\nCost after iteration 49100: 0.672274\nCost after iteration 49200: 0.672274\nCost after iteration 49300: 0.672274\nCost after iteration 49400: 0.672273\nCost after iteration 49500: 0.672273\nCost after iteration 49600: 0.672273\nCost after iteration 49700: 0.672273\nCost after iteration 49800: 0.672273\nCost after iteration 49900: 0.672273\ntraining setâ€˜s accuracyï¼š0.579064587973274\ntesting setâ€™s accuracyï¼š0.49777777777777776\n"
      }
     ],
     "persistentId": "b175609a-a7d0-4bf8-84b6-23b93d651481",
     "text": "if __name__ == '__main__':\n    model(X_train, y_train, X_test, y_test, num_iterations=50000, learning_rate=0.001)\n"
    },
    "executionTime": "2020-02-04T05:27:23.552Z"
   },
   {
    "cell": {
     "executionCount": 64,
     "executionEventId": "011864ca-2137-46e2-8b76-8ed5f4bf675b",
     "hasError": false,
     "id": "611bea7f-db28-443e-b57e-464cc0492891",
     "outputs": [],
     "persistentId": "736561b1-27f2-4958-a474-6d34622f80bb",
     "text": "def model(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate,print_cost=True):\n    \"\"\"\n    Build the logistic regression model by calling all the functions you have implemented.\n    Arguments:\n    X_train - training set\n    Y_train - training label\n    X_test - test set\n    Y_test - test label\n    num_iteration - hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations\n    \n    Returns:\n    d - dictionary should contain following information w,b,training_accuracy, test_accuracy,cost\n    eg: d = {\"w\":w,\n             \"b\":b,\n             \"training_accuracy\": traing_accuracy,\n             \"test_accuracy\":test_accuracy,\n             \"cost\":cost}\n    \"\"\"\n    # æ•°æ®ä¿®æ”¹ä¸ºä¸€ç»´\n    x_train = X_train.reshape(-1,X_train.shape[0])\n    y_train = Y_train.reshape(-1,Y_train.shape[0])\n    x_test = X_test.reshape(-1,X_test.shape[0])\n    y_test = Y_test.reshape(-1,Y_test.shape[0])\n    \n    # åˆå§‹åŒ–å‚æ•°\n    w,b = initialize_parameters(x_train.shape[0])\n    \n    # params:æ›´æ–°åçš„ç½‘ç»œå‚æ•°\n    # grads:æœ€æ–°çš„æ¢¯åº¦(ä¸‹é™æŸå¤±)\n    # costs:æ¯æ¬¡æ›´æ–°çš„æŸå¤±åˆ—è¡¨\n    params, grads, costs = optimize(w,b,x_train,y_train,num_iterations,learning_rate,print_cost)\n    \n    # è·å–è®­ç»ƒçš„å‚æ•°\n    w = params[\"w\"]\n    b = params[\"b\"]\n    \n    # é¢„æµ‹ç»“æœ\n    y_prediction_train = predict(w,b,x_train)\n    y_prediction_test = predict(w,b,x_test)\n    \n#     count_right = np.sum(y_prediction_train == y_train)\n#     accuracy = count_right / y_train.shape[1]\n#     print(accuracy)\n\n    \n    # æ‰“å°å‡†ç¡®ç‡ï¼ŒæŸå¤±\n#     print(\"training setâ€˜s accuracyï¼š{}\".format(np.sum(y_prediction_train == y_train)/ y_train.shape[1]))\n#     print(\"testing setâ€™s accuracyï¼š{}\".format(np.sum(y_prediction_test == y_test)/ y_test.shape[1]))\n    \n    # æ‰“å°å‡†ç¡®ç‡\n    print(\"è®­ç»ƒé›†å‡†ç¡®ç‡: {} \".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"æµ‹è¯•é›†å‡†ç¡®ç‡: {} \".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \n    \n    \n    "
    },
    "executionTime": "2020-02-04T05:28:30.222Z"
   },
   {
    "cell": {
     "executionCount": 65,
     "executionEventId": "c49009f0-fd34-4f85-9c67-89d9545ddb84",
     "hasError": false,
     "id": "1fde88e1-df7e-49c2-ae1f-e396411d30e3",
     "outputs": [
      {
       "name": "stderr",
       "output_type": "stream",
       "text": "/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: divide by zero encountered in log\n  del sys.path[0]\n/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in multiply\n  del sys.path[0]\n"
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "Cost after iteration 0: nan\nCost after iteration 100: nan\nCost after iteration 200: nan\nCost after iteration 300: nan\nCost after iteration 400: nan\nCost after iteration 500: nan\nCost after iteration 600: nan\nCost after iteration 700: nan\nCost after iteration 800: nan\nCost after iteration 900: nan\nCost after iteration 1000: nan\nCost after iteration 1100: nan\nCost after iteration 1200: nan\nCost after iteration 1300: nan\nCost after iteration 1400: nan\nCost after iteration 1500: nan\nCost after iteration 1600: nan\nCost after iteration 1700: nan\nCost after iteration 1800: nan\nCost after iteration 1900: nan\nCost after iteration 2000: nan\nCost after iteration 2100: nan\nCost after iteration 2200: nan\nCost after iteration 2300: nan\nCost after iteration 2400: nan\nCost after iteration 2500: nan\nCost after iteration 2600: nan\nCost after iteration 2700: nan\nCost after iteration 2800: nan\nCost after iteration 2900: nan\nCost after iteration 3000: nan\nCost after iteration 3100: 3.681858\nCost after iteration 3200: 3.430670\nCost after iteration 3300: 3.186594\nCost after iteration 3400: 2.949994\nCost after iteration 3500: 2.721266\nCost after iteration 3600: 2.500864\nCost after iteration 3700: 2.289253\nCost after iteration 3800: 2.086891\nCost after iteration 3900: 1.894308\nCost after iteration 4000: 1.712210\nCost after iteration 4100: 1.541544\nCost after iteration 4200: 1.383488\nCost after iteration 4300: 1.239400\nCost after iteration 4400: 1.110744\nCost after iteration 4500: 0.998988\nCost after iteration 4600: 0.905417\nCost after iteration 4700: 0.830766\nCost after iteration 4800: 0.774677\nCost after iteration 4900: 0.735299\nCost after iteration 5000: 0.709477\nCost after iteration 5100: 0.693540\nCost after iteration 5200: 0.684163\nCost after iteration 5300: 0.678829\nCost after iteration 5400: 0.675860\nCost after iteration 5500: 0.674229\nCost after iteration 5600: 0.673339\nCost after iteration 5700: 0.672855\nCost after iteration 5800: 0.672592\nCost after iteration 5900: 0.672449\nCost after iteration 6000: 0.672371\nCost after iteration 6100: 0.672329\nCost after iteration 6200: 0.672305\nCost after iteration 6300: 0.672293\nCost after iteration 6400: 0.672286\nCost after iteration 6500: 0.672282\nCost after iteration 6600: 0.672280\nCost after iteration 6700: 0.672279\nCost after iteration 6800: 0.672278\nCost after iteration 6900: 0.672277\nCost after iteration 7000: 0.672277\nCost after iteration 7100: 0.672277\nCost after iteration 7200: 0.672277\nCost after iteration 7300: 0.672277\nCost after iteration 7400: 0.672277\nCost after iteration 7500: 0.672277\nCost after iteration 7600: 0.672277\nCost after iteration 7700: 0.672277\nCost after iteration 7800: 0.672277\nCost after iteration 7900: 0.672277\nCost after iteration 8000: 0.672277\nCost after iteration 8100: 0.672277\nCost after iteration 8200: 0.672277\nCost after iteration 8300: 0.672277\nCost after iteration 8400: 0.672277\nCost after iteration 8500: 0.672277\nCost after iteration 8600: 0.672276\nCost after iteration 8700: 0.672276\nCost after iteration 8800: 0.672276\nCost after iteration 8900: 0.672276\nCost after iteration 9000: 0.672276\nCost after iteration 9100: 0.672276\nCost after iteration 9200: 0.672276\nCost after iteration 9300: 0.672276\nCost after iteration 9400: 0.672276\nCost after iteration 9500: 0.672276\nCost after iteration 9600: 0.672276\nCost after iteration 9700: 0.672276\nCost after iteration 9800: 0.672276\nCost after iteration 9900: 0.672276\nCost after iteration 10000: 0.672276\nCost after iteration 10100: 0.672276\nCost after iteration 10200: 0.672276\nCost after iteration 10300: 0.672276\nCost after iteration 10400: 0.672276\nCost after iteration 10500: 0.672276\nCost after iteration 10600: 0.672276\nCost after iteration 10700: 0.672276\nCost after iteration 10800: 0.672276\nCost after iteration 10900: 0.672276\nCost after iteration 11000: 0.672276\nCost after iteration 11100: 0.672276\nCost after iteration 11200: 0.672276\nCost after iteration 11300: 0.672276\nCost after iteration 11400: 0.672276\nCost after iteration 11500: 0.672276\nCost after iteration 11600: 0.672275\nCost after iteration 11700: 0.672275\nCost after iteration 11800: 0.672275\nCost after iteration 11900: 0.672275\nCost after iteration 12000: 0.672275\nCost after iteration 12100: 0.672275\nCost after iteration 12200: 0.672275\nCost after iteration 12300: 0.672275\nCost after iteration 12400: 0.672275\nCost after iteration 12500: 0.672275\nCost after iteration 12600: 0.672275\nCost after iteration 12700: 0.672275\nCost after iteration 12800: 0.672275\nCost after iteration 12900: 0.672275\nCost after iteration 13000: 0.672275\nCost after iteration 13100: 0.672275\nCost after iteration 13200: 0.672275\nCost after iteration 13300: 0.672275\nCost after iteration 13400: 0.672275\nCost after iteration 13500: 0.672275\nCost after iteration 13600: 0.672275\nCost after iteration 13700: 0.672275\nCost after iteration 13800: 0.672275\nCost after iteration 13900: 0.672275\nCost after iteration 14000: 0.672275\nCost after iteration 14100: 0.672275\nCost after iteration 14200: 0.672275\nCost after iteration 14300: 0.672275\nCost after iteration 14400: 0.672275\nCost after iteration 14500: 0.672275\nCost after iteration 14600: 0.672274\nCost after iteration 14700: 0.672274\nCost after iteration 14800: 0.672274\nCost after iteration 14900: 0.672274\nCost after iteration 15000: 0.672274\nCost after iteration 15100: 0.672274\nCost after iteration 15200: 0.672274\nCost after iteration 15300: 0.672274\nCost after iteration 15400: 0.672274\nCost after iteration 15500: 0.672274\nCost after iteration 15600: 0.672274\nCost after iteration 15700: 0.672274\nCost after iteration 15800: 0.672274\nCost after iteration 15900: 0.672274\nCost after iteration 16000: 0.672274\nCost after iteration 16100: 0.672274\nCost after iteration 16200: 0.672274\nCost after iteration 16300: 0.672274\nCost after iteration 16400: 0.672274\nCost after iteration 16500: 0.672274\nCost after iteration 16600: 0.672274\nCost after iteration 16700: 0.672274\nCost after iteration 16800: 0.672274\nCost after iteration 16900: 0.672274\nCost after iteration 17000: 0.672274\nCost after iteration 17100: 0.672274\nCost after iteration 17200: 0.672274\nCost after iteration 17300: 0.672274\nCost after iteration 17400: 0.672274\nCost after iteration 17500: 0.672274\nCost after iteration 17600: 0.672274\nCost after iteration 17700: 0.672273\nCost after iteration 17800: 0.672273\nCost after iteration 17900: 0.672273\nCost after iteration 18000: 0.672273\nCost after iteration 18100: 0.672273\nCost after iteration 18200: 0.672273\nCost after iteration 18300: 0.672273\nCost after iteration 18400: 0.672273\nCost after iteration 18500: 0.672273\nCost after iteration 18600: 0.672273\nCost after iteration 18700: 0.672273\nCost after iteration 18800: 0.672273\nCost after iteration 18900: 0.672273\nCost after iteration 19000: 0.672273\nCost after iteration 19100: 0.672273\nCost after iteration 19200: 0.672273\nCost after iteration 19300: 0.672273\nCost after iteration 19400: 0.672273\nCost after iteration 19500: 0.672273\nCost after iteration 19600: 0.672273\nCost after iteration 19700: 0.672273\nCost after iteration 19800: 0.672273\nCost after iteration 19900: 0.672273\nCost after iteration 20000: 0.672273\nCost after iteration 20100: 0.672273\nCost after iteration 20200: 0.672273\nCost after iteration 20300: 0.672273\nCost after iteration 20400: 0.672273\nCost after iteration 20500: 0.672273\nCost after iteration 20600: 0.672273\nCost after iteration 20700: 0.672273\nCost after iteration 20800: 0.672272\nCost after iteration 20900: 0.672272\nCost after iteration 21000: 0.672272\nCost after iteration 21100: 0.672272\nCost after iteration 21200: 0.672272\nCost after iteration 21300: 0.672272\nCost after iteration 21400: 0.672272\nCost after iteration 21500: 0.672272\nCost after iteration 21600: 0.672272\nCost after iteration 21700: 0.672272\nCost after iteration 21800: 0.672272\nCost after iteration 21900: 0.672272\nCost after iteration 22000: 0.672272\nCost after iteration 22100: 0.672272\nCost after iteration 22200: 0.672272\nCost after iteration 22300: 0.672272\nCost after iteration 22400: 0.672272\nCost after iteration 22500: 0.672272\nCost after iteration 22600: 0.672272\nCost after iteration 22700: 0.672272\nCost after iteration 22800: 0.672272\nCost after iteration 22900: 0.672272\nCost after iteration 23000: 0.672272\nCost after iteration 23100: 0.672272\nCost after iteration 23200: 0.672272\nCost after iteration 23300: 0.672272\nCost after iteration 23400: 0.672272\nCost after iteration 23500: 0.672272\nCost after iteration 23600: 0.672272\nCost after iteration 23700: 0.672272\nCost after iteration 23800: 0.672272\nCost after iteration 23900: 0.672272\nCost after iteration 24000: 0.672271\nCost after iteration 24100: 0.672271\nCost after iteration 24200: 0.672271\nCost after iteration 24300: 0.672271\nCost after iteration 24400: 0.672271\nCost after iteration 24500: 0.672271\nCost after iteration 24600: 0.672271\nCost after iteration 24700: 0.672271\nCost after iteration 24800: 0.672271\nCost after iteration 24900: 0.672271\nCost after iteration 25000: 0.672271\nCost after iteration 25100: 0.672271\nCost after iteration 25200: 0.672271\nCost after iteration 25300: 0.672271\nCost after iteration 25400: 0.672271\nCost after iteration 25500: 0.672271\nCost after iteration 25600: 0.672271\nCost after iteration 25700: 0.672271\nCost after iteration 25800: 0.672271\nCost after iteration 25900: 0.672271\nCost after iteration 26000: 0.672271\nCost after iteration 26100: 0.672271\nCost after iteration 26200: 0.672271\nCost after iteration 26300: 0.672271\nCost after iteration 26400: 0.672271\nCost after iteration 26500: 0.672271\nCost after iteration 26600: 0.672271\nCost after iteration 26700: 0.672271\nCost after iteration 26800: 0.672271\nCost after iteration 26900: 0.672271\nCost after iteration 27000: 0.672271\nCost after iteration 27100: 0.672271\nCost after iteration 27200: 0.672271\nCost after iteration 27300: 0.672270\nCost after iteration 27400: 0.672270\nCost after iteration 27500: 0.672270\nCost after iteration 27600: 0.672270\nCost after iteration 27700: 0.672270\nCost after iteration 27800: 0.672270\nCost after iteration 27900: 0.672270\nCost after iteration 28000: 0.672270\nCost after iteration 28100: 0.672270\nCost after iteration 28200: 0.672270\nCost after iteration 28300: 0.672270\nCost after iteration 28400: 0.672270\nCost after iteration 28500: 0.672270\nCost after iteration 28600: 0.672270\nCost after iteration 28700: 0.672270\nCost after iteration 28800: 0.672270\nCost after iteration 28900: 0.672270\nCost after iteration 29000: 0.672270\nCost after iteration 29100: 0.672270\nCost after iteration 29200: 0.672270\nCost after iteration 29300: 0.672270\nCost after iteration 29400: 0.672270\nCost after iteration 29500: 0.672270\nCost after iteration 29600: 0.672270\nCost after iteration 29700: 0.672270\nCost after iteration 29800: 0.672270\nCost after iteration 29900: 0.672270\nCost after iteration 30000: 0.672270\nCost after iteration 30100: 0.672270\nCost after iteration 30200: 0.672270\nCost after iteration 30300: 0.672270\nCost after iteration 30400: 0.672270\nCost after iteration 30500: 0.672270\nCost after iteration 30600: 0.672269\nCost after iteration 30700: 0.672269\nCost after iteration 30800: 0.672269\nCost after iteration 30900: 0.672269\nCost after iteration 31000: 0.672269\nCost after iteration 31100: 0.672269\nCost after iteration 31200: 0.672269\nCost after iteration 31300: 0.672269\nCost after iteration 31400: 0.672269\nCost after iteration 31500: 0.672269\nCost after iteration 31600: 0.672269\nCost after iteration 31700: 0.672269\nCost after iteration 31800: 0.672269\nCost after iteration 31900: 0.672269\nCost after iteration 32000: 0.672269\nCost after iteration 32100: 0.672269\nCost after iteration 32200: 0.672269\nCost after iteration 32300: 0.672269\nCost after iteration 32400: 0.672269\nCost after iteration 32500: 0.672269\nCost after iteration 32600: 0.672269\nCost after iteration 32700: 0.672269\nCost after iteration 32800: 0.672269\nCost after iteration 32900: 0.672269\nCost after iteration 33000: 0.672269\nCost after iteration 33100: 0.672269\nCost after iteration 33200: 0.672269\nCost after iteration 33300: 0.672269\nCost after iteration 33400: 0.672269\nCost after iteration 33500: 0.672269\nCost after iteration 33600: 0.672269\nCost after iteration 33700: 0.672269\nCost after iteration 33800: 0.672269\nCost after iteration 33900: 0.672269\nCost after iteration 34000: 0.672268\nCost after iteration 34100: 0.672268\nCost after iteration 34200: 0.672268\nCost after iteration 34300: 0.672268\nCost after iteration 34400: 0.672268\nCost after iteration 34500: 0.672268\nCost after iteration 34600: 0.672268\nCost after iteration 34700: 0.672268\nCost after iteration 34800: 0.672268\nCost after iteration 34900: 0.672268\nCost after iteration 35000: 0.672268\nCost after iteration 35100: 0.672268\nCost after iteration 35200: 0.672268\nCost after iteration 35300: 0.672268\nCost after iteration 35400: 0.672268\nCost after iteration 35500: 0.672268\nCost after iteration 35600: 0.672268\nCost after iteration 35700: 0.672268\nCost after iteration 35800: 0.672268\nCost after iteration 35900: 0.672268\nCost after iteration 36000: 0.672268\nCost after iteration 36100: 0.672268\nCost after iteration 36200: 0.672268\nCost after iteration 36300: 0.672268\nCost after iteration 36400: 0.672268\nCost after iteration 36500: 0.672268\nCost after iteration 36600: 0.672268\nCost after iteration 36700: 0.672268\nCost after iteration 36800: 0.672268\nCost after iteration 36900: 0.672268\nCost after iteration 37000: 0.672268\nCost after iteration 37100: 0.672268\nCost after iteration 37200: 0.672268\nCost after iteration 37300: 0.672268\nCost after iteration 37400: 0.672267\nCost after iteration 37500: 0.672267\nCost after iteration 37600: 0.672267\nCost after iteration 37700: 0.672267\nCost after iteration 37800: 0.672267\nCost after iteration 37900: 0.672267\nCost after iteration 38000: 0.672267\nCost after iteration 38100: 0.672267\nCost after iteration 38200: 0.672267\nCost after iteration 38300: 0.672267\nCost after iteration 38400: 0.672267\nCost after iteration 38500: 0.672267\nCost after iteration 38600: 0.672267\nCost after iteration 38700: 0.672267\nCost after iteration 38800: 0.672267\nCost after iteration 38900: 0.672267\nCost after iteration 39000: 0.672267\nCost after iteration 39100: 0.672267\nCost after iteration 39200: 0.672267\nCost after iteration 39300: 0.672267\nCost after iteration 39400: 0.672267\nCost after iteration 39500: 0.672267\nCost after iteration 39600: 0.672267\nCost after iteration 39700: 0.672267\nCost after iteration 39800: 0.672267\nCost after iteration 39900: 0.672267\nCost after iteration 40000: 0.672267\nCost after iteration 40100: 0.672267\nCost after iteration 40200: 0.672267\nCost after iteration 40300: 0.672267\nCost after iteration 40400: 0.672267\nCost after iteration 40500: 0.672267\nCost after iteration 40600: 0.672267\nCost after iteration 40700: 0.672267\nCost after iteration 40800: 0.672267\nCost after iteration 40900: 0.672266\nCost after iteration 41000: 0.672266\nCost after iteration 41100: 0.672266\nCost after iteration 41200: 0.672266\nCost after iteration 41300: 0.672266\nCost after iteration 41400: 0.672266\nCost after iteration 41500: 0.672266\nCost after iteration 41600: 0.672266\nCost after iteration 41700: 0.672266\nCost after iteration 41800: 0.672266\nCost after iteration 41900: 0.672266\nCost after iteration 42000: 0.672266\nCost after iteration 42100: 0.672266\nCost after iteration 42200: 0.672266\nCost after iteration 42300: 0.672266\nCost after iteration 42400: 0.672266\nCost after iteration 42500: 0.672266\nCost after iteration 42600: 0.672266\nCost after iteration 42700: 0.672266\nCost after iteration 42800: 0.672266\nCost after iteration 42900: 0.672266\nCost after iteration 43000: 0.672266\nCost after iteration 43100: 0.672266\nCost after iteration 43200: 0.672266\nCost after iteration 43300: 0.672266\nCost after iteration 43400: 0.672266\nCost after iteration 43500: 0.672266\nCost after iteration 43600: 0.672266\nCost after iteration 43700: 0.672266\nCost after iteration 43800: 0.672266\nCost after iteration 43900: 0.672266\nCost after iteration 44000: 0.672266\nCost after iteration 44100: 0.672266\nCost after iteration 44200: 0.672266\nCost after iteration 44300: 0.672266\nCost after iteration 44400: 0.672265\nCost after iteration 44500: 0.672265\nCost after iteration 44600: 0.672265\nCost after iteration 44700: 0.672265\nCost after iteration 44800: 0.672265\nCost after iteration 44900: 0.672265\nCost after iteration 45000: 0.672265\nCost after iteration 45100: 0.672265\nCost after iteration 45200: 0.672265\nCost after iteration 45300: 0.672265\nCost after iteration 45400: 0.672265\nCost after iteration 45500: 0.672265\nCost after iteration 45600: 0.672265\nCost after iteration 45700: 0.672265\nCost after iteration 45800: 0.672265\nCost after iteration 45900: 0.672265\nCost after iteration 46000: 0.672265\nCost after iteration 46100: 0.672265\nCost after iteration 46200: 0.672265\nCost after iteration 46300: 0.672265\nCost after iteration 46400: 0.672265\nCost after iteration 46500: 0.672265\nCost after iteration 46600: 0.672265\nCost after iteration 46700: 0.672265\nCost after iteration 46800: 0.672265\nCost after iteration 46900: 0.672265\nCost after iteration 47000: 0.672265\nCost after iteration 47100: 0.672265\nCost after iteration 47200: 0.672265\nCost after iteration 47300: 0.672265\nCost after iteration 47400: 0.672265\nCost after iteration 47500: 0.672265\nCost after iteration 47600: 0.672265\nCost after iteration 47700: 0.672265\nCost after iteration 47800: 0.672265\nCost after iteration 47900: 0.672265\nCost after iteration 48000: 0.672265\nCost after iteration 48100: 0.672264\nCost after iteration 48200: 0.672264\nCost after iteration 48300: 0.672264\nCost after iteration 48400: 0.672264\nCost after iteration 48500: 0.672264\nCost after iteration 48600: 0.672264\nCost after iteration 48700: 0.672264\nCost after iteration 48800: 0.672264\nCost after iteration 48900: 0.672264\nCost after iteration 49000: 0.672264\nCost after iteration 49100: 0.672264\nCost after iteration 49200: 0.672264\nCost after iteration 49300: 0.672264\nCost after iteration 49400: 0.672264\nCost after iteration 49500: 0.672264\nCost after iteration 49600: 0.672264\nCost after iteration 49700: 0.672264\nCost after iteration 49800: 0.672264\nCost after iteration 49900: 0.672264\nè®­ç»ƒé›†å‡†ç¡®ç‡: 57.906458797327396 \næµ‹è¯•é›†å‡†ç¡®ç‡: 49.77777777777778 \n"
      }
     ],
     "persistentId": "b175609a-a7d0-4bf8-84b6-23b93d651481",
     "text": "if __name__ == '__main__':\n    model(X_train, y_train, X_test, y_test, num_iterations=50000, learning_rate=0.001)\n"
    },
    "executionTime": "2020-02-04T05:28:37.689Z"
   },
   {
    "cell": {
     "executionCount": 66,
     "executionEventId": "231101c4-26f0-4a94-a21f-445bc5cfa3fb",
     "hasError": false,
     "id": "f0685016-4bda-4ca9-9976-ae5f4793ece6",
     "outputs": [],
     "persistentId": "39737492-b457-4dcf-8d9f-76a392488151",
     "text": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost=True):\n    '''\n    This function optimize w and b by running a gradient descen algorithm\n    \n    Arguments:\n    w - weights\n    b - bias\n    X - data\n    Y - ground truth\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n    \n    Returns:\n    params - dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n    \n    '''\n    \n    costs = []\n    \n    for i in range(num_iterations):\n        \n        grads, cost = propagate(w,b,X,Y)\n        \n        dw = grads['dw']\n        db = grads['db']\n        \n        w = w - learning_rate * dw\n        b = b - learning_rate * db\n        \n        if i % 500 == 0:\n            costs.append(cost)\n        if print_cost and i % 100 == 0:\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    \n    params = {\"w\":w,\n              \"b\":b}\n    \n    grads = {\"dw\":dw,\n             \"db\":db}\n    \n    return params, grads, costs"
    },
    "executionTime": "2020-02-04T05:33:18.421Z"
   },
   {
    "cell": {
     "executionCount": 67,
     "executionEventId": "2b07617b-15f5-49bd-8629-4791deb7efa1",
     "hasError": false,
     "id": "3dab0397-2b00-4236-ad61-4e8886ec359c",
     "outputs": [],
     "persistentId": "afe4201d-351d-44c7-af57-927730e72d92",
     "text": "def predict(w, b, X):\n    '''\n    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n    \n    Arguments:\n    w -- weights\n    b -- bias \n    X -- data \n    \n    Returns:\n    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n    '''\n    m = X.shape[1]\n    Y_prediction = np.zeros((1,m))\n    w = w.reshape(X.shape[0],1)\n    \n    A = sigmoid(np.dot(w.T,X) + b)\n    \n    for j in range(A.shape[1]):\n        if A[0,j] <= 0.5:\n            Y_prediction[0,j] = 0\n        else:\n            Y_prediction[0,j] = 1\n    \n    assert(Y_prediction.shape == (1,m))\n    \n    return Y_prediction"
    },
    "executionTime": "2020-02-04T05:33:21.727Z"
   },
   {
    "cell": {
     "executionCount": 68,
     "executionEventId": "2828b120-8456-49cf-9b21-d7e95b624ec3",
     "hasError": false,
     "id": "611bea7f-db28-443e-b57e-464cc0492891",
     "outputs": [],
     "persistentId": "736561b1-27f2-4958-a474-6d34622f80bb",
     "text": "def model(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate,print_cost=True):\n    \"\"\"\n    Build the logistic regression model by calling all the functions you have implemented.\n    Arguments:\n    X_train - training set\n    Y_train - training label\n    X_test - test set\n    Y_test - test label\n    num_iteration - hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations è¿™é‡Œæˆ‘æƒ³æ”¹æˆ500ï¼Œprintå¤ªå¤šäº†\n    \n    Returns:\n    d - dictionary should contain following information w,b,training_accuracy, test_accuracy,cost\n    eg: d = {\"w\":w,\n             \"b\":b,\n             \"training_accuracy\": traing_accuracy,\n             \"test_accuracy\":test_accuracy,\n             \"cost\":cost}\n    \"\"\"\n    # æ•°æ®ä¿®æ”¹ä¸ºä¸€ç»´\n    x_train = X_train.reshape(-1,X_train.shape[0])\n    y_train = Y_train.reshape(-1,Y_train.shape[0])\n    x_test = X_test.reshape(-1,X_test.shape[0])\n    y_test = Y_test.reshape(-1,Y_test.shape[0])\n    \n    # åˆå§‹åŒ–å‚æ•°\n    w,b = initialize_parameters(x_train.shape[0])\n    \n    # params:æ›´æ–°åçš„ç½‘ç»œå‚æ•°\n    # grads:æœ€æ–°çš„æ¢¯åº¦(ä¸‹é™æŸå¤±)\n    # costs:æ¯æ¬¡æ›´æ–°çš„æŸå¤±åˆ—è¡¨\n    params, grads, costs = optimize(w,b,x_train,y_train,num_iterations,learning_rate,print_cost)\n    \n    # è·å–è®­ç»ƒçš„å‚æ•°\n    w = params[\"w\"]\n    b = params[\"b\"]\n    \n    # é¢„æµ‹ç»“æœ\n    y_prediction_train = predict(w,b,x_train)\n    y_prediction_test = predict(w,b,x_test)\n    \n#     count_right = np.sum(y_prediction_train == y_train)\n#     accuracy = count_right / y_train.shape[1]\n#     print(accuracy)\n\n    \n    # æ‰“å°å‡†ç¡®ç‡ï¼ŒæŸå¤±\n#     print(\"training setâ€˜s accuracyï¼š{}\".format(np.sum(y_prediction_train == y_train)/ y_train.shape[1]))\n#     print(\"testing setâ€™s accuracyï¼š{}\".format(np.sum(y_prediction_test == y_test)/ y_test.shape[1]))\n    \n    # æ‰“å°å‡†ç¡®ç‡\n    print(\"è®­ç»ƒé›†å‡†ç¡®ç‡: {} \".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"æµ‹è¯•é›†å‡†ç¡®ç‡: {} \".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \n    \n    \n    "
    },
    "executionTime": "2020-02-04T05:33:26.390Z"
   },
   {
    "cell": {
     "executionCount": 69,
     "executionEventId": "29f7a9d4-7d56-4c65-b0f2-a5f8e1508664",
     "hasError": false,
     "id": "1fde88e1-df7e-49c2-ae1f-e396411d30e3",
     "outputs": [
      {
       "name": "stderr",
       "output_type": "stream",
       "text": "/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: divide by zero encountered in log\n  del sys.path[0]\n/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in multiply\n  del sys.path[0]\n"
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "Cost after iteration 0: nan\nCost after iteration 100: nan\nCost after iteration 200: nan\nCost after iteration 300: nan\nCost after iteration 400: 1.291793\nCost after iteration 500: 1.630820\nCost after iteration 600: 1.516530\nCost after iteration 700: 1.501815\nCost after iteration 800: 1.500295\nCost after iteration 900: 1.499972\nCost after iteration 1000: 1.499872\nCost after iteration 1100: 1.499838\nCost after iteration 1200: 1.499827\nCost after iteration 1300: 1.499824\nCost after iteration 1400: 1.499824\nCost after iteration 1500: 1.499827\nCost after iteration 1600: 1.499830\nCost after iteration 1700: 1.499833\nCost after iteration 1800: 1.499836\nCost after iteration 1900: 1.499840\nCost after iteration 2000: 1.499843\nCost after iteration 2100: 1.499847\nCost after iteration 2200: 1.499850\nCost after iteration 2300: 1.499854\nCost after iteration 2400: 1.499857\nCost after iteration 2500: 1.499861\nCost after iteration 2600: 1.499864\nCost after iteration 2700: 1.499868\nCost after iteration 2800: 1.499871\nCost after iteration 2900: 1.499875\nCost after iteration 3000: 1.499878\nCost after iteration 3100: 1.499882\nCost after iteration 3200: 1.499885\nCost after iteration 3300: 1.499888\nCost after iteration 3400: 1.499892\nCost after iteration 3500: 1.499895\nCost after iteration 3600: 1.499899\nCost after iteration 3700: 1.499902\nCost after iteration 3800: 1.499906\nCost after iteration 3900: 1.499909\nCost after iteration 4000: 1.499913\nCost after iteration 4100: 1.499916\nCost after iteration 4200: 1.499919\nCost after iteration 4300: 1.499923\nCost after iteration 4400: 1.499926\nCost after iteration 4500: 1.499930\nCost after iteration 4600: 1.499933\nCost after iteration 4700: 1.499937\nCost after iteration 4800: 1.499940\nCost after iteration 4900: 1.499943\nCost after iteration 5000: 1.499947\nCost after iteration 5100: 1.499950\nCost after iteration 5200: 1.499954\nCost after iteration 5300: 1.499957\nCost after iteration 5400: 1.499961\nCost after iteration 5500: 1.499964\nCost after iteration 5600: 1.499967\nCost after iteration 5700: 1.499971\nCost after iteration 5800: 1.499974\nCost after iteration 5900: 1.499978\nCost after iteration 6000: 1.499981\nCost after iteration 6100: 1.499984\nCost after iteration 6200: 1.499988\nCost after iteration 6300: 1.499991\nCost after iteration 6400: 1.499994\nCost after iteration 6500: 1.499998\nCost after iteration 6600: 1.500001\nCost after iteration 6700: 1.500005\nCost after iteration 6800: 1.500008\nCost after iteration 6900: 1.500011\nCost after iteration 7000: 1.500015\nCost after iteration 7100: 1.500018\nCost after iteration 7200: 1.500021\nCost after iteration 7300: 1.500025\nCost after iteration 7400: 1.500028\nCost after iteration 7500: 1.500031\nCost after iteration 7600: 1.500035\nCost after iteration 7700: 1.500038\nCost after iteration 7800: 1.500042\nCost after iteration 7900: 1.500045\nCost after iteration 8000: 1.500048\nCost after iteration 8100: 1.500052\nCost after iteration 8200: 1.500055\nCost after iteration 8300: 1.500058\nCost after iteration 8400: 1.500062\nCost after iteration 8500: 1.500065\nCost after iteration 8600: 1.500068\nCost after iteration 8700: 1.500071\nCost after iteration 8800: 1.500075\nCost after iteration 8900: 1.500078\nCost after iteration 9000: 1.500081\nCost after iteration 9100: 1.500085\nCost after iteration 9200: 1.500088\nCost after iteration 9300: 1.500091\nCost after iteration 9400: 1.500095\nCost after iteration 9500: 1.500098\nCost after iteration 9600: 1.500101\nCost after iteration 9700: 1.500105\nCost after iteration 9800: 1.500108\nCost after iteration 9900: 1.500111\nè®­ç»ƒé›†å‡†ç¡®ç‡: 50.334075723830736 \næµ‹è¯•é›†å‡†ç¡®ç‡: 53.55555555555556 \n"
      }
     ],
     "persistentId": "b175609a-a7d0-4bf8-84b6-23b93d651481",
     "text": "if __name__ == '__main__':\n    model(X_train, y_train, X_test, y_test, num_iterations=10000, learning_rate=0.01)\n"
    },
    "executionTime": "2020-02-04T05:33:39.380Z"
   },
   {
    "cell": {
     "executionCount": 70,
     "executionEventId": "a6ab96ab-a2a3-439e-a03b-56cf47fc5066",
     "hasError": false,
     "id": "1fde88e1-df7e-49c2-ae1f-e396411d30e3",
     "outputs": [
      {
       "name": "stderr",
       "output_type": "stream",
       "text": "/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: divide by zero encountered in log\n  del sys.path[0]\n/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in multiply\n  del sys.path[0]\n"
      },
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "Cost after iteration 0: nan\nCost after iteration 100: nan\nCost after iteration 200: nan\nCost after iteration 300: nan\nCost after iteration 400: nan\nCost after iteration 500: nan\nCost after iteration 600: nan\nCost after iteration 700: 3.056625\nCost after iteration 800: 1.997950\nCost after iteration 900: 1.179382\nCost after iteration 1000: 0.752513\nCost after iteration 1100: 0.676861\nCost after iteration 1200: 0.672521\nCost after iteration 1300: 0.672313\nCost after iteration 1400: 0.672303\nCost after iteration 1500: 0.672302\nCost after iteration 1600: 0.672302\nCost after iteration 1700: 0.672301\nCost after iteration 1800: 0.672301\nCost after iteration 1900: 0.672301\nCost after iteration 2000: 0.672301\nCost after iteration 2100: 0.672300\nCost after iteration 2200: 0.672300\nCost after iteration 2300: 0.672300\nCost after iteration 2400: 0.672300\nCost after iteration 2500: 0.672299\nCost after iteration 2600: 0.672299\nCost after iteration 2700: 0.672299\nCost after iteration 2800: 0.672299\nCost after iteration 2900: 0.672299\nCost after iteration 3000: 0.672298\nCost after iteration 3100: 0.672298\nCost after iteration 3200: 0.672298\nCost after iteration 3300: 0.672298\nCost after iteration 3400: 0.672297\nCost after iteration 3500: 0.672297\nCost after iteration 3600: 0.672297\nCost after iteration 3700: 0.672297\nCost after iteration 3800: 0.672296\nCost after iteration 3900: 0.672296\nCost after iteration 4000: 0.672296\nCost after iteration 4100: 0.672296\nCost after iteration 4200: 0.672296\nCost after iteration 4300: 0.672295\nCost after iteration 4400: 0.672295\nCost after iteration 4500: 0.672295\nCost after iteration 4600: 0.672295\nCost after iteration 4700: 0.672295\nCost after iteration 4800: 0.672294\nCost after iteration 4900: 0.672294\nCost after iteration 5000: 0.672294\nCost after iteration 5100: 0.672294\nCost after iteration 5200: 0.672293\nCost after iteration 5300: 0.672293\nCost after iteration 5400: 0.672293\nCost after iteration 5500: 0.672293\nCost after iteration 5600: 0.672293\nCost after iteration 5700: 0.672292\nCost after iteration 5800: 0.672292\nCost after iteration 5900: 0.672292\nCost after iteration 6000: 0.672292\nCost after iteration 6100: 0.672292\nCost after iteration 6200: 0.672291\nCost after iteration 6300: 0.672291\nCost after iteration 6400: 0.672291\nCost after iteration 6500: 0.672291\nCost after iteration 6600: 0.672291\nCost after iteration 6700: 0.672290\nCost after iteration 6800: 0.672290\nCost after iteration 6900: 0.672290\nCost after iteration 7000: 0.672290\nCost after iteration 7100: 0.672290\nCost after iteration 7200: 0.672289\nCost after iteration 7300: 0.672289\nCost after iteration 7400: 0.672289\nCost after iteration 7500: 0.672289\nCost after iteration 7600: 0.672289\nCost after iteration 7700: 0.672288\nCost after iteration 7800: 0.672288\nCost after iteration 7900: 0.672288\nCost after iteration 8000: 0.672288\nCost after iteration 8100: 0.672288\nCost after iteration 8200: 0.672287\nCost after iteration 8300: 0.672287\nCost after iteration 8400: 0.672287\nCost after iteration 8500: 0.672287\nCost after iteration 8600: 0.672287\nCost after iteration 8700: 0.672286\nCost after iteration 8800: 0.672286\nCost after iteration 8900: 0.672286\nCost after iteration 9000: 0.672286\nCost after iteration 9100: 0.672286\nCost after iteration 9200: 0.672285\nCost after iteration 9300: 0.672285\nCost after iteration 9400: 0.672285\nCost after iteration 9500: 0.672285\nCost after iteration 9600: 0.672285\nCost after iteration 9700: 0.672284\nCost after iteration 9800: 0.672284\nCost after iteration 9900: 0.672284\nè®­ç»ƒé›†å‡†ç¡®ç‡: 57.906458797327396 \næµ‹è¯•é›†å‡†ç¡®ç‡: 49.77777777777778 \n"
      }
     ],
     "persistentId": "b175609a-a7d0-4bf8-84b6-23b93d651481",
     "text": "if __name__ == '__main__':\n    model(X_train, y_train, X_test, y_test, num_iterations=10000, learning_rate=0.005)\n"
    },
    "executionTime": "2020-02-04T05:33:53.288Z"
   },
   {
    "cell": {
     "executionCount": 71,
     "executionEventId": "98af75c3-78ca-4241-96c3-470787d207c9",
     "hasError": false,
     "id": "2e14e518-c694-442e-8483-58f4a46d56e4",
     "outputs": [],
     "persistentId": "d2518a89-1eeb-44fb-95a8-4eab59019f67",
     "text": "def propagate(w,b,X,Y):\n    '''\n    Implement the cost function and its gradient for the propagation\n    \n    Arguments:\n    w - weights\n    b - bias\n    X - data\n    Y - ground truth\n    '''\n    m = X.shape[1]\n    A = sigmoid(np.dot(w.T,X) + b)\n    cost = -1/m * np.sum(Y * np.log(A + 1e-5) + (1-Y) * np.log(1-A + 1e-5))\n#     print(cost)\n    \n    dw = 1/m * np.dot(X,(A-Y).T)\n    db = 1/m * np.sum(A-Y)\n    \n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    cost = np.squeeze(cost)\n    assert(cost.shape == ())\n    \n    grads = {'dw':dw,\n             'db':db}\n    return grads, cost"
    },
    "executionTime": "2020-02-04T05:37:24.032Z"
   },
   {
    "cell": {
     "executionCount": 72,
     "executionEventId": "4ba6593a-b1ba-45db-b480-48c3a3fbd708",
     "hasError": false,
     "id": "1fde88e1-df7e-49c2-ae1f-e396411d30e3",
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "Cost after iteration 0: 5.534694\nCost after iteration 100: 4.863950\nCost after iteration 200: 4.723861\nCost after iteration 300: 4.573539\nCost after iteration 400: 4.393476\nCost after iteration 500: 4.169203\nCost after iteration 600: 3.900011\nCost after iteration 700: 3.569190\nCost after iteration 800: 3.142705\nCost after iteration 900: 2.594390\nCost after iteration 1000: 1.931907\nCost after iteration 1100: 1.267301\nCost after iteration 1200: 0.821290\nCost after iteration 1300: 0.686247\nCost after iteration 1400: 0.673119\nCost after iteration 1500: 0.672375\nCost after iteration 1600: 0.672334\nCost after iteration 1700: 0.672332\nCost after iteration 1800: 0.672331\nCost after iteration 1900: 0.672331\nCost after iteration 2000: 0.672330\nCost after iteration 2100: 0.672330\nCost after iteration 2200: 0.672330\nCost after iteration 2300: 0.672329\nCost after iteration 2400: 0.672329\nCost after iteration 2500: 0.672329\nCost after iteration 2600: 0.672328\nCost after iteration 2700: 0.672328\nCost after iteration 2800: 0.672328\nCost after iteration 2900: 0.672327\nCost after iteration 3000: 0.672327\nCost after iteration 3100: 0.672327\nCost after iteration 3200: 0.672326\nCost after iteration 3300: 0.672326\nCost after iteration 3400: 0.672325\nCost after iteration 3500: 0.672325\nCost after iteration 3600: 0.672325\nCost after iteration 3700: 0.672324\nCost after iteration 3800: 0.672324\nCost after iteration 3900: 0.672324\nCost after iteration 4000: 0.672323\nCost after iteration 4100: 0.672323\nCost after iteration 4200: 0.672323\nCost after iteration 4300: 0.672322\nCost after iteration 4400: 0.672322\nCost after iteration 4500: 0.672322\nCost after iteration 4600: 0.672321\nCost after iteration 4700: 0.672321\nCost after iteration 4800: 0.672321\nCost after iteration 4900: 0.672320\nCost after iteration 5000: 0.672320\nCost after iteration 5100: 0.672320\nCost after iteration 5200: 0.672319\nCost after iteration 5300: 0.672319\nCost after iteration 5400: 0.672319\nCost after iteration 5500: 0.672318\nCost after iteration 5600: 0.672318\nCost after iteration 5700: 0.672318\nCost after iteration 5800: 0.672317\nCost after iteration 5900: 0.672317\nCost after iteration 6000: 0.672317\nCost after iteration 6100: 0.672316\nCost after iteration 6200: 0.672316\nCost after iteration 6300: 0.672316\nCost after iteration 6400: 0.672315\nCost after iteration 6500: 0.672315\nCost after iteration 6600: 0.672315\nCost after iteration 6700: 0.672314\nCost after iteration 6800: 0.672314\nCost after iteration 6900: 0.672314\nCost after iteration 7000: 0.672313\nCost after iteration 7100: 0.672313\nCost after iteration 7200: 0.672313\nCost after iteration 7300: 0.672313\nCost after iteration 7400: 0.672312\nCost after iteration 7500: 0.672312\nCost after iteration 7600: 0.672312\nCost after iteration 7700: 0.672311\nCost after iteration 7800: 0.672311\nCost after iteration 7900: 0.672311\nCost after iteration 8000: 0.672310\nCost after iteration 8100: 0.672310\nCost after iteration 8200: 0.672310\nCost after iteration 8300: 0.672309\nCost after iteration 8400: 0.672309\nCost after iteration 8500: 0.672309\nCost after iteration 8600: 0.672308\nCost after iteration 8700: 0.672308\nCost after iteration 8800: 0.672308\nCost after iteration 8900: 0.672308\nCost after iteration 9000: 0.672307\nCost after iteration 9100: 0.672307\nCost after iteration 9200: 0.672307\nCost after iteration 9300: 0.672306\nCost after iteration 9400: 0.672306\nCost after iteration 9500: 0.672306\nCost after iteration 9600: 0.672305\nCost after iteration 9700: 0.672305\nCost after iteration 9800: 0.672305\nCost after iteration 9900: 0.672305\nè®­ç»ƒé›†å‡†ç¡®ç‡: 57.906458797327396 \næµ‹è¯•é›†å‡†ç¡®ç‡: 49.77777777777778 \n"
      }
     ],
     "persistentId": "b175609a-a7d0-4bf8-84b6-23b93d651481",
     "text": "if __name__ == '__main__':\n    model(X_train, y_train, X_test, y_test, num_iterations=10000, learning_rate=0.005)\n"
    },
    "executionTime": "2020-02-04T05:37:30.985Z"
   },
   {
    "cell": {
     "executionCount": 73,
     "executionEventId": "ef4372d8-b5ce-491c-b678-81634bd0f00d",
     "hasError": false,
     "id": "2e14e518-c694-442e-8483-58f4a46d56e4",
     "outputs": [],
     "persistentId": "d2518a89-1eeb-44fb-95a8-4eab59019f67",
     "text": "def propagate(w,b,X,Y):\n    '''\n    Implement the cost function and its gradient for the propagation\n    \n    Arguments:\n    w - weights\n    b - bias\n    X - data\n    Y - ground truth\n    '''\n    m = X.shape[1]\n    A = sigmoid(np.dot(w.T,X) + b)\n    cost = -1/m * np.sum(Y * np.log(A + 1e-5) + (1-Y) * np.log(1-A + 1e-5)) #æŸå¤±å‡½æ•°è¿™é‡Œæœ€å¥½åŠ ä¸€ä¸ªå°æ•°ï¼Œé˜²æ­¢é™¤0\n#     print(cost)\n    \n    dw = 1/m * np.dot(X,(A-Y).T)\n    db = 1/m * np.sum(A-Y)\n    \n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    cost = np.squeeze(cost)\n    assert(cost.shape == ())\n    \n    grads = {'dw':dw,\n             'db':db}\n    return grads, cost"
    },
    "executionTime": "2020-02-04T05:38:20.495Z"
   },
   {
    "cell": {
     "executionCount": 74,
     "executionEventId": "c9aaa8a6-8bb9-4cfb-b338-a95a04bdd405",
     "hasError": false,
     "id": "3d3ecbb2-8a49-4008-a62b-e2c48d8d3f3e",
     "outputs": [],
     "persistentId": "4c5120f7-b102-4340-ba26-e02a666ab9f6",
     "text": "# Split the data into training set and test set \nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.2)"
    },
    "executionTime": "2020-02-04T05:38:39.405Z"
   },
   {
    "cell": {
     "executionCount": 75,
     "executionEventId": "b09f1912-6b02-47c2-a885-fad70089a643",
     "hasError": false,
     "id": "88d37c78-04ea-4ceb-836f-ba6c68d1b792",
     "outputs": [],
     "persistentId": "e2580523-6d94-469d-9bde-b795e40ef9dd",
     "text": "# reformulate the label. \n# If the digit is smaller than 5, the label is 0.\n# If the digit is larger than 5, the label is 1.\n\ny_train[y_train < 5 ] = 0\ny_train[y_train >= 5] = 1\ny_test[y_test < 5] = 0\ny_test[y_test >= 5] = 1"
    },
    "executionTime": "2020-02-04T05:38:40.008Z"
   },
   {
    "cell": {
     "executionCount": 76,
     "executionEventId": "b2c8705c-3469-4390-b611-a5bcbda386f6",
     "hasError": false,
     "id": "0402c7fd-ee85-463d-a198-7a6578292ac4",
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "(1437, 64)\n(360, 64)\n(1437,)\n(360,)\n"
      }
     ],
     "persistentId": "6d8bcceb-9c81-4f4f-b84a-fb06e0846465",
     "text": "print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)"
    },
    "executionTime": "2020-02-04T05:38:41.141Z"
   },
   {
    "cell": {
     "executionCount": 77,
     "executionEventId": "c28f7d79-0f6a-447a-a95f-56fea42f2362",
     "hasError": false,
     "id": "b491624c-2615-4642-be33-f30c39b82169",
     "outputs": [],
     "persistentId": "cb15bbde-4a6d-461a-ae73-8bc399b9d35b",
     "text": "import numpy as np\ndef sigmoid(z):\n    '''\n    Compute the sigmoid of z\n    Arguments: z -- a scalar or numpy array of any size.\n    \n    Return:\n    s -- sigmoid(z)\n    '''\n    s = 1./(1 + np.exp(-1 * z))\n    \n    return s"
    },
    "executionTime": "2020-02-04T05:38:45.659Z"
   },
   {
    "cell": {
     "executionCount": 78,
     "executionEventId": "90850d70-4f07-4013-87ad-15cd290c0658",
     "hasError": false,
     "id": "1fde88e1-df7e-49c2-ae1f-e396411d30e3",
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "Cost after iteration 0: 5.391141\nCost after iteration 100: 4.509111\nCost after iteration 200: 4.278550\nCost after iteration 300: 3.970082\nCost after iteration 400: 3.575946\nCost after iteration 500: 3.091429\nCost after iteration 600: 2.480223\nCost after iteration 700: 1.736994\nCost after iteration 800: 1.047098\nCost after iteration 900: 0.722619\nCost after iteration 1000: 0.676617\nCost after iteration 1100: 0.674148\nCost after iteration 1200: 0.674040\nCost after iteration 1300: 0.674035\nCost after iteration 1400: 0.674035\nCost after iteration 1500: 0.674035\nCost after iteration 1600: 0.674035\nCost after iteration 1700: 0.674035\nCost after iteration 1800: 0.674034\nCost after iteration 1900: 0.674034\nCost after iteration 2000: 0.674034\nCost after iteration 2100: 0.674034\nCost after iteration 2200: 0.674034\nCost after iteration 2300: 0.674034\nCost after iteration 2400: 0.674034\nCost after iteration 2500: 0.674034\nCost after iteration 2600: 0.674033\nCost after iteration 2700: 0.674033\nCost after iteration 2800: 0.674033\nCost after iteration 2900: 0.674033\nCost after iteration 3000: 0.674033\nCost after iteration 3100: 0.674033\nCost after iteration 3200: 0.674033\nCost after iteration 3300: 0.674033\nCost after iteration 3400: 0.674032\nCost after iteration 3500: 0.674032\nCost after iteration 3600: 0.674032\nCost after iteration 3700: 0.674032\nCost after iteration 3800: 0.674032\nCost after iteration 3900: 0.674032\nCost after iteration 4000: 0.674032\nCost after iteration 4100: 0.674032\nCost after iteration 4200: 0.674031\nCost after iteration 4300: 0.674031\nCost after iteration 4400: 0.674031\nCost after iteration 4500: 0.674031\nCost after iteration 4600: 0.674031\nCost after iteration 4700: 0.674031\nCost after iteration 4800: 0.674031\nCost after iteration 4900: 0.674031\nCost after iteration 5000: 0.674031\nCost after iteration 5100: 0.674030\nCost after iteration 5200: 0.674030\nCost after iteration 5300: 0.674030\nCost after iteration 5400: 0.674030\nCost after iteration 5500: 0.674030\nCost after iteration 5600: 0.674030\nCost after iteration 5700: 0.674030\nCost after iteration 5800: 0.674030\nCost after iteration 5900: 0.674030\nCost after iteration 6000: 0.674029\nCost after iteration 6100: 0.674029\nCost after iteration 6200: 0.674029\nCost after iteration 6300: 0.674029\nCost after iteration 6400: 0.674029\nCost after iteration 6500: 0.674029\nCost after iteration 6600: 0.674029\nCost after iteration 6700: 0.674029\nCost after iteration 6800: 0.674029\nCost after iteration 6900: 0.674028\nCost after iteration 7000: 0.674028\nCost after iteration 7100: 0.674028\nCost after iteration 7200: 0.674028\nCost after iteration 7300: 0.674028\nCost after iteration 7400: 0.674028\nCost after iteration 7500: 0.674028\nCost after iteration 7600: 0.674028\nCost after iteration 7700: 0.674028\nCost after iteration 7800: 0.674028\nCost after iteration 7900: 0.674027\nCost after iteration 8000: 0.674027\nCost after iteration 8100: 0.674027\nCost after iteration 8200: 0.674027\nCost after iteration 8300: 0.674027\nCost after iteration 8400: 0.674027\nCost after iteration 8500: 0.674027\nCost after iteration 8600: 0.674027\nCost after iteration 8700: 0.674027\nCost after iteration 8800: 0.674026\nCost after iteration 8900: 0.674026\nCost after iteration 9000: 0.674026\nCost after iteration 9100: 0.674026\nCost after iteration 9200: 0.674026\nCost after iteration 9300: 0.674026\nCost after iteration 9400: 0.674026\nCost after iteration 9500: 0.674026\nCost after iteration 9600: 0.674026\nCost after iteration 9700: 0.674026\nCost after iteration 9800: 0.674025\nCost after iteration 9900: 0.674025\nè®­ç»ƒé›†å‡†ç¡®ç‡: 58.24634655532359 \næµ‹è¯•é›†å‡†ç¡®ç‡: 48.05555555555555 \n"
      }
     ],
     "persistentId": "b175609a-a7d0-4bf8-84b6-23b93d651481",
     "text": "if __name__ == '__main__':\n    model(X_train, y_train, X_test, y_test, num_iterations=10000, learning_rate=0.005)\n"
    },
    "executionTime": "2020-02-04T05:38:53.360Z"
   },
   {
    "cell": {
     "executionCount": 79,
     "executionEventId": "00622fb1-15e2-4349-b8d0-2e99bcefe20b",
     "hasError": false,
     "id": "1fde88e1-df7e-49c2-ae1f-e396411d30e3",
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "Cost after iteration 0: 5.664853\nCost after iteration 100: 5.670215\nCost after iteration 200: 5.597504\nCost after iteration 300: 5.472391\nCost after iteration 400: 5.128660\nCost after iteration 500: 5.076192\nCost after iteration 600: 5.035500\nCost after iteration 700: 4.969093\nCost after iteration 800: 4.916464\nCost after iteration 900: 4.869623\nCost after iteration 1000: 4.823006\nCost after iteration 1100: 4.774677\nCost after iteration 1200: 4.723755\nCost after iteration 1300: 4.669365\nCost after iteration 1400: 4.610634\nCost after iteration 1500: 4.547096\nCost after iteration 1600: 4.478937\nCost after iteration 1700: 4.406668\nCost after iteration 1800: 4.330673\nCost after iteration 1900: 4.250849\nCost after iteration 2000: 4.166766\nCost after iteration 2100: 4.077773\nCost after iteration 2200: 3.983280\nCost after iteration 2300: 3.882899\nCost after iteration 2400: 3.776007\nCost after iteration 2500: 3.661158\nCost after iteration 2600: 3.536606\nCost after iteration 2700: 3.401735\nCost after iteration 2800: 3.256771\nCost after iteration 2900: 3.101656\nCost after iteration 3000: 2.936178\nCost after iteration 3100: 2.760746\nCost after iteration 3200: 2.576399\nCost after iteration 3300: 2.384950\nCost after iteration 3400: 2.188940\nCost after iteration 3500: 1.991051\nCost after iteration 3600: 1.794162\nCost after iteration 3700: 1.602248\nCost after iteration 3800: 1.421089\nCost after iteration 3900: 1.255559\nCost after iteration 4000: 1.109303\nCost after iteration 4100: 0.985523\nCost after iteration 4200: 0.886159\nCost after iteration 4300: 0.811280\nCost after iteration 4400: 0.758650\nCost after iteration 4500: 0.724113\nCost after iteration 4600: 0.702773\nCost after iteration 4700: 0.690192\nCost after iteration 4800: 0.683012\nCost after iteration 4900: 0.678997\nCost after iteration 5000: 0.676777\nCost after iteration 5100: 0.675557\nCost after iteration 5200: 0.674887\nCost after iteration 5300: 0.674520\nCost after iteration 5400: 0.674319\nCost after iteration 5500: 0.674208\nCost after iteration 5600: 0.674147\nCost after iteration 5700: 0.674113\nCost after iteration 5800: 0.674095\nCost after iteration 5900: 0.674084\nCost after iteration 6000: 0.674079\nCost after iteration 6100: 0.674075\nCost after iteration 6200: 0.674074\nCost after iteration 6300: 0.674073\nCost after iteration 6400: 0.674072\nCost after iteration 6500: 0.674072\nCost after iteration 6600: 0.674071\nCost after iteration 6700: 0.674071\nCost after iteration 6800: 0.674071\nCost after iteration 6900: 0.674071\nCost after iteration 7000: 0.674071\nCost after iteration 7100: 0.674071\nCost after iteration 7200: 0.674071\nCost after iteration 7300: 0.674071\nCost after iteration 7400: 0.674071\nCost after iteration 7500: 0.674071\nCost after iteration 7600: 0.674071\nCost after iteration 7700: 0.674071\nCost after iteration 7800: 0.674071\nCost after iteration 7900: 0.674071\nCost after iteration 8000: 0.674071\nCost after iteration 8100: 0.674071\nCost after iteration 8200: 0.674071\nCost after iteration 8300: 0.674070\nCost after iteration 8400: 0.674070\nCost after iteration 8500: 0.674070\nCost after iteration 8600: 0.674070\nCost after iteration 8700: 0.674070\nCost after iteration 8800: 0.674070\nCost after iteration 8900: 0.674070\nCost after iteration 9000: 0.674070\nCost after iteration 9100: 0.674070\nCost after iteration 9200: 0.674070\nCost after iteration 9300: 0.674070\nCost after iteration 9400: 0.674070\nCost after iteration 9500: 0.674070\nCost after iteration 9600: 0.674070\nCost after iteration 9700: 0.674070\nCost after iteration 9800: 0.674070\nCost after iteration 9900: 0.674070\nè®­ç»ƒé›†å‡†ç¡®ç‡: 58.037578288100214 \næµ‹è¯•é›†å‡†ç¡®ç‡: 49.72222222222222 \n"
      }
     ],
     "persistentId": "b175609a-a7d0-4bf8-84b6-23b93d651481",
     "text": "if __name__ == '__main__':\n    model(X_train, y_train, X_test, y_test, num_iterations=10000, learning_rate=0.001)\n"
    },
    "executionTime": "2020-02-04T05:39:40.392Z"
   },
   {
    "cell": {
     "executionCount": 80,
     "executionEventId": "c884e601-4f3d-48a5-af70-89aa33746b25",
     "hasError": false,
     "id": "78cedb5d-5d89-4417-8005-0373eb1a9094",
     "outputs": [],
     "persistentId": "a95a0c56-ed75-4037-b248-43adcba4e53d",
     "text": "# Random innitialize the parameters\n\ndef initialize_parameters(dim):\n    '''\n    Argument: dim -- size of the w vector\n    \n    Returns:\n    w -- initialized vector of shape (dim,1)\n    b -- initializaed scalar\n    '''\n    \n    w = np.random.randn(dim,1)\n    b = np.zeros(1)\n    \n    assert(w.shape == (dim,1))\n    assert(isinstance(b,float) or isinstance(b,int))\n    \n    return w,b"
    },
    "executionTime": "2020-02-04T05:43:49.762Z"
   },
   {
    "cell": {
     "executionCount": 81,
     "executionEventId": "746bec76-bc83-44c3-b1dc-6c2f3127d998",
     "hasError": false,
     "id": "611bea7f-db28-443e-b57e-464cc0492891",
     "outputs": [],
     "persistentId": "736561b1-27f2-4958-a474-6d34622f80bb",
     "text": "def model(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate,print_cost=True):\n    \"\"\"\n    Build the logistic regression model by calling all the functions you have implemented.\n    Arguments:\n    X_train - training set\n    Y_train - training label\n    X_test - test set\n    Y_test - test label\n    num_iteration - hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations è¿™é‡Œæˆ‘æƒ³æ”¹æˆ500ï¼Œprintå¤ªå¤šäº†\n    \n    Returns:\n    d - dictionary should contain following information w,b,training_accuracy, test_accuracy,cost\n    eg: d = {\"w\":w,\n             \"b\":b,\n             \"training_accuracy\": traing_accuracy,\n             \"test_accuracy\":test_accuracy,\n             \"cost\":cost}\n    \"\"\"\n    # æ•°æ®ä¿®æ”¹ä¸ºä¸€ç»´\n    x_train = X_train.reshape(-1,X_train.shape[0])\n    y_train = Y_train.reshape(-1,Y_train.shape[0])\n    x_test = X_test.reshape(-1,X_test.shape[0])\n    y_test = Y_test.reshape(-1,Y_test.shape[0])\n    \n    # åˆå§‹åŒ–å‚æ•°\n    w,b = initialize_parameters(x_train.shape[0])\n    \n    # params:æ›´æ–°åçš„ç½‘ç»œå‚æ•°\n    # grads:æœ€æ–°çš„æ¢¯åº¦(ä¸‹é™æŸå¤±)\n    # costs:æ¯æ¬¡æ›´æ–°çš„æŸå¤±åˆ—è¡¨\n    params, grads, costs = optimize(w,b,x_train,y_train,num_iterations,learning_rate,print_cost)\n    \n    # è·å–è®­ç»ƒçš„å‚æ•°\n    w = params[\"w\"]\n    b = params[\"b\"]\n    \n    # é¢„æµ‹ç»“æœ\n    y_prediction_train = predict(w,b,x_train)\n    y_prediction_test = predict(w,b,x_test)\n\n    \n    # æ‰“å°å‡†ç¡®ç‡\n#     print(\"training setâ€˜s accuracyï¼š{}\".format(np.sum(y_prediction_train == y_train)/ y_train.shape[1]))\n#     print(\"testing setâ€™s accuracyï¼š{}\".format(np.sum(y_prediction_test == y_test)/ y_test.shape[1]))\n    \n    # æ‰“å°å‡†ç¡®ç‡\n    print(\"è®­ç»ƒé›†å‡†ç¡®ç‡: {} \".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"æµ‹è¯•é›†å‡†ç¡®ç‡: {} \".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \n    \n    \n    "
    },
    "executionTime": "2020-02-04T05:43:57.205Z"
   },
   {
    "cell": {
     "executionCount": 82,
     "executionEventId": "9caa3718-f9ef-4ff8-8d7b-a3c91b9260c4",
     "hasError": true,
     "id": "1fde88e1-df7e-49c2-ae1f-e396411d30e3",
     "outputs": [
      {
       "ename": "AssertionError",
       "evalue": "",
       "output_type": "error",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
        "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-82-e7209a7b89b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-81-00dd57e936be>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate, print_cost)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# åˆå§‹åŒ–å‚æ•°\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# params:æ›´æ–°åçš„ç½‘ç»œå‚æ•°\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-80-04f2f7614805>\u001b[0m in \u001b[0;36minitialize_parameters\u001b[0;34m(dim)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mAssertionError\u001b[0m: "
       ]
      }
     ],
     "persistentId": "b175609a-a7d0-4bf8-84b6-23b93d651481",
     "text": "if __name__ == '__main__':\n    model(X_train, y_train, X_test, y_test, num_iterations=10000, learning_rate=0.001)\n"
    },
    "executionTime": "2020-02-04T05:43:57.614Z"
   },
   {
    "cell": {
     "executionCount": 83,
     "executionEventId": "22add745-8347-45e0-832a-6a024c450bc3",
     "hasError": true,
     "id": "cd9602a9-ccbe-4d35-93ca-a12749bad5d7",
     "outputs": [
      {
       "ename": "AttributeError",
       "evalue": "module 'numpy' has no attribute 'zero'",
       "output_type": "error",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
        "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-83-2cbcf845d925>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'zero'"
       ]
      }
     ],
     "persistentId": "b2d719a8-91ab-4bcc-b8b0-4cf84e45d95a",
     "text": "np.zero(1)"
    },
    "executionTime": "2020-02-04T05:45:03.993Z"
   },
   {
    "cell": {
     "executionCount": 84,
     "executionEventId": "1aff5556-7436-4af5-abf8-7d5280229ccc",
     "hasError": false,
     "id": "cd9602a9-ccbe-4d35-93ca-a12749bad5d7",
     "outputs": [
      {
       "data": {
        "text/plain": "array([0.])"
       },
       "execution_count": 84,
       "metadata": {},
       "output_type": "execute_result"
      }
     ],
     "persistentId": "b2d719a8-91ab-4bcc-b8b0-4cf84e45d95a",
     "text": "np.zeros(1)"
    },
    "executionTime": "2020-02-04T05:45:11.189Z"
   },
   {
    "cell": {
     "executionCount": 85,
     "executionEventId": "043b2165-0686-4a8e-9967-822fc2db80c0",
     "hasError": false,
     "id": "cd9602a9-ccbe-4d35-93ca-a12749bad5d7",
     "outputs": [
      {
       "data": {
        "text/plain": "array([0.10286053])"
       },
       "execution_count": 85,
       "metadata": {},
       "output_type": "execute_result"
      }
     ],
     "persistentId": "b2d719a8-91ab-4bcc-b8b0-4cf84e45d95a",
     "text": "np.random.randn(1)"
    },
    "executionTime": "2020-02-04T05:49:44.747Z"
   },
   {
    "cell": {
     "executionCount": 86,
     "executionEventId": "b1178c09-b9ca-43d3-aeab-92f829e34138",
     "hasError": false,
     "id": "78cedb5d-5d89-4417-8005-0373eb1a9094",
     "outputs": [],
     "persistentId": "a95a0c56-ed75-4037-b248-43adcba4e53d",
     "text": "# Random innitialize the parameters\n\ndef initialize_parameters(dim):\n    '''\n    Argument: dim -- size of the w vector\n    \n    Returns:\n    w -- initialized vector of shape (dim,1)\n    b -- initializaed scalar\n    '''\n    \n    w = np.random.randn(dim,1)\n    b = np.random.randn(1)\n    \n    assert(w.shape == (dim,1))\n    assert(isinstance(b,float) or isinstance(b,int))\n    \n    return w,b"
    },
    "executionTime": "2020-02-04T05:50:01.747Z"
   },
   {
    "cell": {
     "executionCount": 87,
     "executionEventId": "97fffafa-f6d3-4d8a-873f-5f76c5fe69de",
     "hasError": false,
     "id": "611bea7f-db28-443e-b57e-464cc0492891",
     "outputs": [],
     "persistentId": "736561b1-27f2-4958-a474-6d34622f80bb",
     "text": "def model(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate,print_cost=True):\n    \"\"\"\n    Build the logistic regression model by calling all the functions you have implemented.\n    Arguments:\n    X_train - training set\n    Y_train - training label\n    X_test - test set\n    Y_test - test label\n    num_iteration - hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations è¿™é‡Œæˆ‘æƒ³æ”¹æˆ500ï¼Œprintå¤ªå¤šäº†\n    \n    Returns:\n    d - dictionary should contain following information w,b,training_accuracy, test_accuracy,cost\n    eg: d = {\"w\":w,\n             \"b\":b,\n             \"training_accuracy\": traing_accuracy,\n             \"test_accuracy\":test_accuracy,\n             \"cost\":cost}\n    \"\"\"\n    # æ•°æ®ä¿®æ”¹ä¸ºä¸€ç»´\n    x_train = X_train.reshape(-1,X_train.shape[0])\n    y_train = Y_train.reshape(-1,Y_train.shape[0])\n    x_test = X_test.reshape(-1,X_test.shape[0])\n    y_test = Y_test.reshape(-1,Y_test.shape[0])\n    \n    # åˆå§‹åŒ–å‚æ•°\n    w,b = initialize_parameters(x_train.shape[0])\n    \n    # params:æ›´æ–°åçš„ç½‘ç»œå‚æ•°\n    # grads:æœ€æ–°çš„æ¢¯åº¦(ä¸‹é™æŸå¤±)\n    # costs:æ¯æ¬¡æ›´æ–°çš„æŸå¤±åˆ—è¡¨\n    params, grads, costs = optimize(w,b,x_train,y_train,num_iterations,learning_rate,print_cost)\n    \n    # è·å–è®­ç»ƒçš„å‚æ•°\n    w = params[\"w\"]\n    b = params[\"b\"]\n    \n    # é¢„æµ‹ç»“æœ\n    y_prediction_train = predict(w,b,x_train)\n    y_prediction_test = predict(w,b,x_test)\n\n    \n    # æ‰“å°å‡†ç¡®ç‡\n#     print(\"training setâ€˜s accuracyï¼š{}\".format(np.sum(y_prediction_train == y_train)/ y_train.shape[1]))\n#     print(\"testing setâ€™s accuracyï¼š{}\".format(np.sum(y_prediction_test == y_test)/ y_test.shape[1]))\n    \n    # æ‰“å°å‡†ç¡®ç‡\n    print(\"è®­ç»ƒé›†å‡†ç¡®ç‡: {} \".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"æµ‹è¯•é›†å‡†ç¡®ç‡: {} \".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \n    \n    \n    "
    },
    "executionTime": "2020-02-04T05:50:12.640Z"
   },
   {
    "cell": {
     "executionCount": 88,
     "executionEventId": "52f464f0-3800-4a77-b5ac-e8daf1b9e088",
     "hasError": true,
     "id": "1fde88e1-df7e-49c2-ae1f-e396411d30e3",
     "outputs": [
      {
       "ename": "AssertionError",
       "evalue": "",
       "output_type": "error",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
        "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-88-e7209a7b89b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-87-00dd57e936be>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate, print_cost)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# åˆå§‹åŒ–å‚æ•°\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# params:æ›´æ–°åçš„ç½‘ç»œå‚æ•°\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-86-b69156043357>\u001b[0m in \u001b[0;36minitialize_parameters\u001b[0;34m(dim)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mAssertionError\u001b[0m: "
       ]
      }
     ],
     "persistentId": "b175609a-a7d0-4bf8-84b6-23b93d651481",
     "text": "if __name__ == '__main__':\n    model(X_train, y_train, X_test, y_test, num_iterations=10000, learning_rate=0.001)\n"
    },
    "executionTime": "2020-02-04T05:50:13.222Z"
   },
   {
    "cell": {
     "executionCount": 89,
     "executionEventId": "3813672d-0656-4a01-bc8c-ba3e535090c5",
     "hasError": false,
     "id": "78cedb5d-5d89-4417-8005-0373eb1a9094",
     "outputs": [],
     "persistentId": "a95a0c56-ed75-4037-b248-43adcba4e53d",
     "text": "# Random innitialize the parameters\n\ndef initialize_parameters(dim):\n    '''\n    Argument: dim -- size of the w vector\n    \n    Returns:\n    w -- initialized vector of shape (dim,1)\n    b -- initializaed scalar\n    '''\n    \n    w = np.random.randn(dim,1)\n    b = 0\n    \n    assert(w.shape == (dim,1))\n    assert(isinstance(b,float) or isinstance(b,int))\n    \n    return w,b"
    },
    "executionTime": "2020-02-04T05:50:47.019Z"
   },
   {
    "cell": {
     "executionCount": 90,
     "executionEventId": "7843c57f-4800-4e31-a15e-ab514ecc093b",
     "hasError": false,
     "id": "611bea7f-db28-443e-b57e-464cc0492891",
     "outputs": [],
     "persistentId": "736561b1-27f2-4958-a474-6d34622f80bb",
     "text": "def model(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate,print_cost=True):\n    \"\"\"\n    Build the logistic regression model by calling all the functions you have implemented.\n    Arguments:\n    X_train - training set\n    Y_train - training label\n    X_test - test set\n    Y_test - test label\n    num_iteration - hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations è¿™é‡Œæˆ‘æƒ³æ”¹æˆ500ï¼Œprintå¤ªå¤šäº†\n    \n    Returns:\n    d - dictionary should contain following information w,b,training_accuracy, test_accuracy,cost\n    eg: d = {\"w\":w,\n             \"b\":b,\n             \"training_accuracy\": traing_accuracy,\n             \"test_accuracy\":test_accuracy,\n             \"cost\":cost}\n    \"\"\"\n    # æ•°æ®ä¿®æ”¹ä¸ºä¸€ç»´\n    x_train = X_train.reshape(-1,X_train.shape[0])\n    y_train = Y_train.reshape(-1,Y_train.shape[0])\n    x_test = X_test.reshape(-1,X_test.shape[0])\n    y_test = Y_test.reshape(-1,Y_test.shape[0])\n    \n    # åˆå§‹åŒ–å‚æ•°\n    w,b = initialize_parameters(x_train.shape[0])\n    \n    # params:æ›´æ–°åçš„ç½‘ç»œå‚æ•°\n    # grads:æœ€æ–°çš„æ¢¯åº¦(ä¸‹é™æŸå¤±)\n    # costs:æ¯æ¬¡æ›´æ–°çš„æŸå¤±åˆ—è¡¨\n    params, grads, costs = optimize(w,b,x_train,y_train,num_iterations,learning_rate,print_cost)\n    \n    # è·å–è®­ç»ƒçš„å‚æ•°\n    w = params[\"w\"]\n    b = params[\"b\"]\n    \n    # é¢„æµ‹ç»“æœ\n    y_prediction_train = predict(w,b,x_train)\n    y_prediction_test = predict(w,b,x_test)\n\n    \n    # æ‰“å°å‡†ç¡®ç‡\n#     print(\"training setâ€˜s accuracyï¼š{}\".format(np.sum(y_prediction_train == y_train)/ y_train.shape[1]))\n#     print(\"testing setâ€™s accuracyï¼š{}\".format(np.sum(y_prediction_test == y_test)/ y_test.shape[1]))\n    \n    # æ‰“å°å‡†ç¡®ç‡\n    print(\"è®­ç»ƒé›†å‡†ç¡®ç‡: {} \".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"æµ‹è¯•é›†å‡†ç¡®ç‡: {} \".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \n    \n    \n"
    },
    "executionTime": "2020-02-04T05:50:53.467Z"
   },
   {
    "cell": {
     "executionCount": 91,
     "executionEventId": "261c8580-2760-484f-a86a-bcf00a816755",
     "hasError": false,
     "id": "3dab0397-2b00-4236-ad61-4e8886ec359c",
     "outputs": [],
     "persistentId": "afe4201d-351d-44c7-af57-927730e72d92",
     "text": "def predict(w, b, X):\n    '''\n    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n    \n    Arguments:\n    w -- weights\n    b -- bias \n    X -- data \n    \n    Returns:\n    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n    '''\n    m = X.shape[1]\n    Y_prediction = np.zeros((1,m))\n    w = w.reshape(X.shape[0],1)\n    \n    A = sigmoid(np.dot(w.T,X) + b)\n    \n    for j in range(A.shape[1]):\n        if A[0,j] <= 0.5:\n            Y_prediction[0,j] = 0\n        else:\n            Y_prediction[0,j] = 1\n    \n    assert(Y_prediction.shape == (1,m))\n    \n    return Y_prediction"
    },
    "executionTime": "2020-02-04T05:52:23.095Z"
   },
   {
    "cell": {
     "executionCount": 92,
     "executionEventId": "0a3cf2b0-0c37-4a74-8951-f0d5533213fe",
     "hasError": false,
     "id": "611bea7f-db28-443e-b57e-464cc0492891",
     "outputs": [],
     "persistentId": "736561b1-27f2-4958-a474-6d34622f80bb",
     "text": "def model(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate,print_cost=True):\n    \"\"\"\n    Build the logistic regression model by calling all the functions you have implemented.\n    Arguments:\n    X_train - training set\n    Y_train - training label\n    X_test - test set\n    Y_test - test label\n    num_iteration - hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations è¿™é‡Œæˆ‘æƒ³æ”¹æˆ500ï¼Œprintå¤ªå¤šäº†\n    \n    Returns:\n    d - dictionary should contain following information w,b,training_accuracy, test_accuracy,cost\n    eg: d = {\"w\":w,\n             \"b\":b,\n             \"training_accuracy\": traing_accuracy,\n             \"test_accuracy\":test_accuracy,\n             \"cost\":cost}\n    \"\"\"\n    # æ•°æ®ä¿®æ”¹ä¸ºä¸€ç»´\n    x_train = X_train.reshape(-1,X_train.shape[0])\n    y_train = Y_train.reshape(-1,Y_train.shape[0])\n    x_test = X_test.reshape(-1,X_test.shape[0])\n    y_test = Y_test.reshape(-1,Y_test.shape[0])\n    \n    # åˆå§‹åŒ–å‚æ•°\n    w,b = initialize_parameters(x_train.shape[0])\n    \n    # params:æ›´æ–°åçš„ç½‘ç»œå‚æ•°\n    # grads:æœ€æ–°çš„æ¢¯åº¦(ä¸‹é™æŸå¤±)\n    # costs:æ¯æ¬¡æ›´æ–°çš„æŸå¤±åˆ—è¡¨\n    params, grads, costs = optimize(w,b,x_train,y_train,num_iterations,learning_rate,print_cost)\n    \n    # è·å–è®­ç»ƒçš„å‚æ•°\n    w = params[\"w\"]\n    b = params[\"b\"]\n    \n    # é¢„æµ‹ç»“æœ\n    y_prediction_train = predict(w,b,x_train)\n    y_prediction_test = predict(w,b,x_test)\n\n    \n    # æ‰“å°å‡†ç¡®ç‡\n#     print(\"training setâ€˜s accuracyï¼š{}\".format(np.sum(y_prediction_train == y_train)/ y_train.shape[1]))\n#     print(\"testing setâ€™s accuracyï¼š{}\".format(np.sum(y_prediction_test == y_test)/ y_test.shape[1]))\n    \n    # æ‰“å°å‡†ç¡®ç‡\n    print(\"è®­ç»ƒé›†å‡†ç¡®ç‡: {} \".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"æµ‹è¯•é›†å‡†ç¡®ç‡: {} \".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \n    \n    \n"
    },
    "executionTime": "2020-02-04T05:52:27.706Z"
   },
   {
    "cell": {
     "executionCount": 93,
     "executionEventId": "4ef2df76-30e2-4d7c-97f9-831db9299e3c",
     "hasError": false,
     "id": "1fde88e1-df7e-49c2-ae1f-e396411d30e3",
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "Cost after iteration 0: 5.531571\nCost after iteration 100: 5.308899\nCost after iteration 200: 5.031354\nCost after iteration 300: 5.039777\nCost after iteration 400: 5.007623\nCost after iteration 500: 4.969616\nCost after iteration 600: 4.929431\nCost after iteration 700: 4.888229\nCost after iteration 800: 4.843805\nCost after iteration 900: 4.795716\nCost after iteration 1000: 4.743908\nCost after iteration 1100: 4.688556\nCost after iteration 1200: 4.629661\nCost after iteration 1300: 4.566911\nCost after iteration 1400: 4.500478\nCost after iteration 1500: 4.430975\nCost after iteration 1600: 4.358862\nCost after iteration 1700: 4.284235\nCost after iteration 1800: 4.206614\nCost after iteration 1900: 4.125047\nCost after iteration 2000: 4.038469\nCost after iteration 2100: 3.946078\nCost after iteration 2200: 3.847696\nCost after iteration 2300: 3.743566\nCost after iteration 2400: 3.633799\nCost after iteration 2500: 3.517810\nCost after iteration 2600: 3.394532\nCost after iteration 2700: 3.263290\nCost after iteration 2800: 3.123791\nCost after iteration 2900: 2.975726\nCost after iteration 3000: 2.818686\nCost after iteration 3100: 2.652592\nCost after iteration 3200: 2.478149\nCost after iteration 3300: 2.296368\nCost after iteration 3400: 2.108242\nCost after iteration 3500: 1.915530\nCost after iteration 3600: 1.722472\nCost after iteration 3700: 1.535153\nCost after iteration 3800: 1.358550\nCost after iteration 3900: 1.196683\nCost after iteration 4000: 1.054225\nCost after iteration 4100: 0.935431\nCost after iteration 4200: 0.842816\nCost after iteration 4300: 0.776257\nCost after iteration 4400: 0.732499\nCost after iteration 4500: 0.706049\nCost after iteration 4600: 0.691096\nCost after iteration 4700: 0.683009\nCost after iteration 4800: 0.678740\nCost after iteration 4900: 0.676507\nCost after iteration 5000: 0.675340\nCost after iteration 5100: 0.674727\nCost after iteration 5200: 0.674404\nCost after iteration 5300: 0.674232\nCost after iteration 5400: 0.674140\nCost after iteration 5500: 0.674090\nCost after iteration 5600: 0.674063\nCost after iteration 5700: 0.674048\nCost after iteration 5800: 0.674040\nCost after iteration 5900: 0.674036\nCost after iteration 6000: 0.674033\nCost after iteration 6100: 0.674032\nCost after iteration 6200: 0.674031\nCost after iteration 6300: 0.674031\nCost after iteration 6400: 0.674030\nCost after iteration 6500: 0.674030\nCost after iteration 6600: 0.674030\nCost after iteration 6700: 0.674030\nCost after iteration 6800: 0.674030\nCost after iteration 6900: 0.674030\nCost after iteration 7000: 0.674030\nCost after iteration 7100: 0.674030\nCost after iteration 7200: 0.674030\nCost after iteration 7300: 0.674030\nCost after iteration 7400: 0.674030\nCost after iteration 7500: 0.674030\nCost after iteration 7600: 0.674030\nCost after iteration 7700: 0.674030\nCost after iteration 7800: 0.674030\nCost after iteration 7900: 0.674030\nCost after iteration 8000: 0.674030\nCost after iteration 8100: 0.674030\nCost after iteration 8200: 0.674030\nCost after iteration 8300: 0.674030\nCost after iteration 8400: 0.674030\nCost after iteration 8500: 0.674030\nCost after iteration 8600: 0.674030\nCost after iteration 8700: 0.674030\nCost after iteration 8800: 0.674030\nCost after iteration 8900: 0.674030\nCost after iteration 9000: 0.674030\nCost after iteration 9100: 0.674030\nCost after iteration 9200: 0.674029\nCost after iteration 9300: 0.674029\nCost after iteration 9400: 0.674029\nCost after iteration 9500: 0.674029\nCost after iteration 9600: 0.674029\nCost after iteration 9700: 0.674029\nCost after iteration 9800: 0.674029\nCost after iteration 9900: 0.674029\nè®­ç»ƒé›†å‡†ç¡®ç‡: 58.24634655532359 \næµ‹è¯•é›†å‡†ç¡®ç‡: 47.22222222222222 \n"
      }
     ],
     "persistentId": "b175609a-a7d0-4bf8-84b6-23b93d651481",
     "text": "if __name__ == '__main__':\n    model(X_train, y_train, X_test, y_test, num_iterations=10000, learning_rate=0.001)\n"
    },
    "executionTime": "2020-02-04T05:52:29.535Z"
   },
   {
    "cell": {
     "executionCount": 94,
     "executionEventId": "5b6b31f9-8332-427e-b950-f580d82d7d2c",
     "hasError": false,
     "id": "1fde88e1-df7e-49c2-ae1f-e396411d30e3",
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "Cost after iteration 0: 5.188845\nCost after iteration 100: 5.132163\nCost after iteration 200: 5.039845\nCost after iteration 300: 4.984321\nCost after iteration 400: 4.942232\nCost after iteration 500: 4.885943\nCost after iteration 600: 4.816744\nCost after iteration 700: 4.759779\nCost after iteration 800: 4.724195\nCost after iteration 900: 4.703509\nCost after iteration 1000: 4.689720\nCost after iteration 1100: 4.678718\nCost after iteration 1200: 4.668469\nCost after iteration 1300: 4.658104\nCost after iteration 1400: 4.647340\nCost after iteration 1500: 4.636118\nCost after iteration 1600: 4.624433\nCost after iteration 1700: 4.612290\nCost after iteration 1800: 4.599681\nCost after iteration 1900: 4.586595\nCost after iteration 2000: 4.573019\nCost after iteration 2100: 4.558952\nCost after iteration 2200: 4.544408\nCost after iteration 2300: 4.529422\nCost after iteration 2400: 4.514044\nCost after iteration 2500: 4.498342\nCost after iteration 2600: 4.482397\nCost after iteration 2700: 4.466293\nCost after iteration 2800: 4.450104\nCost after iteration 2900: 4.433895\nCost after iteration 3000: 4.417709\nCost after iteration 3100: 4.401567\nCost after iteration 3200: 4.385468\nCost after iteration 3300: 4.369391\nCost after iteration 3400: 4.353288\nCost after iteration 3500: 4.337087\nCost after iteration 3600: 4.320695\nCost after iteration 3700: 4.304006\nCost after iteration 3800: 4.286912\nCost after iteration 3900: 4.269315\nCost after iteration 4000: 4.251133\nCost after iteration 4100: 4.232298\nCost after iteration 4200: 4.212764\nCost after iteration 4300: 4.192499\nCost after iteration 4400: 4.171484\nCost after iteration 4500: 4.149702\nCost after iteration 4600: 4.127131\nCost after iteration 4700: 4.103739\nCost after iteration 4800: 4.079487\nCost after iteration 4900: 4.054350\nCost after iteration 5000: 4.028331\nCost after iteration 5100: 4.001463\nCost after iteration 5200: 3.973794\nCost after iteration 5300: 3.945360\nCost after iteration 5400: 3.916174\nCost after iteration 5500: 3.886225\nCost after iteration 5600: 3.855480\nCost after iteration 5700: 3.823894\nCost after iteration 5800: 3.791409\nCost after iteration 5900: 3.757954\nCost after iteration 6000: 3.723442\nCost after iteration 6100: 3.687769\nCost after iteration 6200: 3.650819\nCost after iteration 6300: 3.612484\nCost after iteration 6400: 3.572669\nCost after iteration 6500: 3.531314\nCost after iteration 6600: 3.488395\nCost after iteration 6700: 3.443936\nCost after iteration 6800: 3.397995\nCost after iteration 6900: 3.350644\nCost after iteration 7000: 3.301929\nCost after iteration 7100: 3.251845\nCost after iteration 7200: 3.200334\nCost after iteration 7300: 3.147318\nCost after iteration 7400: 3.092732\nCost after iteration 7500: 3.036562\nCost after iteration 7600: 2.978841\nCost after iteration 7700: 2.919633\nCost after iteration 7800: 2.858999\nCost after iteration 7900: 2.796977\nCost after iteration 8000: 2.733581\nCost after iteration 8100: 2.668815\nCost after iteration 8200: 2.602684\nCost after iteration 8300: 2.535196\nCost after iteration 8400: 2.466374\nCost after iteration 8500: 2.396262\nCost after iteration 8600: 2.324944\nCost after iteration 8700: 2.252538\nCost after iteration 8800: 2.179187\nCost after iteration 8900: 2.105058\nCost after iteration 9000: 2.030343\nCost after iteration 9100: 1.955272\nCost after iteration 9200: 1.880121\nCost after iteration 9300: 1.805200\nCost after iteration 9400: 1.730839\nCost after iteration 9500: 1.657345\nCost after iteration 9600: 1.584961\nCost after iteration 9700: 1.513855\nCost after iteration 9800: 1.444178\nCost after iteration 9900: 1.376133\nè®­ç»ƒé›†å‡†ç¡®ç‡: 54.41892832289492 \næµ‹è¯•é›†å‡†ç¡®ç‡: 49.72222222222222 \n"
      }
     ],
     "persistentId": "b175609a-a7d0-4bf8-84b6-23b93d651481",
     "text": "if __name__ == '__main__':\n    model(X_train, y_train, X_test, y_test, num_iterations=10000, learning_rate=0.0005)\n"
    },
    "executionTime": "2020-02-04T05:52:52.622Z"
   },
   {
    "cell": {
     "executionCount": 95,
     "executionEventId": "732c8438-5d61-4aea-83c9-029fe0b15c22",
     "hasError": false,
     "id": "1fde88e1-df7e-49c2-ae1f-e396411d30e3",
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "Cost after iteration 0: 5.503487\nCost after iteration 100: 4.652030\nCost after iteration 200: 4.126478\nCost after iteration 300: 3.245647\nCost after iteration 400: 1.807427\nCost after iteration 500: 0.721400\nCost after iteration 600: 1.573052\nCost after iteration 700: 1.538352\nCost after iteration 800: 1.534747\nCost after iteration 900: 1.534011\nCost after iteration 1000: 1.533807\nCost after iteration 1100: 1.533747\nCost after iteration 1200: 1.533730\nCost after iteration 1300: 1.533726\nCost after iteration 1400: 1.533727\nCost after iteration 1500: 1.533730\nCost after iteration 1600: 1.533733\nCost after iteration 1700: 1.533736\nCost after iteration 1800: 1.533740\nCost after iteration 1900: 1.533743\nCost after iteration 2000: 1.533746\nCost after iteration 2100: 1.533750\nCost after iteration 2200: 1.533753\nCost after iteration 2300: 1.533756\nCost after iteration 2400: 1.533760\nCost after iteration 2500: 1.533763\nCost after iteration 2600: 1.533766\nCost after iteration 2700: 1.533770\nCost after iteration 2800: 1.533773\nCost after iteration 2900: 1.533777\nCost after iteration 3000: 1.533780\nCost after iteration 3100: 1.533783\nCost after iteration 3200: 1.533787\nCost after iteration 3300: 1.533790\nCost after iteration 3400: 1.533793\nCost after iteration 3500: 1.533797\nCost after iteration 3600: 1.533800\nCost after iteration 3700: 1.533803\nCost after iteration 3800: 1.533806\nCost after iteration 3900: 1.533810\nCost after iteration 4000: 1.533813\nCost after iteration 4100: 1.533816\nCost after iteration 4200: 1.533820\nCost after iteration 4300: 1.533823\nCost after iteration 4400: 1.533826\nCost after iteration 4500: 1.533830\nCost after iteration 4600: 1.533833\nCost after iteration 4700: 1.533836\nCost after iteration 4800: 1.533840\nCost after iteration 4900: 1.533843\nCost after iteration 5000: 1.533846\nCost after iteration 5100: 1.533849\nCost after iteration 5200: 1.533853\nCost after iteration 5300: 1.533856\nCost after iteration 5400: 1.533859\nCost after iteration 5500: 1.533862\nCost after iteration 5600: 1.533866\nCost after iteration 5700: 1.533869\nCost after iteration 5800: 1.533872\nCost after iteration 5900: 1.533876\nCost after iteration 6000: 1.533879\nCost after iteration 6100: 1.533882\nCost after iteration 6200: 1.533885\nCost after iteration 6300: 1.533889\nCost after iteration 6400: 1.533892\nCost after iteration 6500: 1.533895\nCost after iteration 6600: 1.533898\nCost after iteration 6700: 1.533902\nCost after iteration 6800: 1.533905\nCost after iteration 6900: 1.533908\nCost after iteration 7000: 1.533911\nCost after iteration 7100: 1.533914\nCost after iteration 7200: 1.533918\nCost after iteration 7300: 1.533921\nCost after iteration 7400: 1.533924\nCost after iteration 7500: 1.533927\nCost after iteration 7600: 1.533931\nCost after iteration 7700: 1.533934\nCost after iteration 7800: 1.533937\nCost after iteration 7900: 1.533940\nCost after iteration 8000: 1.533943\nCost after iteration 8100: 1.533947\nCost after iteration 8200: 1.533950\nCost after iteration 8300: 1.533953\nCost after iteration 8400: 1.533956\nCost after iteration 8500: 1.533959\nCost after iteration 8600: 1.533963\nCost after iteration 8700: 1.533966\nCost after iteration 8800: 1.533969\nCost after iteration 8900: 1.533972\nCost after iteration 9000: 1.533975\nCost after iteration 9100: 1.533978\nCost after iteration 9200: 1.533982\nCost after iteration 9300: 1.533985\nCost after iteration 9400: 1.533988\nCost after iteration 9500: 1.533991\nCost after iteration 9600: 1.533994\nCost after iteration 9700: 1.533997\nCost after iteration 9800: 1.534001\nCost after iteration 9900: 1.534004\nè®­ç»ƒé›†å‡†ç¡®ç‡: 50.86986778009743 \næµ‹è¯•é›†å‡†ç¡®ç‡: 47.5 \n"
      }
     ],
     "persistentId": "b175609a-a7d0-4bf8-84b6-23b93d651481",
     "text": "if __name__ == '__main__':\n    model(X_train, y_train, X_test, y_test, num_iterations=10000, learning_rate=0.01)\n"
    },
    "executionTime": "2020-02-04T05:53:03.152Z"
   },
   {
    "cell": {
     "executionCount": 96,
     "executionEventId": "24c0ddb9-75c7-441a-ad35-24e11c5b6775",
     "hasError": false,
     "id": "1fde88e1-df7e-49c2-ae1f-e396411d30e3",
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "Cost after iteration 0: 5.235663\nCost after iteration 100: 4.739852\nCost after iteration 200: 4.573378\nCost after iteration 300: 4.491750\nCost after iteration 400: 4.397550\nCost after iteration 500: 4.288133\nCost after iteration 600: 4.162706\nCost after iteration 700: 4.018243\nCost after iteration 800: 3.847538\nCost after iteration 900: 3.640376\nCost after iteration 1000: 3.383037\nCost after iteration 1100: 3.063827\nCost after iteration 1200: 2.676803\nCost after iteration 1300: 2.214619\nCost after iteration 1400: 1.694310\nCost after iteration 1500: 1.202098\nCost after iteration 1600: 0.856805\nCost after iteration 1700: 0.710439\nCost after iteration 1800: 0.679306\nCost after iteration 1900: 0.674790\nCost after iteration 2000: 0.674158\nCost after iteration 2100: 0.674065\nCost after iteration 2200: 0.674050\nCost after iteration 2300: 0.674048\nCost after iteration 2400: 0.674047\nCost after iteration 2500: 0.674047\nCost after iteration 2600: 0.674047\nCost after iteration 2700: 0.674047\nCost after iteration 2800: 0.674047\nCost after iteration 2900: 0.674047\nCost after iteration 3000: 0.674047\nCost after iteration 3100: 0.674047\nCost after iteration 3200: 0.674047\nCost after iteration 3300: 0.674047\nCost after iteration 3400: 0.674046\nCost after iteration 3500: 0.674046\nCost after iteration 3600: 0.674046\nCost after iteration 3700: 0.674046\nCost after iteration 3800: 0.674046\nCost after iteration 3900: 0.674046\nCost after iteration 4000: 0.674046\nCost after iteration 4100: 0.674046\nCost after iteration 4200: 0.674046\nCost after iteration 4300: 0.674046\nCost after iteration 4400: 0.674046\nCost after iteration 4500: 0.674045\nCost after iteration 4600: 0.674045\nCost after iteration 4700: 0.674045\nCost after iteration 4800: 0.674045\nCost after iteration 4900: 0.674045\nCost after iteration 5000: 0.674045\nCost after iteration 5100: 0.674045\nCost after iteration 5200: 0.674045\nCost after iteration 5300: 0.674045\nCost after iteration 5400: 0.674045\nCost after iteration 5500: 0.674044\nCost after iteration 5600: 0.674044\nCost after iteration 5700: 0.674044\nCost after iteration 5800: 0.674044\nCost after iteration 5900: 0.674044\nCost after iteration 6000: 0.674044\nCost after iteration 6100: 0.674044\nCost after iteration 6200: 0.674044\nCost after iteration 6300: 0.674044\nCost after iteration 6400: 0.674044\nCost after iteration 6500: 0.674044\nCost after iteration 6600: 0.674043\nCost after iteration 6700: 0.674043\nCost after iteration 6800: 0.674043\nCost after iteration 6900: 0.674043\nCost after iteration 7000: 0.674043\nCost after iteration 7100: 0.674043\nCost after iteration 7200: 0.674043\nCost after iteration 7300: 0.674043\nCost after iteration 7400: 0.674043\nCost after iteration 7500: 0.674043\nCost after iteration 7600: 0.674043\nCost after iteration 7700: 0.674042\nCost after iteration 7800: 0.674042\nCost after iteration 7900: 0.674042\nCost after iteration 8000: 0.674042\nCost after iteration 8100: 0.674042\nCost after iteration 8200: 0.674042\nCost after iteration 8300: 0.674042\nCost after iteration 8400: 0.674042\nCost after iteration 8500: 0.674042\nCost after iteration 8600: 0.674042\nCost after iteration 8700: 0.674042\nCost after iteration 8800: 0.674042\nCost after iteration 8900: 0.674041\nCost after iteration 9000: 0.674041\nCost after iteration 9100: 0.674041\nCost after iteration 9200: 0.674041\nCost after iteration 9300: 0.674041\nCost after iteration 9400: 0.674041\nCost after iteration 9500: 0.674041\nCost after iteration 9600: 0.674041\nCost after iteration 9700: 0.674041\nCost after iteration 9800: 0.674041\nCost after iteration 9900: 0.674041\nCost after iteration 10000: 0.674040\nCost after iteration 10100: 0.674040\nCost after iteration 10200: 0.674040\nCost after iteration 10300: 0.674040\nCost after iteration 10400: 0.674040\nCost after iteration 10500: 0.674040\nCost after iteration 10600: 0.674040\nCost after iteration 10700: 0.674040\nCost after iteration 10800: 0.674040\nCost after iteration 10900: 0.674040\nCost after iteration 11000: 0.674040\nCost after iteration 11100: 0.674040\nCost after iteration 11200: 0.674039\nCost after iteration 11300: 0.674039\nCost after iteration 11400: 0.674039\nCost after iteration 11500: 0.674039\nCost after iteration 11600: 0.674039\nCost after iteration 11700: 0.674039\nCost after iteration 11800: 0.674039\nCost after iteration 11900: 0.674039\nCost after iteration 12000: 0.674039\nCost after iteration 12100: 0.674039\nCost after iteration 12200: 0.674039\nCost after iteration 12300: 0.674039\nCost after iteration 12400: 0.674038\nCost after iteration 12500: 0.674038\nCost after iteration 12600: 0.674038\nCost after iteration 12700: 0.674038\nCost after iteration 12800: 0.674038\nCost after iteration 12900: 0.674038\nCost after iteration 13000: 0.674038\nCost after iteration 13100: 0.674038\nCost after iteration 13200: 0.674038\nCost after iteration 13300: 0.674038\nCost after iteration 13400: 0.674038\nCost after iteration 13500: 0.674038\nCost after iteration 13600: 0.674037\nCost after iteration 13700: 0.674037\nCost after iteration 13800: 0.674037\nCost after iteration 13900: 0.674037\nCost after iteration 14000: 0.674037\nCost after iteration 14100: 0.674037\nCost after iteration 14200: 0.674037\nCost after iteration 14300: 0.674037\nCost after iteration 14400: 0.674037\nCost after iteration 14500: 0.674037\nCost after iteration 14600: 0.674037\nCost after iteration 14700: 0.674037\nCost after iteration 14800: 0.674037\nCost after iteration 14900: 0.674036\nè®­ç»ƒé›†å‡†ç¡®ç‡: 58.24634655532359 \næµ‹è¯•é›†å‡†ç¡®ç‡: 47.77777777777777 \n"
      }
     ],
     "persistentId": "b175609a-a7d0-4bf8-84b6-23b93d651481",
     "text": "if __name__ == '__main__':\n    model(X_train, y_train, X_test, y_test, num_iterations=15000, learning_rate=0.003)\n"
    },
    "executionTime": "2020-02-04T05:53:22.429Z"
   },
   {
    "cell": {
     "executionCount": 97,
     "executionEventId": "e3abd3e0-55c6-4189-873d-1b6e0ef39450",
     "hasError": false,
     "id": "bfa4db5f-1e7d-4f75-9d51-75b75a88b95e",
     "outputs": [],
     "persistentId": "a7917915-adf9-4667-88b6-f01c4b8e2de3",
     "text": "import matplotlib.pyplot as plt"
    },
    "executionTime": "2020-02-04T05:56:14.659Z"
   },
   {
    "cell": {
     "executionCount": 98,
     "executionEventId": "51d2e203-d903-46fe-91c0-0ff4b98b0395",
     "hasError": false,
     "id": "611bea7f-db28-443e-b57e-464cc0492891",
     "outputs": [],
     "persistentId": "736561b1-27f2-4958-a474-6d34622f80bb",
     "text": "def model(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate,print_cost=True):\n    \"\"\"\n    Build the logistic regression model by calling all the functions you have implemented.\n    Arguments:\n    X_train - training set\n    Y_train - training label\n    X_test - test set\n    Y_test - test label\n    num_iteration - hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations è¿™é‡Œæˆ‘æƒ³æ”¹æˆ500ï¼Œprintå¤ªå¤šäº†\n    \n    Returns:\n    d - dictionary should contain following information w,b,training_accuracy, test_accuracy,cost\n    eg: d = {\"w\":w,\n             \"b\":b,\n             \"training_accuracy\": traing_accuracy,\n             \"test_accuracy\":test_accuracy,\n             \"cost\":cost}\n    \"\"\"\n    # æ•°æ®ä¿®æ”¹ä¸ºä¸€ç»´\n    x_train = X_train.reshape(-1,X_train.shape[0])\n    y_train = Y_train.reshape(-1,Y_train.shape[0])\n    x_test = X_test.reshape(-1,X_test.shape[0])\n    y_test = Y_test.reshape(-1,Y_test.shape[0])\n    \n    # åˆå§‹åŒ–å‚æ•°\n    w,b = initialize_parameters(x_train.shape[0])\n    \n    # params:æ›´æ–°åçš„ç½‘ç»œå‚æ•°\n    # grads:æœ€æ–°çš„æ¢¯åº¦(ä¸‹é™æŸå¤±)\n    # costs:æ¯æ¬¡æ›´æ–°çš„æŸå¤±åˆ—è¡¨\n    params, grads, costs = optimize(w,b,x_train,y_train,num_iterations,learning_rate,print_cost)\n    \n    # è·å–è®­ç»ƒçš„å‚æ•°\n    w = params[\"w\"]\n    b = params[\"b\"]\n    \n    # é¢„æµ‹ç»“æœ\n    y_prediction_train = predict(w,b,x_train)\n    y_prediction_test = predict(w,b,x_test)\n\n    \n    # æ‰“å°å‡†ç¡®ç‡\n#     print(\"training setâ€˜s accuracyï¼š{}\".format(np.sum(y_prediction_train == y_train)/ y_train.shape[1]))\n#     print(\"testing setâ€™s accuracyï¼š{}\".format(np.sum(y_prediction_test == y_test)/ y_test.shape[1]))\n    \n    # æ‰“å°å‡†ç¡®ç‡\n    plt.plot(range(len(costs),costs))\n    print(\"è®­ç»ƒé›†å‡†ç¡®ç‡: {} \".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"æµ‹è¯•é›†å‡†ç¡®ç‡: {} \".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \n    \n    \n"
    },
    "executionTime": "2020-02-04T05:57:12.113Z"
   },
   {
    "cell": {
     "executionCount": 99,
     "executionEventId": "fe16844c-469a-4f62-96e9-c725ce5b51e8",
     "hasError": true,
     "id": "1fde88e1-df7e-49c2-ae1f-e396411d30e3",
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "Cost after iteration 0: 5.180974\nCost after iteration 100: 4.830107\nCost after iteration 200: 4.775526\nCost after iteration 300: 4.687215\nCost after iteration 400: 4.583378\nCost after iteration 500: 4.473145\nCost after iteration 600: 4.361883\nCost after iteration 700: 4.235288\nCost after iteration 800: 4.072745\nCost after iteration 900: 3.876058\nCost after iteration 1000: 3.642342\nCost after iteration 1100: 3.352072\nCost after iteration 1200: 2.997904\nCost after iteration 1300: 2.572210\nCost after iteration 1400: 2.077077\nCost after iteration 1500: 1.555020\nCost after iteration 1600: 1.099004\nCost after iteration 1700: 0.808851\nCost after iteration 1800: 0.700249\nCost after iteration 1900: 0.678004\nCost after iteration 2000: 0.674625\nCost after iteration 2100: 0.674140\nCost after iteration 2200: 0.674070\nCost after iteration 2300: 0.674059\nCost after iteration 2400: 0.674057\nCost after iteration 2500: 0.674057\nCost after iteration 2600: 0.674057\nCost after iteration 2700: 0.674057\nCost after iteration 2800: 0.674057\nCost after iteration 2900: 0.674057\nCost after iteration 3000: 0.674057\nCost after iteration 3100: 0.674056\nCost after iteration 3200: 0.674056\nCost after iteration 3300: 0.674056\nCost after iteration 3400: 0.674056\nCost after iteration 3500: 0.674056\nCost after iteration 3600: 0.674056\nCost after iteration 3700: 0.674056\nCost after iteration 3800: 0.674056\nCost after iteration 3900: 0.674056\nCost after iteration 4000: 0.674055\nCost after iteration 4100: 0.674055\nCost after iteration 4200: 0.674055\nCost after iteration 4300: 0.674055\nCost after iteration 4400: 0.674055\nCost after iteration 4500: 0.674055\nCost after iteration 4600: 0.674055\nCost after iteration 4700: 0.674055\nCost after iteration 4800: 0.674055\nCost after iteration 4900: 0.674054\nCost after iteration 5000: 0.674054\nCost after iteration 5100: 0.674054\nCost after iteration 5200: 0.674054\nCost after iteration 5300: 0.674054\nCost after iteration 5400: 0.674054\nCost after iteration 5500: 0.674054\nCost after iteration 5600: 0.674054\nCost after iteration 5700: 0.674054\nCost after iteration 5800: 0.674053\nCost after iteration 5900: 0.674053\nCost after iteration 6000: 0.674053\nCost after iteration 6100: 0.674053\nCost after iteration 6200: 0.674053\nCost after iteration 6300: 0.674053\nCost after iteration 6400: 0.674053\nCost after iteration 6500: 0.674053\nCost after iteration 6600: 0.674053\nCost after iteration 6700: 0.674052\nCost after iteration 6800: 0.674052\nCost after iteration 6900: 0.674052\nCost after iteration 7000: 0.674052\nCost after iteration 7100: 0.674052\nCost after iteration 7200: 0.674052\nCost after iteration 7300: 0.674052\nCost after iteration 7400: 0.674052\nCost after iteration 7500: 0.674052\nCost after iteration 7600: 0.674052\nCost after iteration 7700: 0.674051\nCost after iteration 7800: 0.674051\nCost after iteration 7900: 0.674051\nCost after iteration 8000: 0.674051\nCost after iteration 8100: 0.674051\nCost after iteration 8200: 0.674051\nCost after iteration 8300: 0.674051\nCost after iteration 8400: 0.674051\nCost after iteration 8500: 0.674051\nCost after iteration 8600: 0.674050\nCost after iteration 8700: 0.674050\nCost after iteration 8800: 0.674050\nCost after iteration 8900: 0.674050\nCost after iteration 9000: 0.674050\nCost after iteration 9100: 0.674050\nCost after iteration 9200: 0.674050\nCost after iteration 9300: 0.674050\nCost after iteration 9400: 0.674050\nCost after iteration 9500: 0.674050\nCost after iteration 9600: 0.674049\nCost after iteration 9700: 0.674049\nCost after iteration 9800: 0.674049\nCost after iteration 9900: 0.674049\nCost after iteration 10000: 0.674049\nCost after iteration 10100: 0.674049\nCost after iteration 10200: 0.674049\nCost after iteration 10300: 0.674049\nCost after iteration 10400: 0.674049\nCost after iteration 10500: 0.674049\nCost after iteration 10600: 0.674048\nCost after iteration 10700: 0.674048\nCost after iteration 10800: 0.674048\nCost after iteration 10900: 0.674048\nCost after iteration 11000: 0.674048\nCost after iteration 11100: 0.674048\nCost after iteration 11200: 0.674048\nCost after iteration 11300: 0.674048\nCost after iteration 11400: 0.674048\nCost after iteration 11500: 0.674048\nCost after iteration 11600: 0.674047\nCost after iteration 11700: 0.674047\nCost after iteration 11800: 0.674047\nCost after iteration 11900: 0.674047\nCost after iteration 12000: 0.674047\nCost after iteration 12100: 0.674047\nCost after iteration 12200: 0.674047\nCost after iteration 12300: 0.674047\nCost after iteration 12400: 0.674047\nCost after iteration 12500: 0.674047\nCost after iteration 12600: 0.674046\nCost after iteration 12700: 0.674046\nCost after iteration 12800: 0.674046\nCost after iteration 12900: 0.674046\nCost after iteration 13000: 0.674046\nCost after iteration 13100: 0.674046\nCost after iteration 13200: 0.674046\nCost after iteration 13300: 0.674046\nCost after iteration 13400: 0.674046\nCost after iteration 13500: 0.674046\nCost after iteration 13600: 0.674046\nCost after iteration 13700: 0.674045\nCost after iteration 13800: 0.674045\nCost after iteration 13900: 0.674045\nCost after iteration 14000: 0.674045\nCost after iteration 14100: 0.674045\nCost after iteration 14200: 0.674045\nCost after iteration 14300: 0.674045\nCost after iteration 14400: 0.674045\nCost after iteration 14500: 0.674045\nCost after iteration 14600: 0.674045\nCost after iteration 14700: 0.674045\nCost after iteration 14800: 0.674044\nCost after iteration 14900: 0.674044\n"
      },
      {
       "ename": "TypeError",
       "evalue": "'list' object cannot be interpreted as an integer",
       "output_type": "error",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
        "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-99-e9bbc55e860c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.003\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-98-036a1a74843d>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate, print_cost)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# æ‰“å°å‡†ç¡®ç‡\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcosts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcosts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"è®­ç»ƒé›†å‡†ç¡®ç‡: {} \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_prediction_train\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"æµ‹è¯•é›†å‡†ç¡®ç‡: {} \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_prediction_test\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mTypeError\u001b[0m: 'list' object cannot be interpreted as an integer"
       ]
      }
     ],
     "persistentId": "b175609a-a7d0-4bf8-84b6-23b93d651481",
     "text": "if __name__ == '__main__':\n    model(X_train, y_train, X_test, y_test, num_iterations=15000, learning_rate=0.003)\n"
    },
    "executionTime": "2020-02-04T05:57:14.814Z"
   },
   {
    "cell": {
     "executionCount": 100,
     "executionEventId": "cc112456-2c98-4fb4-9ae3-9225b58bf94d",
     "hasError": false,
     "id": "611bea7f-db28-443e-b57e-464cc0492891",
     "outputs": [],
     "persistentId": "736561b1-27f2-4958-a474-6d34622f80bb",
     "text": "def model(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate,print_cost=True):\n    \"\"\"\n    Build the logistic regression model by calling all the functions you have implemented.\n    Arguments:\n    X_train - training set\n    Y_train - training label\n    X_test - test set\n    Y_test - test label\n    num_iteration - hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations è¿™é‡Œæˆ‘æƒ³æ”¹æˆ500ï¼Œprintå¤ªå¤šäº†\n    \n    Returns:\n    d - dictionary should contain following information w,b,training_accuracy, test_accuracy,cost\n    eg: d = {\"w\":w,\n             \"b\":b,\n             \"training_accuracy\": traing_accuracy,\n             \"test_accuracy\":test_accuracy,\n             \"cost\":cost}\n    \"\"\"\n    # æ•°æ®ä¿®æ”¹ä¸ºä¸€ç»´\n    x_train = X_train.reshape(-1,X_train.shape[0])\n    y_train = Y_train.reshape(-1,Y_train.shape[0])\n    x_test = X_test.reshape(-1,X_test.shape[0])\n    y_test = Y_test.reshape(-1,Y_test.shape[0])\n    \n    # åˆå§‹åŒ–å‚æ•°\n    w,b = initialize_parameters(x_train.shape[0])\n    \n    # params:æ›´æ–°åçš„ç½‘ç»œå‚æ•°\n    # grads:æœ€æ–°çš„æ¢¯åº¦(ä¸‹é™æŸå¤±)\n    # costs:æ¯æ¬¡æ›´æ–°çš„æŸå¤±åˆ—è¡¨\n    params, grads, costs = optimize(w,b,x_train,y_train,num_iterations,learning_rate,print_cost)\n    \n    # è·å–è®­ç»ƒçš„å‚æ•°\n    w = params[\"w\"]\n    b = params[\"b\"]\n    \n    # é¢„æµ‹ç»“æœ\n    y_prediction_train = predict(w,b,x_train)\n    y_prediction_test = predict(w,b,x_test)\n\n    \n    # æ‰“å°å‡†ç¡®ç‡\n#     print(\"training setâ€˜s accuracyï¼š{}\".format(np.sum(y_prediction_train == y_train)/ y_train.shape[1]))\n#     print(\"testing setâ€™s accuracyï¼š{}\".format(np.sum(y_prediction_test == y_test)/ y_test.shape[1]))\n    \n    # æ‰“å°å‡†ç¡®ç‡\n    plt.plot(range(len(costs),costs))\n    print(\"è®­ç»ƒé›†å‡†ç¡®ç‡: {} \".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"æµ‹è¯•é›†å‡†ç¡®ç‡: {} \".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \n    return costs\n    \n    \n    \n"
    },
    "executionTime": "2020-02-04T05:59:09.480Z"
   },
   {
    "cell": {
     "executionCount": 101,
     "executionEventId": "8a3d34b8-6ee9-4ea6-a0a4-bb9dd09bd8f4",
     "hasError": true,
     "id": "1fde88e1-df7e-49c2-ae1f-e396411d30e3",
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "Cost after iteration 0: 5.743900\nCost after iteration 100: 5.191281\nCost after iteration 200: 5.028015\nCost after iteration 300: 4.910195\nCost after iteration 400: 4.788221\nCost after iteration 500: 4.639000\nCost after iteration 600: 4.452892\nCost after iteration 700: 4.231854\nCost after iteration 800: 3.979461\nCost after iteration 900: 3.691189\nCost after iteration 1000: 3.361392\nCost after iteration 1100: 2.979809\nCost after iteration 1200: 2.541529\nCost after iteration 1300: 2.046707\nCost after iteration 1400: 1.536397\nCost after iteration 1500: 1.096991\nCost after iteration 1600: 0.815642\nCost after iteration 1700: 0.704489\nCost after iteration 1800: 0.679220\nCost after iteration 1900: 0.674875\nCost after iteration 2000: 0.674166\nCost after iteration 2100: 0.674049\nCost after iteration 2200: 0.674030\nCost after iteration 2300: 0.674026\nCost after iteration 2400: 0.674026\nCost after iteration 2500: 0.674025\nCost after iteration 2600: 0.674025\nCost after iteration 2700: 0.674025\nCost after iteration 2800: 0.674025\nCost after iteration 2900: 0.674025\nCost after iteration 3000: 0.674025\nCost after iteration 3100: 0.674025\nCost after iteration 3200: 0.674025\nCost after iteration 3300: 0.674025\nCost after iteration 3400: 0.674025\nCost after iteration 3500: 0.674025\nCost after iteration 3600: 0.674025\nCost after iteration 3700: 0.674025\nCost after iteration 3800: 0.674025\nCost after iteration 3900: 0.674025\nCost after iteration 4000: 0.674024\nCost after iteration 4100: 0.674024\nCost after iteration 4200: 0.674024\nCost after iteration 4300: 0.674024\nCost after iteration 4400: 0.674024\nCost after iteration 4500: 0.674024\nCost after iteration 4600: 0.674024\nCost after iteration 4700: 0.674024\nCost after iteration 4800: 0.674024\nCost after iteration 4900: 0.674024\nCost after iteration 5000: 0.674024\nCost after iteration 5100: 0.674024\nCost after iteration 5200: 0.674024\nCost after iteration 5300: 0.674024\nCost after iteration 5400: 0.674024\nCost after iteration 5500: 0.674024\nCost after iteration 5600: 0.674024\nCost after iteration 5700: 0.674024\nCost after iteration 5800: 0.674023\nCost after iteration 5900: 0.674023\nCost after iteration 6000: 0.674023\nCost after iteration 6100: 0.674023\nCost after iteration 6200: 0.674023\nCost after iteration 6300: 0.674023\nCost after iteration 6400: 0.674023\nCost after iteration 6500: 0.674023\nCost after iteration 6600: 0.674023\nCost after iteration 6700: 0.674023\nCost after iteration 6800: 0.674023\nCost after iteration 6900: 0.674023\nCost after iteration 7000: 0.674023\nCost after iteration 7100: 0.674023\nCost after iteration 7200: 0.674023\nCost after iteration 7300: 0.674023\nCost after iteration 7400: 0.674023\nCost after iteration 7500: 0.674023\nCost after iteration 7600: 0.674022\nCost after iteration 7700: 0.674022\nCost after iteration 7800: 0.674022\nCost after iteration 7900: 0.674022\nCost after iteration 8000: 0.674022\nCost after iteration 8100: 0.674022\nCost after iteration 8200: 0.674022\nCost after iteration 8300: 0.674022\nCost after iteration 8400: 0.674022\nCost after iteration 8500: 0.674022\nCost after iteration 8600: 0.674022\nCost after iteration 8700: 0.674022\nCost after iteration 8800: 0.674022\nCost after iteration 8900: 0.674022\nCost after iteration 9000: 0.674022\nCost after iteration 9100: 0.674022\nCost after iteration 9200: 0.674022\nCost after iteration 9300: 0.674022\nCost after iteration 9400: 0.674022\nCost after iteration 9500: 0.674021\nCost after iteration 9600: 0.674021\nCost after iteration 9700: 0.674021\nCost after iteration 9800: 0.674021\nCost after iteration 9900: 0.674021\nCost after iteration 10000: 0.674021\nCost after iteration 10100: 0.674021\nCost after iteration 10200: 0.674021\nCost after iteration 10300: 0.674021\nCost after iteration 10400: 0.674021\nCost after iteration 10500: 0.674021\nCost after iteration 10600: 0.674021\nCost after iteration 10700: 0.674021\nCost after iteration 10800: 0.674021\nCost after iteration 10900: 0.674021\nCost after iteration 11000: 0.674021\nCost after iteration 11100: 0.674021\nCost after iteration 11200: 0.674021\nCost after iteration 11300: 0.674021\nCost after iteration 11400: 0.674020\nCost after iteration 11500: 0.674020\nCost after iteration 11600: 0.674020\nCost after iteration 11700: 0.674020\nCost after iteration 11800: 0.674020\nCost after iteration 11900: 0.674020\nCost after iteration 12000: 0.674020\nCost after iteration 12100: 0.674020\nCost after iteration 12200: 0.674020\nCost after iteration 12300: 0.674020\nCost after iteration 12400: 0.674020\nCost after iteration 12500: 0.674020\nCost after iteration 12600: 0.674020\nCost after iteration 12700: 0.674020\nCost after iteration 12800: 0.674020\nCost after iteration 12900: 0.674020\nCost after iteration 13000: 0.674020\nCost after iteration 13100: 0.674020\nCost after iteration 13200: 0.674020\nCost after iteration 13300: 0.674020\nCost after iteration 13400: 0.674019\nCost after iteration 13500: 0.674019\nCost after iteration 13600: 0.674019\nCost after iteration 13700: 0.674019\nCost after iteration 13800: 0.674019\nCost after iteration 13900: 0.674019\nCost after iteration 14000: 0.674019\nCost after iteration 14100: 0.674019\nCost after iteration 14200: 0.674019\nCost after iteration 14300: 0.674019\nCost after iteration 14400: 0.674019\nCost after iteration 14500: 0.674019\nCost after iteration 14600: 0.674019\nCost after iteration 14700: 0.674019\nCost after iteration 14800: 0.674019\nCost after iteration 14900: 0.674019\n"
      },
      {
       "ename": "TypeError",
       "evalue": "'list' object cannot be interpreted as an integer",
       "output_type": "error",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
        "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-101-1adc4ea95d52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcosts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.003\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;32m<ipython-input-100-49a22f60a201>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate, print_cost)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# æ‰“å°å‡†ç¡®ç‡\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcosts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcosts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"è®­ç»ƒé›†å‡†ç¡®ç‡: {} \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_prediction_train\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"æµ‹è¯•é›†å‡†ç¡®ç‡: {} \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_prediction_test\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mTypeError\u001b[0m: 'list' object cannot be interpreted as an integer"
       ]
      }
     ],
     "persistentId": "b175609a-a7d0-4bf8-84b6-23b93d651481",
     "text": "costs = model(X_train, y_train, X_test, y_test, num_iterations=15000, learning_rate=0.003)\n"
    },
    "executionTime": "2020-02-04T05:59:32.811Z"
   },
   {
    "cell": {
     "executionCount": 102,
     "executionEventId": "f450a31b-cbb7-487d-ac9e-5bc0206a8575",
     "hasError": false,
     "id": "611bea7f-db28-443e-b57e-464cc0492891",
     "outputs": [],
     "persistentId": "736561b1-27f2-4958-a474-6d34622f80bb",
     "text": "def model(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate,print_cost=True):\n    \"\"\"\n    Build the logistic regression model by calling all the functions you have implemented.\n    Arguments:\n    X_train - training set\n    Y_train - training label\n    X_test - test set\n    Y_test - test label\n    num_iteration - hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations è¿™é‡Œæˆ‘æƒ³æ”¹æˆ500ï¼Œprintå¤ªå¤šäº†\n    \n    Returns:\n    d - dictionary should contain following information w,b,training_accuracy, test_accuracy,cost\n    eg: d = {\"w\":w,\n             \"b\":b,\n             \"training_accuracy\": traing_accuracy,\n             \"test_accuracy\":test_accuracy,\n             \"cost\":cost}\n    \"\"\"\n    # æ•°æ®ä¿®æ”¹ä¸ºä¸€ç»´\n    x_train = X_train.reshape(-1,X_train.shape[0])\n    y_train = Y_train.reshape(-1,Y_train.shape[0])\n    x_test = X_test.reshape(-1,X_test.shape[0])\n    y_test = Y_test.reshape(-1,Y_test.shape[0])\n    \n    # åˆå§‹åŒ–å‚æ•°\n    w,b = initialize_parameters(x_train.shape[0])\n    \n    # params:æ›´æ–°åçš„ç½‘ç»œå‚æ•°\n    # grads:æœ€æ–°çš„æ¢¯åº¦(ä¸‹é™æŸå¤±)\n    # costs:æ¯æ¬¡æ›´æ–°çš„æŸå¤±åˆ—è¡¨\n    params, grads, costs = optimize(w,b,x_train,y_train,num_iterations,learning_rate,print_cost)\n    \n    # è·å–è®­ç»ƒçš„å‚æ•°\n    w = params[\"w\"]\n    b = params[\"b\"]\n    \n    # é¢„æµ‹ç»“æœ\n    y_prediction_train = predict(w,b,x_train)\n    y_prediction_test = predict(w,b,x_test)\n\n    \n    # æ‰“å°å‡†ç¡®ç‡\n#     print(\"training setâ€˜s accuracyï¼š{}\".format(np.sum(y_prediction_train == y_train)/ y_train.shape[1]))\n#     print(\"testing setâ€™s accuracyï¼š{}\".format(np.sum(y_prediction_test == y_test)/ y_test.shape[1]))\n    \n    # æ‰“å°å‡†ç¡®ç‡\n#     plt.plot(range(len(costs),costs))\n    print(\"è®­ç»ƒé›†å‡†ç¡®ç‡: {} \".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"æµ‹è¯•é›†å‡†ç¡®ç‡: {} \".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \n    return costs\n    \n    \n    \n"
    },
    "executionTime": "2020-02-04T05:59:47.334Z"
   },
   {
    "cell": {
     "executionCount": 103,
     "executionEventId": "d77671df-57d1-4b0a-9933-7716b3b7f9a5",
     "hasError": true,
     "id": "1fde88e1-df7e-49c2-ae1f-e396411d30e3",
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "Cost after iteration 0: 5.150155\nCost after iteration 100: 5.142528\nCost after iteration 200: 5.084867\nCost after iteration 300: 4.995074\nCost after iteration 400: 4.889872\nCost after iteration 500: 4.761925\nCost after iteration 600: 4.609725\nCost after iteration 700: 4.432062\nCost after iteration 800: 4.225311\nCost after iteration 900: 3.979901\nCost after iteration 1000: 3.684891\nCost after iteration 1100: 3.333249\nCost after iteration 1200: 2.929466\nCost after iteration 1300: 2.470343\nCost after iteration 1400: 1.971495\nCost after iteration 1500: 1.492476\nCost after iteration 1600: 1.094879\nCost after iteration 1700: 0.829655\nCost after iteration 1800: 0.711724\nCost after iteration 1900: 0.680790\nCost after iteration 2000: 0.675132\nCost after iteration 2100: 0.674222\nCost after iteration 2200: 0.674079\nCost after iteration 2300: 0.674057\nCost after iteration 2400: 0.674054\nCost after iteration 2500: 0.674053\nCost after iteration 2600: 0.674053\nCost after iteration 2700: 0.674053\nCost after iteration 2800: 0.674052\nCost after iteration 2900: 0.674052\nCost after iteration 3000: 0.674052\nCost after iteration 3100: 0.674052\nCost after iteration 3200: 0.674052\nCost after iteration 3300: 0.674052\nCost after iteration 3400: 0.674052\nCost after iteration 3500: 0.674052\nCost after iteration 3600: 0.674052\nCost after iteration 3700: 0.674051\nCost after iteration 3800: 0.674051\nCost after iteration 3900: 0.674051\nCost after iteration 4000: 0.674051\nCost after iteration 4100: 0.674051\nCost after iteration 4200: 0.674051\nCost after iteration 4300: 0.674051\nCost after iteration 4400: 0.674051\nCost after iteration 4500: 0.674051\nCost after iteration 4600: 0.674051\nCost after iteration 4700: 0.674050\nCost after iteration 4800: 0.674050\nCost after iteration 4900: 0.674050\nCost after iteration 5000: 0.674050\nCost after iteration 5100: 0.674050\nCost after iteration 5200: 0.674050\nCost after iteration 5300: 0.674050\nCost after iteration 5400: 0.674050\nCost after iteration 5500: 0.674050\nCost after iteration 5600: 0.674050\nCost after iteration 5700: 0.674049\nCost after iteration 5800: 0.674049\nCost after iteration 5900: 0.674049\nCost after iteration 6000: 0.674049\nCost after iteration 6100: 0.674049\nCost after iteration 6200: 0.674049\nCost after iteration 6300: 0.674049\nCost after iteration 6400: 0.674049\nCost after iteration 6500: 0.674049\nCost after iteration 6600: 0.674049\nCost after iteration 6700: 0.674048\nCost after iteration 6800: 0.674048\nCost after iteration 6900: 0.674048\nCost after iteration 7000: 0.674048\nCost after iteration 7100: 0.674048\nCost after iteration 7200: 0.674048\nCost after iteration 7300: 0.674048\nCost after iteration 7400: 0.674048\nCost after iteration 7500: 0.674048\nCost after iteration 7600: 0.674048\nCost after iteration 7700: 0.674047\nCost after iteration 7800: 0.674047\nCost after iteration 7900: 0.674047\nCost after iteration 8000: 0.674047\nCost after iteration 8100: 0.674047\nCost after iteration 8200: 0.674047\nCost after iteration 8300: 0.674047\nCost after iteration 8400: 0.674047\nCost after iteration 8500: 0.674047\nCost after iteration 8600: 0.674047\nCost after iteration 8700: 0.674046\nCost after iteration 8800: 0.674046\nCost after iteration 8900: 0.674046\nCost after iteration 9000: 0.674046\nCost after iteration 9100: 0.674046\nCost after iteration 9200: 0.674046\nCost after iteration 9300: 0.674046\nCost after iteration 9400: 0.674046\nCost after iteration 9500: 0.674046\nCost after iteration 9600: 0.674046\nCost after iteration 9700: 0.674046\nCost after iteration 9800: 0.674045\nCost after iteration 9900: 0.674045\nCost after iteration 10000: 0.674045\nCost after iteration 10100: 0.674045\nCost after iteration 10200: 0.674045\nCost after iteration 10300: 0.674045\nCost after iteration 10400: 0.674045\nCost after iteration 10500: 0.674045\nCost after iteration 10600: 0.674045\nCost after iteration 10700: 0.674045\nCost after iteration 10800: 0.674044\nCost after iteration 10900: 0.674044\nCost after iteration 11000: 0.674044\nCost after iteration 11100: 0.674044\nCost after iteration 11200: 0.674044\nCost after iteration 11300: 0.674044\nCost after iteration 11400: 0.674044\nCost after iteration 11500: 0.674044\nCost after iteration 11600: 0.674044\nCost after iteration 11700: 0.674044\nCost after iteration 11800: 0.674044\nCost after iteration 11900: 0.674043\nCost after iteration 12000: 0.674043\nCost after iteration 12100: 0.674043\nCost after iteration 12200: 0.674043\nCost after iteration 12300: 0.674043\nCost after iteration 12400: 0.674043\nCost after iteration 12500: 0.674043\nCost after iteration 12600: 0.674043\nCost after iteration 12700: 0.674043\nCost after iteration 12800: 0.674043\nCost after iteration 12900: 0.674043\nCost after iteration 13000: 0.674042\nCost after iteration 13100: 0.674042\nCost after iteration 13200: 0.674042\nCost after iteration 13300: 0.674042\nCost after iteration 13400: 0.674042\nCost after iteration 13500: 0.674042\nCost after iteration 13600: 0.674042\nCost after iteration 13700: 0.674042\nCost after iteration 13800: 0.674042\nCost after iteration 13900: 0.674042\nCost after iteration 14000: 0.674042\nCost after iteration 14100: 0.674042\nCost after iteration 14200: 0.674041\nCost after iteration 14300: 0.674041\nCost after iteration 14400: 0.674041\nCost after iteration 14500: 0.674041\nCost after iteration 14600: 0.674041\nCost after iteration 14700: 0.674041\nCost after iteration 14800: 0.674041\nCost after iteration 14900: 0.674041\nè®­ç»ƒé›†å‡†ç¡®ç‡: 58.176757132915796 \næµ‹è¯•é›†å‡†ç¡®ç‡: 47.22222222222222 \n"
      },
      {
       "ename": "NameError",
       "evalue": "name 'c' is not defined",
       "output_type": "error",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
        "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-103-bcd0049d0241>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcosts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.003\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mNameError\u001b[0m: name 'c' is not defined"
       ]
      }
     ],
     "persistentId": "b175609a-a7d0-4bf8-84b6-23b93d651481",
     "text": "costs = model(X_train, y_train, X_test, y_test, num_iterations=15000, learning_rate=0.003)\nc"
    },
    "executionTime": "2020-02-04T05:59:50.105Z"
   },
   {
    "cell": {
     "executionCount": 104,
     "executionEventId": "a2469151-ae11-4e13-ade2-cffd9b19dd9e",
     "hasError": false,
     "id": "1fde88e1-df7e-49c2-ae1f-e396411d30e3",
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "Cost after iteration 0: 5.437195\nCost after iteration 100: 4.721734\nCost after iteration 200: 4.567366\nCost after iteration 300: 4.467848\nCost after iteration 400: 4.376524\nCost after iteration 500: 4.283015\nCost after iteration 600: 4.180430\nCost after iteration 700: 4.061002\nCost after iteration 800: 3.917502\nCost after iteration 900: 3.745256\nCost after iteration 1000: 3.541793\nCost after iteration 1100: 3.300936\nCost after iteration 1200: 3.011894\nCost after iteration 1300: 2.668739\nCost after iteration 1400: 2.267897\nCost after iteration 1500: 1.827689\nCost after iteration 1600: 1.383669\nCost after iteration 1700: 1.011146\nCost after iteration 1800: 0.781275\nCost after iteration 1900: 0.695496\nCost after iteration 2000: 0.677356\nCost after iteration 2100: 0.674527\nCost after iteration 2200: 0.674112\nCost after iteration 2300: 0.674051\nCost after iteration 2400: 0.674041\nCost after iteration 2500: 0.674040\nCost after iteration 2600: 0.674039\nCost after iteration 2700: 0.674039\nCost after iteration 2800: 0.674039\nCost after iteration 2900: 0.674039\nCost after iteration 3000: 0.674039\nCost after iteration 3100: 0.674039\nCost after iteration 3200: 0.674039\nCost after iteration 3300: 0.674039\nCost after iteration 3400: 0.674039\nCost after iteration 3500: 0.674039\nCost after iteration 3600: 0.674038\nCost after iteration 3700: 0.674038\nCost after iteration 3800: 0.674038\nCost after iteration 3900: 0.674038\nCost after iteration 4000: 0.674038\nCost after iteration 4100: 0.674038\nCost after iteration 4200: 0.674038\nCost after iteration 4300: 0.674038\nCost after iteration 4400: 0.674038\nCost after iteration 4500: 0.674038\nCost after iteration 4600: 0.674038\nCost after iteration 4700: 0.674038\nCost after iteration 4800: 0.674037\nCost after iteration 4900: 0.674037\nCost after iteration 5000: 0.674037\nCost after iteration 5100: 0.674037\nCost after iteration 5200: 0.674037\nCost after iteration 5300: 0.674037\nCost after iteration 5400: 0.674037\nCost after iteration 5500: 0.674037\nCost after iteration 5600: 0.674037\nCost after iteration 5700: 0.674037\nCost after iteration 5800: 0.674037\nCost after iteration 5900: 0.674037\nCost after iteration 6000: 0.674037\nCost after iteration 6100: 0.674036\nCost after iteration 6200: 0.674036\nCost after iteration 6300: 0.674036\nCost after iteration 6400: 0.674036\nCost after iteration 6500: 0.674036\nCost after iteration 6600: 0.674036\nCost after iteration 6700: 0.674036\nCost after iteration 6800: 0.674036\nCost after iteration 6900: 0.674036\nCost after iteration 7000: 0.674036\nCost after iteration 7100: 0.674036\nCost after iteration 7200: 0.674036\nCost after iteration 7300: 0.674035\nCost after iteration 7400: 0.674035\nCost after iteration 7500: 0.674035\nCost after iteration 7600: 0.674035\nCost after iteration 7700: 0.674035\nCost after iteration 7800: 0.674035\nCost after iteration 7900: 0.674035\nCost after iteration 8000: 0.674035\nCost after iteration 8100: 0.674035\nCost after iteration 8200: 0.674035\nCost after iteration 8300: 0.674035\nCost after iteration 8400: 0.674035\nCost after iteration 8500: 0.674035\nCost after iteration 8600: 0.674034\nCost after iteration 8700: 0.674034\nCost after iteration 8800: 0.674034\nCost after iteration 8900: 0.674034\nCost after iteration 9000: 0.674034\nCost after iteration 9100: 0.674034\nCost after iteration 9200: 0.674034\nCost after iteration 9300: 0.674034\nCost after iteration 9400: 0.674034\nCost after iteration 9500: 0.674034\nCost after iteration 9600: 0.674034\nCost after iteration 9700: 0.674034\nCost after iteration 9800: 0.674034\nCost after iteration 9900: 0.674034\nCost after iteration 10000: 0.674033\nCost after iteration 10100: 0.674033\nCost after iteration 10200: 0.674033\nCost after iteration 10300: 0.674033\nCost after iteration 10400: 0.674033\nCost after iteration 10500: 0.674033\nCost after iteration 10600: 0.674033\nCost after iteration 10700: 0.674033\nCost after iteration 10800: 0.674033\nCost after iteration 10900: 0.674033\nCost after iteration 11000: 0.674033\nCost after iteration 11100: 0.674033\nCost after iteration 11200: 0.674033\nCost after iteration 11300: 0.674033\nCost after iteration 11400: 0.674032\nCost after iteration 11500: 0.674032\nCost after iteration 11600: 0.674032\nCost after iteration 11700: 0.674032\nCost after iteration 11800: 0.674032\nCost after iteration 11900: 0.674032\nCost after iteration 12000: 0.674032\nCost after iteration 12100: 0.674032\nCost after iteration 12200: 0.674032\nCost after iteration 12300: 0.674032\nCost after iteration 12400: 0.674032\nCost after iteration 12500: 0.674032\nCost after iteration 12600: 0.674032\nCost after iteration 12700: 0.674032\nCost after iteration 12800: 0.674031\nCost after iteration 12900: 0.674031\nCost after iteration 13000: 0.674031\nCost after iteration 13100: 0.674031\nCost after iteration 13200: 0.674031\nCost after iteration 13300: 0.674031\nCost after iteration 13400: 0.674031\nCost after iteration 13500: 0.674031\nCost after iteration 13600: 0.674031\nCost after iteration 13700: 0.674031\nCost after iteration 13800: 0.674031\nCost after iteration 13900: 0.674031\nCost after iteration 14000: 0.674031\nCost after iteration 14100: 0.674031\nCost after iteration 14200: 0.674030\nCost after iteration 14300: 0.674030\nCost after iteration 14400: 0.674030\nCost after iteration 14500: 0.674030\nCost after iteration 14600: 0.674030\nCost after iteration 14700: 0.674030\nCost after iteration 14800: 0.674030\nCost after iteration 14900: 0.674030\nè®­ç»ƒé›†å‡†ç¡®ç‡: 58.24634655532359 \næµ‹è¯•é›†å‡†ç¡®ç‡: 47.5 \n"
      }
     ],
     "persistentId": "b175609a-a7d0-4bf8-84b6-23b93d651481",
     "text": "costs = model(X_train, y_train, X_test, y_test, num_iterations=15000, learning_rate=0.003)"
    },
    "executionTime": "2020-02-04T06:00:09.962Z"
   },
   {
    "cell": {
     "executionCount": 105,
     "executionEventId": "068e48e8-3746-4723-beeb-af2b05af1746",
     "hasError": false,
     "id": "2f3091fe-c647-4608-8d68-1ac2e6a9a58e",
     "outputs": [
      {
       "data": {
        "text/plain": "[5.4371952601642315,\n 4.283014914460845,\n 3.5417926477967185,\n 1.82768898188418,\n 0.677355531517465,\n 0.6740396287879201,\n 0.6740389254049513,\n 0.6740385125074229,\n 0.6740381032185067,\n 0.6740376974668212,\n 0.6740372952217593,\n 0.6740368964530085,\n 0.6740365011305188,\n 0.674036109224501,\n 0.6740357207054225,\n 0.6740353355440066,\n 0.6740349537112295,\n 0.6740345751783182,\n 0.6740341999167491,\n 0.6740338278982441,\n 0.6740334590947704,\n 0.6740330934785376,\n 0.674032731021995,\n 0.6740323716978307,\n 0.6740320154789684,\n 0.6740316623385663,\n 0.6740313122500143,\n 0.6740309651869328,\n 0.6740306211231698,\n 0.6740302800327997]"
       },
       "execution_count": 105,
       "metadata": {},
       "output_type": "execute_result"
      }
     ],
     "persistentId": "5a7025eb-9ff9-4549-af17-da5334c23c45",
     "text": "costs"
    },
    "executionTime": "2020-02-04T06:00:17.197Z"
   },
   {
    "cell": {
     "executionCount": 106,
     "executionEventId": "36f74d83-7a3e-4e86-91c8-5619045e18f6",
     "hasError": true,
     "id": "2f3091fe-c647-4608-8d68-1ac2e6a9a58e",
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'losses' is not defined",
       "output_type": "error",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
        "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-106-fc099f0b54e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mNameError\u001b[0m: name 'losses' is not defined"
       ]
      }
     ],
     "persistentId": "5a7025eb-9ff9-4549-af17-da5334c23c45",
     "text": "plt.plot([i for i in range(len(losses))], losses)"
    },
    "executionTime": "2020-02-04T06:03:49.081Z"
   },
   {
    "cell": {
     "executionCount": 107,
     "executionEventId": "109f356e-8aab-4a12-9069-882a24a31137",
     "hasError": false,
     "id": "2f3091fe-c647-4608-8d68-1ac2e6a9a58e",
     "outputs": [
      {
       "data": {
        "text/plain": "[<matplotlib.lines.Line2D at 0x1a1b588b50>]"
       },
       "execution_count": 107,
       "metadata": {},
       "output_type": "execute_result"
      },
      {
       "data": {
        "image/png": "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\n",
        "text/plain": "<Figure size 432x288 with 1 Axes>"
       },
       "metadata": {
        "needs_background": "light"
       },
       "output_type": "display_data"
      }
     ],
     "persistentId": "5a7025eb-9ff9-4549-af17-da5334c23c45",
     "text": "plt.plot([i for i in range(len(costs))], costs)"
    },
    "executionTime": "2020-02-04T06:04:05.643Z"
   },
   {
    "cell": {
     "executionCount": 108,
     "executionEventId": "4db34966-83cb-48b3-9d68-51ab33c0e5d7",
     "hasError": false,
     "id": "3684ed3e-1b32-43d7-8d1d-ff157ecad878",
     "outputs": [
      {
       "data": {
        "text/plain": "30"
       },
       "execution_count": 108,
       "metadata": {},
       "output_type": "execute_result"
      }
     ],
     "persistentId": "d8d67115-9bc2-456f-8ab0-ef67ecd1b88e",
     "text": "len(costs)"
    },
    "executionTime": "2020-02-04T06:04:42.672Z"
   },
   {
    "cell": {
     "executionCount": 109,
     "executionEventId": "7b9fdbb4-57bf-4d57-a5ab-2ac7e72c24ef",
     "hasError": false,
     "id": "f0685016-4bda-4ca9-9976-ae5f4793ece6",
     "outputs": [],
     "persistentId": "39737492-b457-4dcf-8d9f-76a392488151",
     "text": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost=True):\n    '''\n    This function optimize w and b by running a gradient descen algorithm\n    \n    Arguments:\n    w - weights\n    b - bias\n    X - data\n    Y - ground truth\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n    \n    Returns:\n    params - dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n    \n    '''\n    \n    costs = []\n    \n    for i in range(num_iterations):\n        \n        grads, cost = propagate(w,b,X,Y)\n        \n        dw = grads['dw']\n        db = grads['db']\n        \n        w = w - learning_rate * dw\n        b = b - learning_rate * db\n        \n        if i % 100 == 0:\n            costs.append(cost)\n        if print_cost and i % 500 == 0:\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    \n    params = {\"w\":w,\n              \"b\":b}\n    \n    grads = {\"dw\":dw,\n             \"db\":db}\n    \n    return params, grads, costs"
    },
    "executionTime": "2020-02-04T06:05:45.339Z"
   },
   {
    "cell": {
     "executionCount": 110,
     "executionEventId": "614e2306-6cb2-430a-903f-ecf35559ab13",
     "hasError": false,
     "id": "1fde88e1-df7e-49c2-ae1f-e396411d30e3",
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "Cost after iteration 0: 5.393816\nCost after iteration 500: 4.534530\nCost after iteration 1000: 3.907767\nCost after iteration 1500: 2.786945\nCost after iteration 2000: 0.953009\nCost after iteration 2500: 0.674089\nCost after iteration 3000: 0.674033\nCost after iteration 3500: 0.674032\nCost after iteration 4000: 0.674032\nCost after iteration 4500: 0.674031\nCost after iteration 5000: 0.674031\nCost after iteration 5500: 0.674031\nCost after iteration 6000: 0.674030\nCost after iteration 6500: 0.674030\nCost after iteration 7000: 0.674030\nCost after iteration 7500: 0.674029\nCost after iteration 8000: 0.674029\nCost after iteration 8500: 0.674029\nCost after iteration 9000: 0.674028\nCost after iteration 9500: 0.674028\nCost after iteration 10000: 0.674028\nCost after iteration 10500: 0.674027\nCost after iteration 11000: 0.674027\nCost after iteration 11500: 0.674027\nCost after iteration 12000: 0.674027\nCost after iteration 12500: 0.674026\nCost after iteration 13000: 0.674026\nCost after iteration 13500: 0.674026\nCost after iteration 14000: 0.674025\nCost after iteration 14500: 0.674025\nè®­ç»ƒé›†å‡†ç¡®ç‡: 58.24634655532359 \næµ‹è¯•é›†å‡†ç¡®ç‡: 47.77777777777777 \n"
      }
     ],
     "persistentId": "b175609a-a7d0-4bf8-84b6-23b93d651481",
     "text": "costs = model(X_train, y_train, X_test, y_test, num_iterations=15000, learning_rate=0.003)"
    },
    "executionTime": "2020-02-04T06:05:52.379Z"
   },
   {
    "cell": {
     "executionCount": 111,
     "executionEventId": "5a004dd5-d0a4-4f93-a6d4-4fab77e64b17",
     "hasError": false,
     "id": "2f3091fe-c647-4608-8d68-1ac2e6a9a58e",
     "outputs": [
      {
       "data": {
        "text/plain": "[<matplotlib.lines.Line2D at 0x1a2503efd0>]"
       },
       "execution_count": 111,
       "metadata": {},
       "output_type": "execute_result"
      },
      {
       "data": {
        "image/png": "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\n",
        "text/plain": "<Figure size 432x288 with 1 Axes>"
       },
       "metadata": {
        "needs_background": "light"
       },
       "output_type": "display_data"
      }
     ],
     "persistentId": "5a7025eb-9ff9-4549-af17-da5334c23c45",
     "text": "plt.plot([i for i in range(len(costs))], costs)"
    },
    "executionTime": "2020-02-04T06:05:58.576Z"
   },
   {
    "cell": {
     "executionCount": 112,
     "executionEventId": "674ed81d-2a5b-4ccd-8168-be7d582760be",
     "hasError": false,
     "id": "2f3091fe-c647-4608-8d68-1ac2e6a9a58e",
     "outputs": [
      {
       "data": {
        "text/plain": "[<matplotlib.lines.Line2D at 0x1a25065810>]"
       },
       "execution_count": 112,
       "metadata": {},
       "output_type": "execute_result"
      },
      {
       "data": {
        "image/png": "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\n",
        "text/plain": "<Figure size 432x288 with 1 Axes>"
       },
       "metadata": {
        "needs_background": "light"
       },
       "output_type": "display_data"
      }
     ],
     "persistentId": "5a7025eb-9ff9-4549-af17-da5334c23c45",
     "text": "plt.plot([i*100 for i in range(len(costs))], costs)"
    },
    "executionTime": "2020-02-04T06:06:18.953Z"
   },
   {
    "cell": {
     "executionCount": 113,
     "executionEventId": "360d6cec-585f-4b70-b4bb-96ad08752295",
     "hasError": false,
     "id": "2f3091fe-c647-4608-8d68-1ac2e6a9a58e",
     "outputs": [
      {
       "data": {
        "text/plain": "Text(0, 0.5, 'Logitic Loss')"
       },
       "execution_count": 113,
       "metadata": {},
       "output_type": "execute_result"
      },
      {
       "data": {
        "image/png": "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\n",
        "text/plain": "<Figure size 432x288 with 1 Axes>"
       },
       "metadata": {
        "needs_background": "light"
       },
       "output_type": "display_data"
      }
     ],
     "persistentId": "5a7025eb-9ff9-4549-af17-da5334c23c45",
     "text": "fig = plt.figure()\nax = fig.add_subplot(1,1,1)\nax.plot([i*100 for i in range(len(costs))], costs)\nax.set_xlabel(\"Epoach\")\nax.set_ylabel(\"Logitic Loss\")"
    },
    "executionTime": "2020-02-04T06:11:45.522Z"
   },
   {
    "cell": {
     "executionCount": 114,
     "executionEventId": "e7403884-91db-49ed-9745-87f06489bf46",
     "hasError": false,
     "id": "2f3091fe-c647-4608-8d68-1ac2e6a9a58e",
     "outputs": [
      {
       "data": {
        "text/plain": "Text(0, 0.5, 'Logistic Loss')"
       },
       "execution_count": 114,
       "metadata": {},
       "output_type": "execute_result"
      },
      {
       "data": {
        "image/png": "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\n",
        "text/plain": "<Figure size 432x288 with 1 Axes>"
       },
       "metadata": {
        "needs_background": "light"
       },
       "output_type": "display_data"
      }
     ],
     "persistentId": "5a7025eb-9ff9-4549-af17-da5334c23c45",
     "text": "fig = plt.figure()\nax = fig.add_subplot(1,1,1)\nax.plot([i*100 for i in range(len(costs))], costs)\nax.set_xlabel(\"Iteration Times\")\nax.set_ylabel(\"Logistic Loss\")"
    },
    "executionTime": "2020-02-04T06:12:46.452Z"
   },
   {
    "cell": {
     "executionCount": 115,
     "executionEventId": "464f134b-7fcd-46cf-82ec-919246682f12",
     "hasError": true,
     "id": "2f3091fe-c647-4608-8d68-1ac2e6a9a58e",
     "outputs": [
      {
       "ename": "ModuleNotFoundError",
       "evalue": "No module named 'ipympl'",
       "output_type": "error",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
        "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-115-b19051956355>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'widget'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcosts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2312\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2313\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2314\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2315\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m</Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/decorator.py:decorator-gen-108>\u001b[0m in \u001b[0;36mmatplotlib\u001b[0;34m(self, line)\u001b[0m\n",
        "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/IPython/core/magics/pylab.py\u001b[0m in \u001b[0;36mmatplotlib\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Available matplotlib backends: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbackends_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0mgui\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_matplotlib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_matplotlib_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgui\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36menable_matplotlib\u001b[0;34m(self, gui)\u001b[0m\n\u001b[1;32m   3412\u001b[0m                 \u001b[0mgui\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_gui_and_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpylab_gui_select\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3414\u001b[0;31m         \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivate_matplotlib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3415\u001b[0m         \u001b[0mpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigure_inline_support\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/IPython/core/pylabtools.py\u001b[0m in \u001b[0;36mactivate_matplotlib\u001b[0;34m(backend)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m     \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m     \u001b[0;31m# This must be imported last in the matplotlib series, after\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mswitch_backend\u001b[0;34m(newbackend)\u001b[0m\n\u001b[1;32m    219\u001b[0m         else \"matplotlib.backends.backend_{}\".format(newbackend.lower()))\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m     \u001b[0mbackend_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackend_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m     Backend = type(\n\u001b[1;32m    223\u001b[0m         \"Backend\", (matplotlib.backends._Backend,), vars(backend_mod))\n",
        "\u001b[0;32m~/opt/anaconda3/lib/python3.7/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m~/opt/anaconda3/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
        "\u001b[0;32m~/opt/anaconda3/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
        "\u001b[0;32m~/opt/anaconda3/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
        "\u001b[0;32m~/opt/anaconda3/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
        "\u001b[0;32m~/opt/anaconda3/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
        "\u001b[0;32m~/opt/anaconda3/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
        "\u001b[0;32m~/opt/anaconda3/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
        "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'ipympl'"
       ]
      }
     ],
     "persistentId": "5a7025eb-9ff9-4549-af17-da5334c23c45",
     "text": "%matplotlib widget\n\nfig = plt.figure()\nax = fig.add_subplot(1,1,1)\nax.plot([i*100 for i in range(len(costs))], costs)\nax.set_xlabel(\"Iteration Times\")\nax.set_ylabel(\"Logistic Loss\")"
    },
    "executionTime": "2020-02-04T06:13:02.993Z"
   },
   {
    "cell": {
     "executionCount": 116,
     "executionEventId": "b2cd800c-8959-430e-8de8-b77ff00ca3db",
     "hasError": false,
     "id": "2f3091fe-c647-4608-8d68-1ac2e6a9a58e",
     "outputs": [
      {
       "data": {
        "application/vnd.jupyter.widget-view+json": {
         "model_id": "070d88278a274debb1da6908ead6af5e",
         "version_major": 2,
         "version_minor": 0
        },
        "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
       },
       "metadata": {},
       "output_type": "display_data"
      },
      {
       "data": {
        "text/plain": "Text(0, 0.5, 'Logistic Loss')"
       },
       "execution_count": 116,
       "metadata": {},
       "output_type": "execute_result"
      }
     ],
     "persistentId": "5a7025eb-9ff9-4549-af17-da5334c23c45",
     "text": "%matplotlib widget\n\nfig = plt.figure()\nax = fig.add_subplot(1,1,1)\nax.plot([i*100 for i in range(len(costs))], costs)\nax.set_xlabel(\"Iteration Times\")\nax.set_ylabel(\"Logistic Loss\")"
    },
    "executionTime": "2020-02-04T06:13:28.093Z"
   },
   {
    "cell": {
     "executionCount": 117,
     "executionEventId": "b7337b9e-667a-49f6-a981-efcd9d05b243",
     "hasError": true,
     "id": "2f3091fe-c647-4608-8d68-1ac2e6a9a58e",
     "outputs": [
      {
       "data": {
        "application/vnd.jupyter.widget-view+json": {
         "model_id": "a332a68bd1414385bd95609d29978bc0",
         "version_major": 2,
         "version_minor": 0
        },
        "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
       },
       "metadata": {},
       "output_type": "display_data"
      },
      {
       "ename": "AttributeError",
       "evalue": "'AxesSubplot' object has no attribute 'set_titile'",
       "output_type": "error",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
        "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-117-1a1060c0885a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Iteration Times\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Logistic Loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_titile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"learning_rate=0.003\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mAttributeError\u001b[0m: 'AxesSubplot' object has no attribute 'set_titile'"
       ]
      }
     ],
     "persistentId": "5a7025eb-9ff9-4549-af17-da5334c23c45",
     "text": "%matplotlib widget\n\nfig = plt.figure()\nax = fig.add_subplot(1,1,1)\nax.plot([i*100 for i in range(len(costs))], costs)\nax.set_xlabel(\"Iteration Times\")\nax.set_ylabel(\"Logistic Loss\")\nax.set_titile(\"learning_rate=0.003\")"
    },
    "executionTime": "2020-02-04T06:14:30.532Z"
   },
   {
    "cell": {
     "executionCount": 118,
     "executionEventId": "84854ab8-e837-46f9-885c-c8da1ae4aece",
     "hasError": false,
     "id": "2f3091fe-c647-4608-8d68-1ac2e6a9a58e",
     "outputs": [
      {
       "data": {
        "application/vnd.jupyter.widget-view+json": {
         "model_id": "a7b0e8c7bb024aa2ba3a8c9bac786e2f",
         "version_major": 2,
         "version_minor": 0
        },
        "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
       },
       "metadata": {},
       "output_type": "display_data"
      },
      {
       "data": {
        "text/plain": "Text(0.5, 1.0, 'learning_rate=0.003')"
       },
       "execution_count": 118,
       "metadata": {},
       "output_type": "execute_result"
      }
     ],
     "persistentId": "5a7025eb-9ff9-4549-af17-da5334c23c45",
     "text": "%matplotlib widget\n\nfig = plt.figure()\nax = fig.add_subplot(1,1,1)\nax.plot([i*100 for i in range(len(costs))], costs)\nax.set_xlabel(\"Iteration Times\")\nax.set_ylabel(\"Logistic Loss\")\nax.set_title(\"learning_rate=0.003\")"
    },
    "executionTime": "2020-02-04T06:14:54.735Z"
   },
   {
    "cell": {
     "executionCount": 119,
     "executionEventId": "94d04844-a205-440e-acf6-d0563d746aeb",
     "hasError": false,
     "id": "1fde88e1-df7e-49c2-ae1f-e396411d30e3",
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "Cost after iteration 0: 5.303207\nCost after iteration 500: 3.533999\nCost after iteration 1000: 0.717257\nCost after iteration 1500: 0.674036\nCost after iteration 2000: 0.674035\nCost after iteration 2500: 0.674035\nCost after iteration 3000: 0.674034\nCost after iteration 3500: 0.674033\nCost after iteration 4000: 0.674033\nCost after iteration 4500: 0.674032\nCost after iteration 5000: 0.674032\nCost after iteration 5500: 0.674031\nCost after iteration 6000: 0.674031\nCost after iteration 6500: 0.674030\nCost after iteration 7000: 0.674029\nCost after iteration 7500: 0.674029\nCost after iteration 8000: 0.674028\nCost after iteration 8500: 0.674028\nCost after iteration 9000: 0.674027\nCost after iteration 9500: 0.674027\nCost after iteration 10000: 0.674026\nCost after iteration 10500: 0.674026\nCost after iteration 11000: 0.674025\nCost after iteration 11500: 0.674025\nCost after iteration 12000: 0.674024\nCost after iteration 12500: 0.674024\nCost after iteration 13000: 0.674023\nCost after iteration 13500: 0.674023\nCost after iteration 14000: 0.674022\nCost after iteration 14500: 0.674022\nè®­ç»ƒé›†å‡†ç¡®ç‡: 58.24634655532359 \næµ‹è¯•é›†å‡†ç¡®ç‡: 47.77777777777777 \nCost after iteration 0: 5.178825\nCost after iteration 500: 0.898289\nCost after iteration 1000: 1.535077\nCost after iteration 1500: 1.535032\nCost after iteration 2000: 1.535012\nCost after iteration 2500: 1.534992\nCost after iteration 3000: 1.534972\nCost after iteration 3500: 1.534952\nCost after iteration 4000: 1.534932\nCost after iteration 4500: 1.534912\nCost after iteration 5000: 1.534892\nCost after iteration 5500: 1.534872\nCost after iteration 6000: 1.534852\nCost after iteration 6500: 1.534833\nCost after iteration 7000: 1.534813\nCost after iteration 7500: 1.534794\nCost after iteration 8000: 1.534774\nCost after iteration 8500: 1.534755\nCost after iteration 9000: 1.534736\nCost after iteration 9500: 1.534716\nCost after iteration 10000: 1.534697\nCost after iteration 10500: 1.534678\nCost after iteration 11000: 1.534659\nCost after iteration 11500: 1.534640\nCost after iteration 12000: 1.534621\nCost after iteration 12500: 1.534603\nCost after iteration 13000: 1.534584\nCost after iteration 13500: 1.534565\nCost after iteration 14000: 1.534547\nCost after iteration 14500: 1.534528\nè®­ç»ƒé›†å‡†ç¡®ç‡: 50.521920668058456 \næµ‹è¯•é›†å‡†ç¡®ç‡: 48.88888888888889 \n"
      }
     ],
     "persistentId": "b175609a-a7d0-4bf8-84b6-23b93d651481",
     "text": "costs_1 = model(X_train, y_train, X_test, y_test, num_iterations=15000, learning_rate=0.005)\ncosts_2 = model(X_train, y_train, X_test, y_test, num_iterations=15000, learning_rate=0.010)"
    },
    "executionTime": "2020-02-04T06:16:28.762Z"
   },
   {
    "cell": {
     "executionCount": 120,
     "executionEventId": "8450a966-8738-44f5-b9f9-a314ee6799f0",
     "hasError": false,
     "id": "2f3091fe-c647-4608-8d68-1ac2e6a9a58e",
     "outputs": [
      {
       "data": {
        "application/vnd.jupyter.widget-view+json": {
         "model_id": "9283ea7ec6e04068b41d31b6a33d707f",
         "version_major": 2,
         "version_minor": 0
        },
        "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
       },
       "metadata": {},
       "output_type": "display_data"
      },
      {
       "data": {
        "text/plain": "Text(0.5, 1.0, 'learning_rate=0.005')"
       },
       "execution_count": 120,
       "metadata": {},
       "output_type": "execute_result"
      }
     ],
     "persistentId": "5a7025eb-9ff9-4549-af17-da5334c23c45",
     "text": "%matplotlib widget\n\nfig = plt.figure()\nax = fig.add_subplot(1,1,1)\nax.plot([i*100 for i in range(len(costs_1))], costs_1)\nax.set_xlabel(\"Iteration Times\")\nax.set_ylabel(\"Logistic Loss\")\nax.set_title(\"learning_rate=0.005\")"
    },
    "executionTime": "2020-02-04T06:17:13.613Z"
   },
   {
    "cell": {
     "executionCount": 121,
     "executionEventId": "dd4aa93c-0d60-4550-85b1-0b8948a1d48f",
     "hasError": false,
     "id": "1fd9429b-46e6-4de1-aa4f-cf53affb9a89",
     "outputs": [
      {
       "data": {
        "application/vnd.jupyter.widget-view+json": {
         "model_id": "5c8560dce1614e3a836be32aca89ac2d",
         "version_major": 2,
         "version_minor": 0
        },
        "text/plain": "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
       },
       "metadata": {},
       "output_type": "display_data"
      },
      {
       "data": {
        "text/plain": "Text(0.5, 1.0, 'learning_rate=0.010')"
       },
       "execution_count": 121,
       "metadata": {},
       "output_type": "execute_result"
      }
     ],
     "persistentId": "d11113f9-daa6-409f-be6d-619265160ef1",
     "text": "fig = plt.figure()\nax = fig.add_subplot(1,1,1)\nax.plot([i*100 for i in range(len(costs_2))], costs_2)\nax.set_xlabel(\"Iteration Times\")\nax.set_ylabel(\"Logistic Loss\")\nax.set_title(\"learning_rate=0.010\")"
    },
    "executionTime": "2020-02-04T06:17:41.939Z"
   },
   {
    "cell": {
     "executionCount": 122,
     "executionEventId": "b392e122-ab57-430a-95d4-f22e1e634a55",
     "hasError": false,
     "id": "0a10fc69-0a53-4ddf-ba76-d069a27a8495",
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "Cost after iteration 0: 5.480770\nCost after iteration 500: 5.054500\nCost after iteration 1000: 4.836736\nCost after iteration 1500: 4.600778\nCost after iteration 2000: 4.306036\nCost after iteration 2500: 3.897267\nCost after iteration 3000: 3.355979\nCost after iteration 3500: 2.638188\nCost after iteration 4000: 1.709383\nCost after iteration 4500: 0.928991\nCost after iteration 5000: 0.690485\nCost after iteration 5500: 0.674641\nCost after iteration 6000: 0.674094\nCost after iteration 6500: 0.674074\nCost after iteration 7000: 0.674073\nCost after iteration 7500: 0.674073\nCost after iteration 8000: 0.674073\nCost after iteration 8500: 0.674072\nCost after iteration 9000: 0.674072\nCost after iteration 9500: 0.674072\nCost after iteration 10000: 0.674072\nCost after iteration 10500: 0.674072\nCost after iteration 11000: 0.674071\nCost after iteration 11500: 0.674071\nCost after iteration 12000: 0.674071\nCost after iteration 12500: 0.674071\nCost after iteration 13000: 0.674070\nCost after iteration 13500: 0.674070\nCost after iteration 14000: 0.674070\nCost after iteration 14500: 0.674070\nè®­ç»ƒé›†å‡†ç¡®ç‡: 58.037578288100214 \næµ‹è¯•é›†å‡†ç¡®ç‡: 49.72222222222222 \nCost after iteration 0: 5.363943\nCost after iteration 500: 5.033437\nCost after iteration 1000: 4.830668\nCost after iteration 1500: 4.632301\nCost after iteration 2000: 4.387513\nCost after iteration 2500: 4.059347\nCost after iteration 3000: 3.608100\nCost after iteration 3500: 3.004029\nCost after iteration 4000: 2.198854\nCost after iteration 4500: 1.326211\nCost after iteration 5000: 0.784382\nCost after iteration 5500: 0.679739\nCost after iteration 6000: 0.674299\nCost after iteration 6500: 0.674082\nCost after iteration 7000: 0.674073\nCost after iteration 7500: 0.674072\nCost after iteration 8000: 0.674072\nCost after iteration 8500: 0.674072\nCost after iteration 9000: 0.674071\nCost after iteration 9500: 0.674071\nCost after iteration 10000: 0.674071\nCost after iteration 10500: 0.674071\nCost after iteration 11000: 0.674070\nCost after iteration 11500: 0.674070\nCost after iteration 12000: 0.674070\nCost after iteration 12500: 0.674070\nCost after iteration 13000: 0.674069\nCost after iteration 13500: 0.674069\nCost after iteration 14000: 0.674069\nCost after iteration 14500: 0.674069\nCost after iteration 15000: 0.674069\nCost after iteration 15500: 0.674068\nCost after iteration 16000: 0.674068\nCost after iteration 16500: 0.674068\nCost after iteration 17000: 0.674068\nCost after iteration 17500: 0.674067\nCost after iteration 18000: 0.674067\nCost after iteration 18500: 0.674067\nCost after iteration 19000: 0.674067\nCost after iteration 19500: 0.674067\nCost after iteration 20000: 0.674066\nCost after iteration 20500: 0.674066\nCost after iteration 21000: 0.674066\nCost after iteration 21500: 0.674066\nCost after iteration 22000: 0.674065\nCost after iteration 22500: 0.674065\nCost after iteration 23000: 0.674065\nCost after iteration 23500: 0.674065\nCost after iteration 24000: 0.674065\nCost after iteration 24500: 0.674064\nCost after iteration 25000: 0.674064\nCost after iteration 25500: 0.674064\nCost after iteration 26000: 0.674064\nCost after iteration 26500: 0.674064\nCost after iteration 27000: 0.674063\nCost after iteration 27500: 0.674063\nCost after iteration 28000: 0.674063\nCost after iteration 28500: 0.674063\nCost after iteration 29000: 0.674063\nCost after iteration 29500: 0.674062\nè®­ç»ƒé›†å‡†ç¡®ç‡: 58.107167710508 \næµ‹è¯•é›†å‡†ç¡®ç‡: 49.72222222222222 \n"
      }
     ],
     "persistentId": "bf483358-d780-4166-8851-b81c338328eb",
     "text": "costs_3 = model(X_train, y_train, X_test, y_test, num_iterations=15000, learning_rate=0.001)\ncosts_4 = model(X_train, y_train, X_test, y_test, num_iterations=30000, learning_rate=0.001)"
    },
    "executionTime": "2020-02-04T06:22:14.085Z"
   },
   {
    "cell": {
     "executionCount": 123,
     "executionEventId": "bde6e64e-0b6e-4453-98ae-95355f02df8e",
     "hasError": false,
     "id": "0a10fc69-0a53-4ddf-ba76-d069a27a8495",
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "Cost after iteration 0: 5.510407\nCost after iteration 500: 4.646729\nCost after iteration 1000: 4.431910\nCost after iteration 1500: 4.188366\nCost after iteration 2000: 3.904072\nCost after iteration 2500: 3.547287\nCost after iteration 3000: 3.097581\nCost after iteration 3500: 2.505989\nCost after iteration 4000: 1.779007\nCost after iteration 4500: 1.109273\nè®­ç»ƒé›†å‡†ç¡®ç‡: 55.741127348643005 \næµ‹è¯•é›†å‡†ç¡®ç‡: 48.88888888888889 \nCost after iteration 0: 5.134843\nCost after iteration 500: 4.918010\nCost after iteration 1000: 4.760169\nCost after iteration 1500: 4.572651\nCost after iteration 2000: 4.353589\nCost after iteration 2500: 4.097068\nCost after iteration 3000: 3.772933\nCost after iteration 3500: 3.368353\nCost after iteration 4000: 2.855341\nCost after iteration 4500: 2.205630\nCost after iteration 5000: 1.472544\nCost after iteration 5500: 0.909153\nCost after iteration 6000: 0.700704\nCost after iteration 6500: 0.675632\nCost after iteration 7000: 0.674142\nCost after iteration 7500: 0.674063\nCost after iteration 8000: 0.674059\nCost after iteration 8500: 0.674058\nCost after iteration 9000: 0.674058\nCost after iteration 9500: 0.674058\nè®­ç»ƒé›†å‡†ç¡®ç‡: 58.176757132915796 \næµ‹è¯•é›†å‡†ç¡®ç‡: 47.5 \n"
      }
     ],
     "persistentId": "bf483358-d780-4166-8851-b81c338328eb",
     "text": "costs_3 = model(X_train, y_train, X_test, y_test, num_iterations=5000, learning_rate=0.001)\ncosts_4 = model(X_train, y_train, X_test, y_test, num_iterations=10000, learning_rate=0.001)"
    },
    "executionTime": "2020-02-04T06:22:32.955Z"
   },
   {
    "cell": {
     "executionCount": 1,
     "executionEventId": "a86c8f49-dc42-4094-8374-2f3f79e03810",
     "hasError": false,
     "id": "b7c3f9f9-46a2-4a43-bcff-d8db6cc3d613",
     "outputs": [],
     "persistentId": "1eed7f4a-251f-46b5-b994-cbed2f5d5ede",
     "text": "def softmax(X):\n    #softmaxå‡½æ•°\n    return np.exp(X) / np.sum(np.exp(X))\n"
    },
    "executionTime": "2020-02-13T04:33:06.168Z"
   },
   {
    "cell": {
     "executionCount": 2,
     "executionEventId": "474a3e21-0134-4878-9919-d6806dabd5c7",
     "hasError": false,
     "id": "d1fcd8de-aaa4-4929-8450-ff6520180f8f",
     "outputs": [],
     "persistentId": "cd5e8289-6c99-4263-a0b9-737ee1fa08e5",
     "text": "from sklearn import datasets\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split"
    },
    "executionTime": "2020-02-13T04:33:11.614Z"
   },
   {
    "cell": {
     "executionCount": 3,
     "executionEventId": "b431a7c8-6961-4574-9993-a868b003911b",
     "hasError": false,
     "id": "0b9c4ea0-870b-4bb2-9f94-3261d7d86f34",
     "outputs": [],
     "persistentId": "73e4c7a0-2a9c-4d42-857b-03bc36289cc1",
     "text": "# Loading the data \ndigits = datasets.load_digits()"
    },
    "executionTime": "2020-02-13T04:33:21.561Z"
   },
   {
    "cell": {
     "executionCount": 4,
     "executionEventId": "776cbf11-d3e6-4a50-a246-e44f364368d5",
     "hasError": false,
     "id": "99b9c1bf-a792-492d-aa77-e0012aa8cbf7",
     "outputs": [
      {
       "data": {
        "image/png": "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\n",
        "text/plain": "<Figure size 432x288 with 10 Axes>"
       },
       "metadata": {},
       "output_type": "display_data"
      }
     ],
     "persistentId": "40b6af44-7b70-474f-8251-cb9fb55b9508",
     "text": "# Vilizating the data\nfor i in range(1,11):\n    plt.subplot(2,5,i)\n    plt.imshow(digits.data[i-1].reshape([8,8]),cmap=plt.cm.gray_r)\n    plt.text(3,10,str(digits.target[i-1]))\n    plt.xticks([])\n    plt.yticks([])\nplt.show()"
    },
    "executionTime": "2020-02-13T04:33:22.562Z"
   },
   {
    "cell": {
     "executionCount": 5,
     "executionEventId": "e7c89502-d908-486f-bf31-e0d9c14ba394",
     "hasError": false,
     "id": "ddc14d63-da76-4fd1-8bdb-36af01bddce3",
     "outputs": [],
     "persistentId": "4c5120f7-b102-4340-ba26-e02a666ab9f6",
     "text": "# Split the data into training set and test set \nX_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.2)"
    },
    "executionTime": "2020-02-13T04:33:25.612Z"
   },
   {
    "cell": {
     "executionCount": 6,
     "executionEventId": "cb32099c-ee3d-479e-863a-493718f387d3",
     "hasError": false,
     "id": "e3fb24e6-280b-4b13-88d6-4335f217d6a1",
     "outputs": [],
     "persistentId": "e2580523-6d94-469d-9bde-b795e40ef9dd",
     "text": "# reformulate the label. \n# If the digit is smaller than 5, the label is 0.\n# If the digit is larger than 5, the label is 1.\n\ny_train[y_train < 5 ] = 0\ny_train[y_train >= 5] = 1\ny_test[y_test < 5] = 0\ny_test[y_test >= 5] = 1"
    },
    "executionTime": "2020-02-13T04:33:27.416Z"
   },
   {
    "cell": {
     "executionCount": 7,
     "executionEventId": "e4db12a3-f1aa-4f4b-859d-541b53deb2de",
     "hasError": false,
     "id": "b3f1ae6a-de82-4cb2-a91d-e1634084241d",
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "(1437, 64)\n(360, 64)\n(1437,)\n(360,)\n"
      }
     ],
     "persistentId": "6d8bcceb-9c81-4f4f-b84a-fb06e0846465",
     "text": "print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)"
    },
    "executionTime": "2020-02-13T04:33:29.293Z"
   },
   {
    "cell": {
     "executionCount": 8,
     "executionEventId": "7ad85af6-aeeb-4d36-98bf-3dc4588bd19a",
     "hasError": false,
     "id": "db332daf-b724-4247-888c-d550746f3259",
     "outputs": [],
     "persistentId": "cb15bbde-4a6d-461a-ae73-8bc399b9d35b",
     "text": "import numpy as np\ndef sigmoid(z):\n    '''\n    Compute the sigmoid of z\n    Arguments: z -- a scalar or numpy array of any size.\n    \n    Return:\n    s -- sigmoid(z)\n    '''\n    s = 1./(1 + np.exp(-1 * z))\n    \n    return s"
    },
    "executionTime": "2020-02-13T04:33:47.160Z"
   },
   {
    "cell": {
     "executionCount": 9,
     "executionEventId": "9dee9b8d-ff98-4532-aa6e-96d4bc6d2615",
     "hasError": false,
     "id": "04b20393-c7b3-40f4-b0b6-1b4d6849c761",
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "sigmoid([0,2]) = [0.5        0.88079708]\n"
      }
     ],
     "persistentId": "e67aab82-7c95-494e-ac25-7c6832e9691a",
     "text": "# Test your code \n# The result should be [0.5 0.88079708]\nprint(\"sigmoid([0,2]) = \" + str(sigmoid(np.array([0,2]))))"
    },
    "executionTime": "2020-02-13T04:33:47.674Z"
   },
   {
    "cell": {
     "executionCount": 10,
     "executionEventId": "f8643ac5-fe74-4fa0-a2e9-5897ce36216e",
     "hasError": false,
     "id": "ba5beb66-4bde-4cd0-9ff3-b37de78bcdf7",
     "outputs": [],
     "persistentId": "a95a0c56-ed75-4037-b248-43adcba4e53d",
     "text": "# Random innitialize the parameters\n\ndef initialize_parameters(dim):\n    '''\n    Argument: dim -- size of the w vector\n    \n    Returns:\n    w -- initialized vector of shape (dim,1)\n    b -- initializaed scalar\n    '''\n    \n    w = np.random.randn(dim,1)\n    b = 0\n    \n    assert(w.shape == (dim,1))\n    assert(isinstance(b,float) or isinstance(b,int))\n    \n    return w,b"
    },
    "executionTime": "2020-02-13T04:38:03.361Z"
   },
   {
    "cell": {
     "executionCount": 11,
     "executionEventId": "4bb032fd-543a-45c1-950a-ade96db9d8d2",
     "hasError": false,
     "id": "aa457c49-b528-40d7-91ba-e76902a48492",
     "outputs": [
      {
       "data": {
        "text/plain": "array([1.03993418])"
       },
       "execution_count": 11,
       "metadata": {},
       "output_type": "execute_result"
      }
     ],
     "persistentId": "b2d719a8-91ab-4bcc-b8b0-4cf84e45d95a",
     "text": "np.random.randn(1)"
    },
    "executionTime": "2020-02-13T04:38:04.000Z"
   },
   {
    "cell": {
     "executionCount": 12,
     "executionEventId": "6c527a9c-afcf-4a59-95cc-53a881721d3d",
     "hasError": false,
     "id": "d2850cd6-284c-4e80-8a53-989484255aa4",
     "outputs": [],
     "persistentId": "d2518a89-1eeb-44fb-95a8-4eab59019f67",
     "text": "def propagate(w,b,X,Y):\n    '''\n    Implement the cost function and its gradient for the propagation\n    \n    Arguments:\n    w - weights\n    b - bias\n    X - data\n    Y - ground truth\n    '''\n    m = X.shape[1]\n    A = sigmoid(np.dot(w.T,X) + b)\n    cost = -1/m * np.sum(Y * np.log(A + 1e-5) + (1-Y) * np.log(1-A + 1e-5)) #æŸå¤±å‡½æ•°è¿™é‡Œæœ€å¥½åŠ ä¸€ä¸ªå°æ•°ï¼Œé˜²æ­¢é™¤0\n#     print(cost)\n    \n    dw = 1/m * np.dot(X,(A-Y).T)\n    db = 1/m * np.sum(A-Y)\n    \n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    cost = np.squeeze(cost)\n    assert(cost.shape == ())\n    \n    grads = {'dw':dw,\n             'db':db}\n    return grads, cost"
    },
    "executionTime": "2020-02-13T04:38:18.206Z"
   },
   {
    "cell": {
     "executionCount": 13,
     "executionEventId": "fc3058cb-bc2b-47ef-874c-ca94ce974d4f",
     "hasError": false,
     "id": "57c714c1-938f-49ee-afff-dee78d6a0641",
     "outputs": [],
     "persistentId": "39737492-b457-4dcf-8d9f-76a392488151",
     "text": "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost=True):\n    '''\n    This function optimize w and b by running a gradient descen algorithm\n    \n    Arguments:\n    w - weights\n    b - bias\n    X - data\n    Y - ground truth\n    num_iterations -- number of iterations of the optimization loop\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 100 steps\n    \n    Returns:\n    params - dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n    \n    '''\n    \n    costs = []\n    \n    for i in range(num_iterations):\n        \n        grads, cost = propagate(w,b,X,Y)\n        \n        dw = grads['dw']\n        db = grads['db']\n        \n        w = w - learning_rate * dw\n        b = b - learning_rate * db\n        \n        if i % 100 == 0:\n            costs.append(cost)\n        if print_cost and i % 500 == 0:\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    \n    params = {\"w\":w,\n              \"b\":b}\n    \n    grads = {\"dw\":dw,\n             \"db\":db}\n    \n    return params, grads, costs"
    },
    "executionTime": "2020-02-13T04:38:23.378Z"
   },
   {
    "cell": {
     "executionCount": 14,
     "executionEventId": "5615da06-a351-48d3-8ed9-7ca001d87143",
     "hasError": false,
     "id": "cf36ed9f-430f-4a0d-8b1b-1479bdff21a6",
     "outputs": [],
     "persistentId": "a7917915-adf9-4667-88b6-f01c4b8e2de3",
     "text": "import matplotlib.pyplot as plt"
    },
    "executionTime": "2020-02-13T04:38:27.076Z"
   },
   {
    "cell": {
     "executionCount": 15,
     "executionEventId": "7a4ab6d8-a84b-4fe4-a63e-f7c0579cf5dc",
     "hasError": false,
     "id": "ac82694b-2d93-49d8-844b-2dead04abb6b",
     "outputs": [],
     "persistentId": "afe4201d-351d-44c7-af57-927730e72d92",
     "text": "def predict(w, b, X):\n    '''\n    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n    \n    Arguments:\n    w -- weights\n    b -- bias \n    X -- data \n    \n    Returns:\n    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n    '''\n    m = X.shape[1]\n    Y_prediction = np.zeros((1,m))\n    w = w.reshape(X.shape[0],1)\n    \n    A = sigmoid(np.dot(w.T,X) + b)\n    \n    for j in range(A.shape[1]):\n        if A[0,j] <= 0.5:\n            Y_prediction[0,j] = 0\n        else:\n            Y_prediction[0,j] = 1\n    \n    assert(Y_prediction.shape == (1,m))\n    \n    return Y_prediction"
    },
    "executionTime": "2020-02-13T04:38:27.475Z"
   },
   {
    "cell": {
     "executionCount": 16,
     "executionEventId": "ee69bf35-9e2e-47c2-8d32-1327cfaf7094",
     "hasError": false,
     "id": "2f79e2e8-6ca3-47aa-8ec6-571d0f2874ca",
     "outputs": [],
     "persistentId": "736561b1-27f2-4958-a474-6d34622f80bb",
     "text": "def model(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate,print_cost=True):\n    \"\"\"\n    Build the logistic regression model by calling all the functions you have implemented.\n    Arguments:\n    X_train - training set\n    Y_train - training label\n    X_test - test set\n    Y_test - test label\n    num_iteration - hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations è¿™é‡Œæˆ‘æƒ³æ”¹æˆ500ï¼Œprintå¤ªå¤šäº†\n    \n    Returns:\n    d - dictionary should contain following information w,b,training_accuracy, test_accuracy,cost\n    eg: d = {\"w\":w,\n             \"b\":b,\n             \"training_accuracy\": traing_accuracy,\n             \"test_accuracy\":test_accuracy,\n             \"cost\":cost}\n    \"\"\"\n    # æ•°æ®ä¿®æ”¹ä¸ºä¸€ç»´\n    x_train = X_train.reshape(-1,X_train.shape[0])\n    y_train = Y_train.reshape(-1,Y_train.shape[0])\n    x_test = X_test.reshape(-1,X_test.shape[0])\n    y_test = Y_test.reshape(-1,Y_test.shape[0])\n    \n    # åˆå§‹åŒ–å‚æ•°\n    w,b = initialize_parameters(x_train.shape[0])\n    \n    # params:æ›´æ–°åçš„ç½‘ç»œå‚æ•°\n    # grads:æœ€æ–°çš„æ¢¯åº¦(ä¸‹é™æŸå¤±)\n    # costs:æ¯æ¬¡æ›´æ–°çš„æŸå¤±åˆ—è¡¨\n    params, grads, costs = optimize(w,b,x_train,y_train,num_iterations,learning_rate,print_cost)\n    \n    # è·å–è®­ç»ƒçš„å‚æ•°\n    w = params[\"w\"]\n    b = params[\"b\"]\n    \n    # é¢„æµ‹ç»“æœ\n    y_prediction_train = predict(w,b,x_train)\n    y_prediction_test = predict(w,b,x_test)\n\n    \n    # æ‰“å°å‡†ç¡®ç‡\n#     print(\"training setâ€˜s accuracyï¼š{}\".format(np.sum(y_prediction_train == y_train)/ y_train.shape[1]))\n#     print(\"testing setâ€™s accuracyï¼š{}\".format(np.sum(y_prediction_test == y_test)/ y_test.shape[1]))\n    \n    # æ‰“å°å‡†ç¡®ç‡\n#     plt.plot(range(len(costs),costs))\n    print(\"è®­ç»ƒé›†å‡†ç¡®ç‡: {} \".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"æµ‹è¯•é›†å‡†ç¡®ç‡: {} \".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \n    return costs\n    \n    \n    \n"
    },
    "executionTime": "2020-02-13T04:38:33.846Z"
   },
   {
    "cell": {
     "executionCount": 17,
     "executionEventId": "22a70d0c-1de2-413b-9621-dbd9ddfd26fc",
     "hasError": false,
     "id": "895a598f-d1cc-4ba8-abd0-a0a7278e03b2",
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "Cost after iteration 0: 5.082180\nCost after iteration 500: 3.821812\nCost after iteration 1000: 0.697330\nCost after iteration 1500: 0.664474\nCost after iteration 2000: 0.664473\nCost after iteration 2500: 0.664473\nCost after iteration 3000: 0.664473\nCost after iteration 3500: 0.664473\nCost after iteration 4000: 0.664472\nCost after iteration 4500: 0.664472\nCost after iteration 5000: 0.664472\nCost after iteration 5500: 0.664471\nCost after iteration 6000: 0.664471\nCost after iteration 6500: 0.664471\nCost after iteration 7000: 0.664471\nCost after iteration 7500: 0.664470\nCost after iteration 8000: 0.664470\nCost after iteration 8500: 0.664470\nCost after iteration 9000: 0.664470\nCost after iteration 9500: 0.664470\nCost after iteration 10000: 0.664469\nCost after iteration 10500: 0.664469\nCost after iteration 11000: 0.664469\nCost after iteration 11500: 0.664469\nCost after iteration 12000: 0.664468\nCost after iteration 12500: 0.664468\nCost after iteration 13000: 0.664468\nCost after iteration 13500: 0.664468\nCost after iteration 14000: 0.664468\nCost after iteration 14500: 0.664467\nè®­ç»ƒé›†å‡†ç¡®ç‡: 59.98608211551844 \næµ‹è¯•é›†å‡†ç¡®ç‡: 50.833333333333336 \nCost after iteration 0: 5.382118\nCost after iteration 500: 1.665617\nCost after iteration 1000: 1.481348\nCost after iteration 1500: 1.481260\nCost after iteration 2000: 1.481267\nCost after iteration 2500: 1.481274\nCost after iteration 3000: 1.481281\nCost after iteration 3500: 1.481288\nCost after iteration 4000: 1.481295\nCost after iteration 4500: 1.481302\nCost after iteration 5000: 1.481309\nCost after iteration 5500: 1.481316\nCost after iteration 6000: 1.481323\nCost after iteration 6500: 1.481330\nCost after iteration 7000: 1.481337\nCost after iteration 7500: 1.481344\nCost after iteration 8000: 1.481350\nCost after iteration 8500: 1.481357\nCost after iteration 9000: 1.481364\nCost after iteration 9500: 1.481371\nCost after iteration 10000: 1.481377\nCost after iteration 10500: 1.481384\nCost after iteration 11000: 1.481391\nCost after iteration 11500: 1.481397\nCost after iteration 12000: 1.481404\nCost after iteration 12500: 1.481410\nCost after iteration 13000: 1.481417\nCost after iteration 13500: 1.481423\nCost after iteration 14000: 1.481430\nCost after iteration 14500: 1.481436\nè®­ç»ƒé›†å‡†ç¡®ç‡: 51.28740431454419 \næµ‹è¯•é›†å‡†ç¡®ç‡: 51.94444444444444 \n"
      }
     ],
     "persistentId": "b175609a-a7d0-4bf8-84b6-23b93d651481",
     "text": "costs_1 = model(X_train, y_train, X_test, y_test, num_iterations=15000, learning_rate=0.005)\ncosts_2 = model(X_train, y_train, X_test, y_test, num_iterations=15000, learning_rate=0.010)"
    },
    "executionTime": "2020-02-13T04:38:38.093Z"
   },
   {
    "cell": {
     "executionCount": 20,
     "executionEventId": "7583149e-47e6-47ab-9281-bc9e67c0cd4d",
     "hasError": false,
     "id": "2fb5997d-68a6-48db-aa09-bb2bd244a7ba",
     "outputs": [
      {
       "data": {
        "text/plain": "(1437, 64)"
       },
       "execution_count": 20,
       "metadata": {},
       "output_type": "execute_result"
      }
     ],
     "persistentId": "d94005a1-ea48-4cbd-bc48-c1b44901a346",
     "text": "X_train.shape"
    },
    "executionTime": "2020-02-13T04:45:44.318Z"
   },
   {
    "cell": {
     "executionCount": 21,
     "executionEventId": "3035b82a-aa79-4532-b2b5-04343afcbf6c",
     "hasError": true,
     "id": "8eb1fff3-bdd2-4e86-a769-f53116d9e61c",
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "only integer scalar arrays can be converted to a scalar index",
       "output_type": "error",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
        "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-21-e1f3aca12190>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_set_x_flatten\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
       ]
      }
     ],
     "persistentId": "872be10f-7ad6-4899-90bc-0f5ecb6d2739",
     "text": "train_set_x_flatten  = X_train.reshape(X_train[0],-1).T"
    },
    "executionTime": "2020-02-13T04:47:28.158Z"
   },
   {
    "cell": {
     "executionCount": 22,
     "executionEventId": "2ba8c5af-5f6c-487e-bfa1-883ae7ad26b2",
     "hasError": false,
     "id": "8eb1fff3-bdd2-4e86-a769-f53116d9e61c",
     "outputs": [],
     "persistentId": "872be10f-7ad6-4899-90bc-0f5ecb6d2739",
     "text": "train_set_x_flatten  = X_train.reshape(X_train.shape[0],-1).T"
    },
    "executionTime": "2020-02-13T04:47:43.051Z"
   },
   {
    "cell": {
     "executionCount": 23,
     "executionEventId": "cb50db29-a885-477d-9072-181c5e180c2b",
     "hasError": false,
     "id": "d13e0317-9895-4b5b-a280-186c0a40656c",
     "outputs": [
      {
       "data": {
        "text/plain": "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       [ 0.,  1.,  2., ...,  0.,  0.,  2.],\n       [ 2.,  9., 14., ...,  0.,  6., 13.],\n       ...,\n       [16.,  1.,  2., ...,  2.,  3.,  7.],\n       [10.,  0.,  0., ...,  0.,  0.,  0.],\n       [ 3.,  0.,  0., ...,  0.,  0.,  0.]])"
       },
       "execution_count": 23,
       "metadata": {},
       "output_type": "execute_result"
      }
     ],
     "persistentId": "272804a9-2450-423e-a91a-be18bed5dcfe",
     "text": "train_set_x_flatten"
    },
    "executionTime": "2020-02-13T04:47:50.304Z"
   },
   {
    "cell": {
     "executionCount": 24,
     "executionEventId": "ff054d09-4f14-40ea-8cb8-2df87ad25479",
     "hasError": false,
     "id": "d13e0317-9895-4b5b-a280-186c0a40656c",
     "outputs": [
      {
       "data": {
        "text/plain": "(64, 1437)"
       },
       "execution_count": 24,
       "metadata": {},
       "output_type": "execute_result"
      }
     ],
     "persistentId": "272804a9-2450-423e-a91a-be18bed5dcfe",
     "text": "train_set_x_flatten.shape"
    },
    "executionTime": "2020-02-13T04:47:54.744Z"
   },
   {
    "cell": {
     "executionCount": 25,
     "executionEventId": "35c24573-1502-4f18-92ff-a7153479b323",
     "hasError": false,
     "id": "98ea38cb-5f5e-4bfe-acae-140849a8109b",
     "outputs": [],
     "persistentId": "0a802e33-097a-471d-9936-b407ebf51317",
     "text": "x_train = X_train.reshape(-1,X_train.shape[0])"
    },
    "executionTime": "2020-02-13T04:48:25.963Z"
   },
   {
    "cell": {
     "executionCount": 26,
     "executionEventId": "8270f437-94c8-4399-94bf-307c5017aab1",
     "hasError": false,
     "id": "98ea38cb-5f5e-4bfe-acae-140849a8109b",
     "outputs": [
      {
       "data": {
        "text/plain": "(1437, 64)"
       },
       "execution_count": 26,
       "metadata": {},
       "output_type": "execute_result"
      }
     ],
     "persistentId": "0a802e33-097a-471d-9936-b407ebf51317",
     "text": "x_train = X_train.reshape(-1,X_train.shape[0])\nX_train.shape"
    },
    "executionTime": "2020-02-13T04:48:35.960Z"
   },
   {
    "cell": {
     "executionCount": 27,
     "executionEventId": "9ff9e169-e54f-41a5-af1f-fdb8d392c169",
     "hasError": false,
     "id": "8eb1fff3-bdd2-4e86-a769-f53116d9e61c",
     "outputs": [],
     "persistentId": "872be10f-7ad6-4899-90bc-0f5ecb6d2739",
     "text": "train_set_x_flatten  = X_train.reshape(X_train.shape[0],-1).T"
    },
    "executionTime": "2020-02-13T04:50:27.569Z"
   },
   {
    "cell": {
     "executionCount": 28,
     "executionEventId": "90ab6e17-3dbd-40a2-992d-48707a345ef8",
     "hasError": false,
     "id": "d13e0317-9895-4b5b-a280-186c0a40656c",
     "outputs": [
      {
       "data": {
        "text/plain": "(64, 1437)"
       },
       "execution_count": 28,
       "metadata": {},
       "output_type": "execute_result"
      }
     ],
     "persistentId": "272804a9-2450-423e-a91a-be18bed5dcfe",
     "text": "train_set_x_flatten.shape"
    },
    "executionTime": "2020-02-13T04:50:28.108Z"
   },
   {
    "cell": {
     "executionCount": 29,
     "executionEventId": "fc0c5956-ce23-4c27-b2c0-c4115293d024",
     "hasError": false,
     "id": "2fb5997d-68a6-48db-aa09-bb2bd244a7ba",
     "outputs": [
      {
       "data": {
        "text/plain": "array([0., 0., 2., ..., 7., 0., 0.])"
       },
       "execution_count": 29,
       "metadata": {},
       "output_type": "execute_result"
      }
     ],
     "persistentId": "d94005a1-ea48-4cbd-bc48-c1b44901a346",
     "text": "X_train.flatten()"
    },
    "executionTime": "2020-02-13T04:51:40.197Z"
   },
   {
    "cell": {
     "executionCount": 30,
     "executionEventId": "cb236017-1b62-47b1-b5c2-5c69a3ccccd0",
     "hasError": false,
     "id": "79ab8c82-57c6-478c-85bf-e496ef1838c1",
     "outputs": [],
     "persistentId": "736561b1-27f2-4958-a474-6d34622f80bb",
     "text": "def model(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate,print_cost=True):\n    \"\"\"\n    Build the logistic regression model by calling all the functions you have implemented.\n    Arguments:\n    X_train - training set\n    Y_train - training label\n    X_test - test set\n    Y_test - test label\n    num_iteration - hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations è¿™é‡Œæˆ‘æƒ³æ”¹æˆ500ï¼Œprintå¤ªå¤šäº†\n    \n    Returns:\n    d - dictionary should contain following information w,b,training_accuracy, test_accuracy,cost\n    eg: d = {\"w\":w,\n             \"b\":b,\n             \"training_accuracy\": traing_accuracy,\n             \"test_accuracy\":test_accuracy,\n             \"cost\":cost}\n    \"\"\"\n    # æ•°æ®ä¿®æ”¹ä¸ºä¸€ç»´\n#     x_train = X_train.reshape(-1,X_train.shape[0])\n#     y_train = Y_train.reshape(-1,Y_train.shape[0])\n#     x_test = X_test.reshape(-1,X_test.shape[0])\n#     y_test = Y_test.reshape(-1,Y_test.shape[0])\n\n    x_train = X_train.flatten()\n    y_train = Y_train.flatten()\n    x_test = X_test.flatten()\n    y_test = Y_test.flatten()\n    \n    # åˆå§‹åŒ–å‚æ•°\n    w,b = initialize_parameters(x_train.shape[0])\n    \n    # params:æ›´æ–°åçš„ç½‘ç»œå‚æ•°\n    # grads:æœ€æ–°çš„æ¢¯åº¦(ä¸‹é™æŸå¤±)\n    # costs:æ¯æ¬¡æ›´æ–°çš„æŸå¤±åˆ—è¡¨\n    params, grads, costs = optimize(w,b,x_train,y_train,num_iterations,learning_rate,print_cost)\n    \n    # è·å–è®­ç»ƒçš„å‚æ•°\n    w = params[\"w\"]\n    b = params[\"b\"]\n    \n    # é¢„æµ‹ç»“æœ\n    y_prediction_train = predict(w,b,x_train)\n    y_prediction_test = predict(w,b,x_test)\n\n    \n    # æ‰“å°å‡†ç¡®ç‡\n#     print(\"training setâ€˜s accuracyï¼š{}\".format(np.sum(y_prediction_train == y_train)/ y_train.shape[1]))\n#     print(\"testing setâ€™s accuracyï¼š{}\".format(np.sum(y_prediction_test == y_test)/ y_test.shape[1]))\n    \n    # æ‰“å°å‡†ç¡®ç‡\n#     plt.plot(range(len(costs),costs))\n    print(\"è®­ç»ƒé›†å‡†ç¡®ç‡: {} \".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"æµ‹è¯•é›†å‡†ç¡®ç‡: {} \".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \n    return costs\n    \n    \n    \n"
    },
    "executionTime": "2020-02-13T04:52:50.055Z"
   },
   {
    "cell": {
     "executionCount": 31,
     "executionEventId": "d74ef645-31d5-4f00-a79d-8a5e9c01a0f8",
     "hasError": true,
     "id": "7aeda777-879a-4e9b-861d-d7cbb26fcad7",
     "outputs": [
      {
       "ename": "IndexError",
       "evalue": "tuple index out of range",
       "output_type": "error",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
        "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-31-50867677b0ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcosts_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcosts_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.010\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-30-4bd0dd7851b7>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate, print_cost)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# grads:æœ€æ–°çš„æ¢¯åº¦(ä¸‹é™æŸå¤±)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# costs:æ¯æ¬¡æ›´æ–°çš„æŸå¤±åˆ—è¡¨\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprint_cost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# è·å–è®­ç»ƒçš„å‚æ•°\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-13-24b5fdd3d858>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(w, b, X, Y, num_iterations, learning_rate, print_cost)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mdw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dw'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-12-3926fa1c6dda>\u001b[0m in \u001b[0;36mpropagate\u001b[0;34m(w, b, X, Y)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mY\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mground\u001b[0m \u001b[0mtruth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     '''\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mA\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#æŸå¤±å‡½æ•°è¿™é‡Œæœ€å¥½åŠ ä¸€ä¸ªå°æ•°ï¼Œé˜²æ­¢é™¤0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
       ]
      }
     ],
     "persistentId": "b175609a-a7d0-4bf8-84b6-23b93d651481",
     "text": "costs_1 = model(X_train, y_train, X_test, y_test, num_iterations=15000, learning_rate=0.005)\ncosts_2 = model(X_train, y_train, X_test, y_test, num_iterations=15000, learning_rate=0.010)"
    },
    "executionTime": "2020-02-13T04:52:50.522Z"
   },
   {
    "cell": {
     "executionCount": 32,
     "executionEventId": "2a33d897-b529-497b-b1fc-1baae8cab30a",
     "hasError": false,
     "id": "fdebb903-7522-4d34-8147-e206901b66c1",
     "outputs": [],
     "persistentId": "afe4201d-351d-44c7-af57-927730e72d92",
     "text": "def predict(w, b, X):\n    '''\n    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n    \n    Arguments:\n    w -- weights\n    b -- bias \n    X -- data \n    \n    Returns:\n    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n    '''\n    m = X.shape[0]\n    Y_prediction = np.zeros((1,m))\n    w = w.reshape(X.shape[0],1)\n    \n    A = sigmoid(np.dot(w.T,X) + b)\n    \n    for j in range(A.shape[1]):\n        if A[0,j] <= 0.5:\n            Y_prediction[0,j] = 0\n        else:\n            Y_prediction[0,j] = 1\n    \n    assert(Y_prediction.shape == (1,m))\n    \n    return Y_prediction"
    },
    "executionTime": "2020-02-13T04:54:56.628Z"
   },
   {
    "cell": {
     "executionCount": 33,
     "executionEventId": "af1250aa-fcbe-4cbc-a589-d8d36c4f4aa7",
     "hasError": false,
     "id": "79ab8c82-57c6-478c-85bf-e496ef1838c1",
     "outputs": [],
     "persistentId": "736561b1-27f2-4958-a474-6d34622f80bb",
     "text": "def model(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate,print_cost=True):\n    \"\"\"\n    Build the logistic regression model by calling all the functions you have implemented.\n    Arguments:\n    X_train - training set\n    Y_train - training label\n    X_test - test set\n    Y_test - test label\n    num_iteration - hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations è¿™é‡Œæˆ‘æƒ³æ”¹æˆ500ï¼Œprintå¤ªå¤šäº†\n    \n    Returns:\n    d - dictionary should contain following information w,b,training_accuracy, test_accuracy,cost\n    eg: d = {\"w\":w,\n             \"b\":b,\n             \"training_accuracy\": traing_accuracy,\n             \"test_accuracy\":test_accuracy,\n             \"cost\":cost}\n    \"\"\"\n    # æ•°æ®ä¿®æ”¹ä¸ºä¸€ç»´\n#     x_train = X_train.reshape(-1,X_train.shape[0])\n#     y_train = Y_train.reshape(-1,Y_train.shape[0])\n#     x_test = X_test.reshape(-1,X_test.shape[0])\n#     y_test = Y_test.reshape(-1,Y_test.shape[0])\n\n    x_train = X_train.flatten()\n    y_train = Y_train.flatten()\n    x_test = X_test.flatten()\n    y_test = Y_test.flatten()\n    \n    # åˆå§‹åŒ–å‚æ•°\n    w,b = initialize_parameters(x_train.shape[0])\n    \n    # params:æ›´æ–°åçš„ç½‘ç»œå‚æ•°\n    # grads:æœ€æ–°çš„æ¢¯åº¦(ä¸‹é™æŸå¤±)\n    # costs:æ¯æ¬¡æ›´æ–°çš„æŸå¤±åˆ—è¡¨\n    params, grads, costs = optimize(w,b,x_train,y_train,num_iterations,learning_rate,print_cost)\n    \n    # è·å–è®­ç»ƒçš„å‚æ•°\n    w = params[\"w\"]\n    b = params[\"b\"]\n    \n    # é¢„æµ‹ç»“æœ\n    y_prediction_train = predict(w,b,x_train)\n    y_prediction_test = predict(w,b,x_test)\n\n    \n    # æ‰“å°å‡†ç¡®ç‡\n#     print(\"training setâ€˜s accuracyï¼š{}\".format(np.sum(y_prediction_train == y_train)/ y_train.shape[1]))\n#     print(\"testing setâ€™s accuracyï¼š{}\".format(np.sum(y_prediction_test == y_test)/ y_test.shape[1]))\n    \n    # æ‰“å°å‡†ç¡®ç‡\n#     plt.plot(range(len(costs),costs))\n    print(\"è®­ç»ƒé›†å‡†ç¡®ç‡: {} \".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"æµ‹è¯•é›†å‡†ç¡®ç‡: {} \".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \n    return costs\n    \n    \n    \n"
    },
    "executionTime": "2020-02-13T04:54:59.575Z"
   },
   {
    "cell": {
     "executionCount": 34,
     "executionEventId": "93dea520-f80c-4b68-917e-ad6fe4be9ec2",
     "hasError": true,
     "id": "7aeda777-879a-4e9b-861d-d7cbb26fcad7",
     "outputs": [
      {
       "ename": "IndexError",
       "evalue": "tuple index out of range",
       "output_type": "error",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
        "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-34-50867677b0ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcosts_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcosts_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.010\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-33-4bd0dd7851b7>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate, print_cost)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# grads:æœ€æ–°çš„æ¢¯åº¦(ä¸‹é™æŸå¤±)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# costs:æ¯æ¬¡æ›´æ–°çš„æŸå¤±åˆ—è¡¨\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprint_cost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# è·å–è®­ç»ƒçš„å‚æ•°\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-13-24b5fdd3d858>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(w, b, X, Y, num_iterations, learning_rate, print_cost)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mdw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dw'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-12-3926fa1c6dda>\u001b[0m in \u001b[0;36mpropagate\u001b[0;34m(w, b, X, Y)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mY\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mground\u001b[0m \u001b[0mtruth\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     '''\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mA\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#æŸå¤±å‡½æ•°è¿™é‡Œæœ€å¥½åŠ ä¸€ä¸ªå°æ•°ï¼Œé˜²æ­¢é™¤0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
       ]
      }
     ],
     "persistentId": "b175609a-a7d0-4bf8-84b6-23b93d651481",
     "text": "costs_1 = model(X_train, y_train, X_test, y_test, num_iterations=15000, learning_rate=0.005)\ncosts_2 = model(X_train, y_train, X_test, y_test, num_iterations=15000, learning_rate=0.010)"
    },
    "executionTime": "2020-02-13T04:55:00.627Z"
   },
   {
    "cell": {
     "executionCount": 35,
     "executionEventId": "510cd95c-147a-49d0-9087-b414a6cd8fc9",
     "hasError": false,
     "id": "06e39dc7-7a02-44c9-a401-18890b35b961",
     "outputs": [],
     "persistentId": "d2518a89-1eeb-44fb-95a8-4eab59019f67",
     "text": "def propagate(w,b,X,Y):\n    '''\n    Implement the cost function and its gradient for the propagation\n    \n    Arguments:\n    w - weights\n    b - bias\n    X - data\n    Y - ground truth\n    '''\n    m = X.shape[0]\n    A = sigmoid(np.dot(w.T,X) + b)\n    cost = -1/m * np.sum(Y * np.log(A + 1e-5) + (1-Y) * np.log(1-A + 1e-5)) #æŸå¤±å‡½æ•°è¿™é‡Œæœ€å¥½åŠ ä¸€ä¸ªå°æ•°ï¼Œé˜²æ­¢é™¤0\n#     print(cost)\n    \n    dw = 1/m * np.dot(X,(A-Y).T)\n    db = 1/m * np.sum(A-Y)\n    \n    assert(dw.shape == w.shape)\n    assert(db.dtype == float)\n    cost = np.squeeze(cost)\n    assert(cost.shape == ())\n    \n    grads = {'dw':dw,\n             'db':db}\n    return grads, cost"
    },
    "executionTime": "2020-02-13T04:55:29.409Z"
   },
   {
    "cell": {
     "executionCount": 36,
     "executionEventId": "4172ab83-97ef-4ec8-8272-e67a930c9636",
     "hasError": false,
     "id": "79ab8c82-57c6-478c-85bf-e496ef1838c1",
     "outputs": [],
     "persistentId": "736561b1-27f2-4958-a474-6d34622f80bb",
     "text": "def model(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate,print_cost=True):\n    \"\"\"\n    Build the logistic regression model by calling all the functions you have implemented.\n    Arguments:\n    X_train - training set\n    Y_train - training label\n    X_test - test set\n    Y_test - test label\n    num_iteration - hyperparameter representing the number of iterations to optimize the parameters\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    print_cost -- Set to true to print the cost every 100 iterations è¿™é‡Œæˆ‘æƒ³æ”¹æˆ500ï¼Œprintå¤ªå¤šäº†\n    \n    Returns:\n    d - dictionary should contain following information w,b,training_accuracy, test_accuracy,cost\n    eg: d = {\"w\":w,\n             \"b\":b,\n             \"training_accuracy\": traing_accuracy,\n             \"test_accuracy\":test_accuracy,\n             \"cost\":cost}\n    \"\"\"\n    # æ•°æ®ä¿®æ”¹ä¸ºä¸€ç»´\n#     x_train = X_train.reshape(-1,X_train.shape[0])\n#     y_train = Y_train.reshape(-1,Y_train.shape[0])\n#     x_test = X_test.reshape(-1,X_test.shape[0])\n#     y_test = Y_test.reshape(-1,Y_test.shape[0])\n\n    x_train = X_train.flatten()\n    y_train = Y_train.flatten()\n    x_test = X_test.flatten()\n    y_test = Y_test.flatten()\n    \n    # åˆå§‹åŒ–å‚æ•°\n    w,b = initialize_parameters(x_train.shape[0])\n    \n    # params:æ›´æ–°åçš„ç½‘ç»œå‚æ•°\n    # grads:æœ€æ–°çš„æ¢¯åº¦(ä¸‹é™æŸå¤±)\n    # costs:æ¯æ¬¡æ›´æ–°çš„æŸå¤±åˆ—è¡¨\n    params, grads, costs = optimize(w,b,x_train,y_train,num_iterations,learning_rate,print_cost)\n    \n    # è·å–è®­ç»ƒçš„å‚æ•°\n    w = params[\"w\"]\n    b = params[\"b\"]\n    \n    # é¢„æµ‹ç»“æœ\n    y_prediction_train = predict(w,b,x_train)\n    y_prediction_test = predict(w,b,x_test)\n\n    \n    # æ‰“å°å‡†ç¡®ç‡\n#     print(\"training setâ€˜s accuracyï¼š{}\".format(np.sum(y_prediction_train == y_train)/ y_train.shape[1]))\n#     print(\"testing setâ€™s accuracyï¼š{}\".format(np.sum(y_prediction_test == y_test)/ y_test.shape[1]))\n    \n    # æ‰“å°å‡†ç¡®ç‡\n#     plt.plot(range(len(costs),costs))\n    print(\"è®­ç»ƒé›†å‡†ç¡®ç‡: {} \".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"æµ‹è¯•é›†å‡†ç¡®ç‡: {} \".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \n    return costs\n    \n    \n    \n"
    },
    "executionTime": "2020-02-13T04:55:33.645Z"
   },
   {
    "cell": {
     "executionCount": 37,
     "executionEventId": "9c1b6fb5-ff03-4424-a32b-09752e09a4f5",
     "hasError": true,
     "id": "7aeda777-879a-4e9b-861d-d7cbb26fcad7",
     "outputs": [
      {
       "ename": "ValueError",
       "evalue": "shapes (91968,) and (1437,) not aligned: 91968 (dim 0) != 1437 (dim 0)",
       "output_type": "error",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
        "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-37-50867677b0ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcosts_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.005\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcosts_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.010\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-36-4bd0dd7851b7>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate, print_cost)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;31m# grads:æœ€æ–°çš„æ¢¯åº¦(ä¸‹é™æŸå¤±)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# costs:æ¯æ¬¡æ›´æ–°çš„æŸå¤±åˆ—è¡¨\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcosts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprint_cost\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;31m# è·å–è®­ç»ƒçš„å‚æ•°\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-13-24b5fdd3d858>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(w, b, X, Y, num_iterations, learning_rate, print_cost)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpropagate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mdw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dw'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<ipython-input-35-a5d61283714c>\u001b[0m in \u001b[0;36mpropagate\u001b[0;34m(w, b, X, Y)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#     print(cost)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mdw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
        "\u001b[0;31mValueError\u001b[0m: shapes (91968,) and (1437,) not aligned: 91968 (dim 0) != 1437 (dim 0)"
       ]
      }
     ],
     "persistentId": "b175609a-a7d0-4bf8-84b6-23b93d651481",
     "text": "costs_1 = model(X_train, y_train, X_test, y_test, num_iterations=15000, learning_rate=0.005)\ncosts_2 = model(X_train, y_train, X_test, y_test, num_iterations=15000, learning_rate=0.010)"
    },
    "executionTime": "2020-02-13T04:55:34.605Z"
   },
   {
    "cell": {
     "executionCount": 38,
     "executionEventId": "3f3a3e9a-4850-438f-825b-49ec9c3bc5f4",
     "hasError": false,
     "id": "0bffd812-69cf-4e83-b241-981dc4c67519",
     "outputs": [],
     "persistentId": "a05da7a5-0667-4106-8b2a-6a8ca00e9299",
     "text": "train_set_x_flatten = X_train.reshape(X_train.shape[0],-1).T"
    },
    "executionTime": "2020-02-13T04:58:39.282Z"
   },
   {
    "cell": {
     "executionCount": 39,
     "executionEventId": "03f91e2e-4c99-4972-a687-6903397924c7",
     "hasError": false,
     "id": "70820e80-d63b-4940-9728-98856aa7b2a3",
     "outputs": [
      {
       "data": {
        "text/plain": "(64, 1437)"
       },
       "execution_count": 39,
       "metadata": {},
       "output_type": "execute_result"
      }
     ],
     "persistentId": "2d7bf83f-32d8-4b4b-a2bf-6a19f71e777c",
     "text": "train_set_x_flatten.shape"
    },
    "executionTime": "2020-02-13T04:58:55.404Z"
   },
   {
    "cell": {
     "executionCount": 40,
     "executionEventId": "8ad1fa73-1b00-487b-8dc8-0cbf67b2a065",
     "hasError": false,
     "id": "70820e80-d63b-4940-9728-98856aa7b2a3",
     "outputs": [
      {
       "name": "stdout",
       "output_type": "stream",
       "text": "(64, 1437)\n"
      }
     ],
     "persistentId": "2d7bf83f-32d8-4b4b-a2bf-6a19f71e777c",
     "text": "print(train_set_x_flatten.shape)"
    },
    "executionTime": "2020-02-13T04:59:05.681Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "uuid": "966608e7-8d44-46d3-8b92-669d92eb9c3c"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
