{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.复习上课内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 回答一下理论题目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. What is independent assumption in Naive bayes ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "贝叶斯公式\n",
    "\n",
    "P(c|x) = P(x,c)/P(x),等同于P(c|x) = P(c)P(x|c)/P(x),将求后验概率P(c|x)的问题转化为求类先验概率P(c),和求类条件概率P(x|c)。\n",
    "\n",
    "根据大数定律，当数据集包含充足的独立同分布样本时，P(c)可以通过各类样本出现的频率进行估计。\n",
    "\n",
    "但对于类条件概率来说，它涉及关于x所有属性的联合概率，但实际情况中，训练样本数往往低于样本空间的所有取值数，使得以样本频率估计P(c|x)不可行。因此，为简化求解。科学家假设所有的属性值互相独立\n",
    "\n",
    "基于属性独立假设\n",
    "\n",
    "P(c|x) = P(c)P(x|c)/P(x) = P(c)/p(x) ∏(i=1,d) P(xi|c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What is MAP(maximum a posterior) and ML(maximum likelihood) ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML:\n",
    "假设一个袋子装有白球与红球，比例未知，现在抽取10次（每次抽完都放回，保证事件独立性），假设抽到了7次白球和3次红球，在此数据样本条件下，可以采用最大似然估计法求解袋子中白球的比例（最大似然估计是一种“模型已定，参数未知”的方法）。\n",
    "\n",
    "f(x1,x2|c) = f(x1|c)f(x2|c),其中c是未知的，因此我们定义似然L为\n",
    "\n",
    "L(c|x1,x2) = f(x1,x2|c) = ∏(i=1,2) f(xi|c)\n",
    "\n",
    "两边取ln，将右边等式的乘号转变为加号，方便求导。\n",
    "\n",
    "lnL(c|x1,x2) = ln∑(i=1,2) f(xi|c) = ∑ln f(xi|c)\n",
    "\n",
    "l（估计） = 1/2 lnL(c|x1,x2) \n",
    "\n",
    "求最大似然，即寻找一个合适的c，使得上式平均对数似然的值为最大"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAP:\n",
    "\n",
    "根据贝叶斯理论，P(c|x) = P(c)P(x|c)/p(x),要使得后验概率P(c|x)为最大\n",
    "\n",
    "即c（估计值）MAP = argmax P(c|x) = argmax P(c)P(x|c)\n",
    "\n",
    "为了求得参数c，对P(x|c)求梯度，并使其等于0：\n",
    "\n",
    "P(c|x)/∂c = P(c)P(x|c)/∂c = 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. What is support vector in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KKT条件下推导：\n",
    "1.ai >= 0\n",
    "2. yi(wixi + b) - 1 >= 0\n",
    "3. ai(yi(wixi + b) - 1) = 0\n",
    "\n",
    "对于任意训练样本，总有ai=0或者yi（w*xi + b）=1。若ai=0，则样本不会在最后求解模型参数的式子中出现。若ai>0,则必有yi（w*xi + b）=1，所对应的样本位于最大间隔边界上，是一个支持向量。这显示SVM训练完成后，大部分训练样本都不需要保留，最终模型仅与支持向量有关。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. What is the intuition behind SVM ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不是很理解这句话。SVM的直觉应该是把二分类问题放到几何上理解。\n",
    "\n",
    "把二分类问题转化为在二维空间中，找到最佳分隔线。\n",
    "\n",
    "这个分隔线怎么找才最好，最直觉的思想就是\"不偏不倚\"，与两类样本点的边界保持相等的最远的距离。\n",
    "\n",
    "我们希望找到这样一个决策边界，这个边界距离两类数据点最远。更进一步的，是距离两类数据点的边界最远，所以定义最接近边界的数据点定义为支持向量。最后，我们的目标变为找到这样一个直线（多维叫超平面），它与支持向量有最大的间隔。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Shortly describ what 'random' means in random forest ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random forest使用了集成学习中的Bagging思想。Bagging算法具体过程如下。\n",
    "\n",
    "1.从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping方法抽取n个训练样本（放回可重复抽样）。共进行k轮抽取。\n",
    "\n",
    "2.每一次使用一个训练集训练得到一个模型，k个训练集得到k个模型。\n",
    "\n",
    "3对分类问题：将上步得到的k个模型用投票方式得到分类结果。\n",
    "\n",
    "random体现在抽取训练集时采用放回可重复抽样。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. What cariterion does XGBoost use to find the best split point in a tree ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGboost中，我们构建新的树，希望损失函数尽可能小。\n",
    "\n",
    "即，min∑（k=1，n）（L(yk,yk(估计)+Ft+1(Xk)） ,使用二阶泰勒展开可以得到\n",
    "\n",
    "约为min∑（k=1，n）(GkFt+1(Xk) + 1/2Hk Ft+1^2(Xk)) + Ω（Ft+1）\n",
    "\n",
    "而树模型fi是由多个叶子节点w1，w2，......,wTi组成\n",
    "\n",
    "min泰勒展开近似可到w* = -∑(i∈Ij)Gi/∑(i∈Ij)Hi + λ\n",
    "\n",
    "该点就是最优决策点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Practial part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Problem description: In this part you are going to build a classifier to detect if a piece of news is published by the Xinhua news agency (新华社）."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据获取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import jieba\n",
    "import collections\n",
    "import thulac\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"C:\\\\NLP学习备用\\\\新华社数据.csv\"\n",
    "data = pd.read_csv(filename,encoding =\"gb18030\",engine = 'python')\n",
    "data = data.dropna(subset=[\"content\"]) #删除内容为空的行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = data[\"content\"].tolist()\n",
    "source = data[\"source\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = [name if isinstance(name,str) else \"unknow\" for name in source] #将没有来源的信息标记为unKnown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## content清洗切词，source分类标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_labels = [1 if name.strip()== \"新华社\" else 0 for name in source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 8393, 1: 78661})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collections.Counter(article_labels)\n",
    "#可以知道正负样本很不平衡，在训练模型时注意调整参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_stopwords():\n",
    "    stopwords = []\n",
    "    with open(r\"c:\\NLP学习备用\\停用词表.txt\",encoding=\"utf-8\") as f:\n",
    "        line_str = f.readline()\n",
    "        while line_str != \"\":\n",
    "            line_str = line_str.strip()\n",
    "            stopwords.append(line_str)\n",
    "            line_str = f.readline()\n",
    "    return set(stopwords)\n",
    "\n",
    "def text_cut(text_list):\n",
    "    stopwords = get_stopwords()\n",
    "    corpus = []\n",
    "    count = 0\n",
    "    str_all = \"\"\n",
    "    for number,line in enumerate(text_list):\n",
    "        sentence_sign = str(line) + \"+sen\" #用来记住每段的末尾\n",
    "        str_all += sentence_sign\n",
    "        \n",
    "        if number % 1000 == 0 and number != 0:          \n",
    "            thu1 = thulac.thulac(seg_only=True,filt=True) \n",
    "            words = thu1.cut(str_all, text=True)\n",
    "            sentences = words.split(r\"+ sen\")\n",
    "            print(number)\n",
    "            str_all = \"\"\n",
    "            corpus = corpus + sentences\n",
    "            \n",
    "        if number == len(text_list) - 1:           \n",
    "            thu1 = thulac.thulac(seg_only=True,filt=True) \n",
    "            words = thu1.cut(str_all, text=True)\n",
    "            sentences = words.split(r\"+ sen\")\n",
    "            print(sentences)\n",
    "            str_all = \"\"\n",
    "            corpus = corpus + sentences\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = text_cut(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_clean(corpus):\n",
    "    clean_corpus = []\n",
    "    stopwords = get_stopwords()\n",
    "    for count,cor in enumerate(corpus):\n",
    "        string = \"\"\n",
    "        for word in cor:\n",
    "            if word not in stopwords:\n",
    "                attend_word = word + \" \"\n",
    "                string = string + attend_word\n",
    "        clean_corpus.append(string)\n",
    "        print(string)\n",
    "        if count % 1000 == 0 and count != 0: \n",
    "            print(count)\n",
    "    return clean_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_clean = corpus_clean(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('c:\\NLP学习备用\\corpus_list','wb') as f:\n",
    "    pickle.dump(corpus,f)\n",
    "with open('c:\\NLP学习备用\\corpus_clean_list','wb') as f:\n",
    "    pickle.dump(corpus_clean,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'c:\\NLP学习备用\\corpus_clean_list','rb') as f:\n",
    "    corpus_clean = pickle.load(f)\n",
    "with open(r'c:\\NLP学习备用\\corpus_list','rb') as f:\n",
    "    corpus = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_label = article_labels[:len(corpus_clean)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用时测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def clock(func):\n",
    "    def clocked(*args, **kwargs):\n",
    "        t0 = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        elapsed = time.time() - t0\n",
    "        name = func.__name__\n",
    "        print('函数 {} 运行时间:{:.2f}s'.format(name,elapsed))\n",
    "        return result\n",
    "    return clocked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def get_performance(clf, x_, y_):\n",
    "    y_hat = clf.predict(x_)\n",
    "    \n",
    "    print('f1_score is: {}'.format(f1_score(y_, y_hat)))\n",
    "    print('accuracy is: {}'.format(accuracy_score(y_, y_hat)))\n",
    "    print('percision is: {}'.format(precision_score(y_, y_hat)))\n",
    "    print('recall is: {}'.format(recall_score(y_, y_hat)))\n",
    "    print('roc_auc is: {}'.format(roc_auc_score(y_, y_hat)))\n",
    "\n",
    "\n",
    "    return [format(f1_score(y_, y_hat),'.4f'),format(accuracy_score(y_, y_hat),'.4f'),\n",
    "            format(f1_score(y_, y_hat),'.4f'),format(recall_score(y_, y_hat),'.4f')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据不均衡处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8393\n",
      "78634\n",
      "16786\n",
      "16786\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "corpus_0 = [a for a,i in zip(corpus_clean, corpus_label) if i==0]\n",
    "corpus_1 = [a for a,i in zip(corpus_clean, corpus_label) if i==1]\n",
    "\n",
    "print(len(corpus_0))\n",
    "print(len(corpus_1))\n",
    "\n",
    "X = corpus_0 + random.sample(corpus_1, len(corpus_0))\n",
    "y = [0]*len(corpus_0) + [1]*len(corpus_0)\n",
    "\n",
    "#后来想了想，这样简单过采样，是会造成数据泄露的，nlp数据增强要研究一下\n",
    "\n",
    "print(len(X))\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 文本向量化，划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "vectorized = TfidfVectorizer(max_features= 5000) #设置文本单词个数最大值\n",
    "X = vectorized.fit_transform(X)\n",
    "X = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#选取30%左右数据作为测试数据\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\junjiexie\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN(X_train, X_test, y_train, y_test,metric='minkowski'):\n",
    "    nbrs = KNeighborsClassifier(n_neighbors=5, algorithm='brute',metric =metric)\n",
    "    nbrs.fit(X_train, y_train)\n",
    "    return get_performance(nbrs,X_test,y_test)\n",
    "\n",
    "def random_forest(X_train, X_test, y_train, y_test,n_estimators=10,criterion='gini',max_depth=None):\n",
    "    clf = RandomForestClassifier(n_estimators=n_estimators,criterion=criterion,max_depth=max_depth)\n",
    "    clf.fit(X_train, y_train)\n",
    "    return get_performance(clf,X_test,y_test)\n",
    "\n",
    "def xgboost(X_train, X_test, y_train, y_test):\n",
    "    clf = xgb.XGBRFClassifier(class_weight=\"balanced\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    return get_performance(clf,X_test,y_test)\n",
    "def lightgbm(X_train, X_test, y_train, y_test):\n",
    "    clf = lgb.LGBMClassifier(class_weight=\"balanced\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    return get_performance(clf,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score is: 0.7450495049504949\n",
      "accuracy is: 0.7545671167593329\n",
      "percision is: 0.7845351867940921\n",
      "recall is: 0.7093479968578161\n",
      "roc_auc is: 0.7550756048546109\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['0.7450', '0.7546', '0.7450', '0.7093']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNN(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score is: 0.9226644669003918\n",
      "accuracy is: 0.9255361397934869\n",
      "percision is: 0.9713417281806339\n",
      "recall is: 0.8786331500392773\n",
      "roc_auc is: 0.9260635629714458\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['0.9227', '0.9255', '0.9227', '0.8786']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score is: 0.9672447013487476\n",
      "accuracy is: 0.9662430500397141\n",
      "percision is: 0.9493192133131618\n",
      "recall is: 0.98586017282011\n",
      "roc_auc is: 0.9660224558879666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\junjiexie\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['0.9672', '0.9662', '0.9672', '0.9859']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgboost(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1_score is: 0.9747081712062257\n",
      "accuracy is: 0.9741858617950755\n",
      "percision is: 0.9656900539707016\n",
      "recall is: 0.9838963079340142\n",
      "roc_auc is: 0.974076668023232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\junjiexie\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['0.9747', '0.9742', '0.9747', '0.9839']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lightgbm(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总的来说，集成学习梯度提升树模型表现是最好的。lightgbm据说效果比xgboost好，好像也的确如此。但实战中发现，lightgbm好像比较容易过拟合，catboost没有试过。但在数据科学竞赛中，三大提升树表现确实是机器学习模型中最好的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hints:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 1. Firstly, you have to come up with a way to represent the news. (Vectorize the sentence, you can find different ways to do so online)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 2. Secondly,  pick a machine learning algorithm that you think is suitable for this task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations! You have completed all assignments in this week. The question below is optional. If you still have time, why don't try it out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try differnt machine learning algorithms with different combinations of parameters in the practical part, and compare their performances (Better use some visualization techiniques)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
