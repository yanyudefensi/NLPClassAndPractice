{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. 请写一下TF-IDF的计算公式"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "TF（Term Frequency，词频）表示一个给定词语t在一篇给定文档d中出现的频率。TF越高，则词语t对文档d来说越重要，\n",
    "TF越低，则词语t对文档d来说越不重要。\n",
    "\n",
    "IDF（Inverse Document Frequency，逆向文件频率）的主要思想是：如果包含词语t的文档越少，则IDF越大，\n",
    "说明词语t在整个文档集层面上具有很好的类别区分能力。\n",
    "\n",
    "对于在某一文档 dj 里的词语 ti 来说，ti 的词频可表示为：\n",
    "\n",
    "TF: tf(i,j) = n(i,j)/∑k n(k,j)\n",
    "\n",
    "其中 ni,j 是词语 ti 在文档 dj 中的出现次数，分母则是在文件 dj 中所有词语的出现次数之和。\n",
    "\n",
    "某一特定词语的IDF，可以由总文件数除以包含该词语的文件数，再将得到的商取对数得到：\n",
    "\n",
    "IDF:  idf(i) = log(10) |D| / | {j: t(i) ∈ d(j)} |\n",
    "\n",
    "其中 |D| 是语料库中所有文档总数，分母是包含词语 ti 的所有文档数。\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. LDA算法的基本假设是什么？\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "LDA假设文档主题的先验分布是Dirichlet分布,假设主题中词的先验分布是Dirichlet分布(数学推导繁琐，自行学习，不做展示）\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. 在TextRank算法中构建图的权重是如何得到的？\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "考察jieba源码后，发现其权重是将共现词典中的词i，词j作为一条边起始点和终止点，共现的次数作为边的权重。\n",
    "共现词典的构建则通过滑动窗口，如（1，2，3） ——> (1,2),(1,3),(2,3),该例子滑动窗口为2。\n",
    "\n",
    "当然，也可以结合word2vec词向量，通过比较词i和词j的相似度，作为边的权重。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. 什么是命名实体识别？ 有什么应用场景？\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "命名实体识别（Named Entity Recognition，简称NER），又称作“专名识别”，是指识别文本中具有特定意义的实体，主要包括人名、地名、机构名、专有名词等。简单的讲，就是识别自然文本中的实体指称的边界和类别。\n",
    "\n",
    "命名实体识别是信息提取、问答系统、句法分析、机器翻译、面向Semantic Web的元数据标注等应用领域的重要基础工具，在自然语言处理技术走向实用化的过程中占有重要地位。"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.NLP主要有哪几类任务 ？\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. 序列标注：分词/POS Tag(词性标注）/NER/语义标注\n",
    "2. 分类任务：文本分类/情感计算\n",
    "3. 句子关系判断：Entailment/QA/自然语言推理\n",
    "4. 生成式任务：机器翻译/文本摘要\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "from operator import itemgetter\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.word2vec import Word2Vec"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 构造text-rank抽取关键词"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 数据准备"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "han_filename = \"/Users/junjiexie/Documents/NLP学习/nlp文本摘要项目/sqlResult_1558435.csv\"\n",
    "data = pd.read_csv(han_filename,encoding=\"GB18030\")\n",
    "articles = data[\"content\"].tolist()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def get_stopwords():\n",
    "    stopwords = []\n",
    "    with open(r\"/Users/junjiexie/Documents/NLP学习/nlp第九课/停用词表.txt\" ,encoding=\"utf-8\") as f:\n",
    "        line_str = f.readline()\n",
    "        while line_str != \"\":\n",
    "            line_str = line_str.strip()\n",
    "            stopwords.append(line_str)\n",
    "            line_str = f.readline()\n",
    "    return set(stopwords)\n",
    "\n",
    "def token(string):return re.findall('\\w+', string)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "def sentences_deal(sentences):\n",
    "    output_list = []\n",
    "    input = \"\".join(token(sentences))\n",
    "    cut_list = \",\".join(jieba.cut(input)).split(\",\")\n",
    "    \n",
    "    stopwords = get_stopwords()\n",
    "    for str in cut_list:\n",
    "        if str in stopwords:\n",
    "            continue\n",
    "        else:\n",
    "            output_list.append(str)\n",
    "    \n",
    "    return output_list\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "# 定义无向有权图\n",
    "class UndirectWeightedGraph:\n",
    "    d = 0.85\n",
    " \n",
    "    def __init__(self):\n",
    "        self.graph = defaultdict(list)\n",
    "    \n",
    "    #有权无向图的数据结构\n",
    "    def addEdge(self, start, end, weight):\n",
    "        # use a tuple (start, end, weight) instead of a Edge object\n",
    "        self.graph[start].append((start, end, weight))\n",
    "        self.graph[end].append((end, start, weight))\n",
    " \n",
    "    def rank(self):\n",
    "        #记录结点权值\n",
    "        ws = defaultdict(float)\n",
    "        #记录结点出度和\n",
    "        outSum = defaultdict(float)\n",
    "        \n",
    "        # 初始化各个结点的权值\n",
    "        wsdef = 1.0 / (len(self.graph) or 1.0)\n",
    "        \n",
    "        # 统计各个结点的出度的次数之和\n",
    "        for n, out in self.graph.items():\n",
    "            ws[n] = wsdef\n",
    "            outSum[n] = sum((e[2] for e in out), 0.0)\n",
    " \n",
    "        # this line for build stable iteration\n",
    "        sorted_keys = sorted(self.graph.keys())\n",
    "        # 遍历若干次，保证权值收敛，这里写了100次\n",
    "        for x in range(100):  \n",
    "            for n in sorted_keys:\n",
    "                s = 0\n",
    "                # 将这些入度结点贡献后的权值相加\n",
    "                # 贡献率 = 入度结点与结点n的共现次数 / 入度结点的所有出度的次数\n",
    "                for e in self.graph[n]:\n",
    "                    s += e[2] / outSum[e[1]] * ws[e[1]]\n",
    "                # 更新结点n的权值\n",
    "                ws[n] = (1 - self.d) + self.d * s\n",
    " \n",
    "        (min_rank, max_rank) = (sys.float_info[0], sys.float_info[3])\n",
    " \n",
    "        for w in ws.values():\n",
    "            if w < min_rank:\n",
    "                min_rank = w\n",
    "            if w > max_rank:\n",
    "                max_rank = w\n",
    "        \n",
    "        #权值归一化，修正数值分布\n",
    "        for n, w in ws.items():\n",
    "            ws[n] = (w - min_rank / 10.0) / (max_rank - min_rank / 10.0)\n",
    " \n",
    "        return ws"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "def cosine_similarity(x, y, norm=False):\n",
    "    \"\"\" 计算两个向量x和y的余弦相似度 \"\"\"\n",
    "    assert len(x) == len(y), \"len(x) != len(y)\"\n",
    "    zero_list = [0] * len(x)\n",
    "    if list(x) == zero_list or list(y) == zero_list:\n",
    "        return float(1) if list(x) == list(y) else float(0)\n",
    "\n",
    "    # method 1\n",
    "    res = np.array([[x[i] * y[i], x[i] * x[i], y[i] * y[i]] for i in range(len(x))])\n",
    "    cos = sum(res[:, 0]) / (np.sqrt(sum(res[:, 1])) * np.sqrt(sum(res[:, 2])))\n",
    "\n",
    "    return 0.5 * cos + 0.5 if norm else cos  # 归一化到[0, 1]区间内\n",
    "\n",
    "def word_similarity(model, word1, word2):\n",
    "\n",
    "    #有可能有些词是不在word2vec里的，因此无法计算,给出一个接近零的相似度\n",
    "    try:\n",
    "        word1_vec = model[word1]\n",
    "    except KeyError:\n",
    "        return 0.001\n",
    "    try:  \n",
    "        word2_vec = model[word2]\n",
    "    except KeyError:\n",
    "        return 0.001\n",
    "    \n",
    "    return cosine_similarity(word1_vec, word2_vec)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "def textrank(sentences, topK=10, span_num=2):\n",
    "    # 导入word2vec\n",
    "    path = '/Users/junjiexie/OursRepository/text-abstract-extraction/Data/wiki_han_word2vec_300维度.model'\n",
    "    model = Word2Vec.load(path)\n",
    "    # 定义无向有权图\n",
    "    g = UndirectWeightedGraph()\n",
    "    # 定义权重词典\n",
    "    cm = defaultdict(int)\n",
    "    # 文本预处理\n",
    "    words = sentences_deal(sentences)\n",
    "    # 依次遍历每个词\n",
    "    for i, wp in enumerate(words):\n",
    "            # 依次遍历词i 之后窗口范围内的词\n",
    "        for j in range(i + 1, i + span_num):\n",
    "            # 词j 不能超出整个句子\n",
    "            if j >= len(words):\n",
    "                break\n",
    "            #判断这个词组是否已经出现过\n",
    "            if cm[(wp, words[j])] == 0:\n",
    "                cm[(wp, words[j])] = word_similarity(model=model, word1=wp, word2=words[j])\n",
    "            else:\n",
    "                continue\n",
    "    \n",
    "    # jieba中对权重的定义是两词共现次数，这里换成word2vec词向量相似度\n",
    "    for terms, w in cm.items():\n",
    "        g.addEdge(terms[0], terms[1], w)\n",
    "    \n",
    "    # 运行text-rank算法\n",
    "    nodes_rank = g.rank()\n",
    "    \n",
    "    # 根据指标值进行排序\n",
    "    tags = sorted(nodes_rank.items(), key=itemgetter(1), reverse=True)\n",
    " \n",
    "    # 输出topK个词作为关键词\n",
    "    if topK:\n",
    "        return tags[:topK]\n",
    "    else:\n",
    "        return tags\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "[('发布', 1.0),\n ('更新', 0.6395818438752795),\n ('月', 0.2810899844929091),\n ('本周', 0.18849534996885373),\n ('含', 0.15672863164471557),\n ('开发', 0.12142483512437663),\n ('更新换代', 0.11155062740108165),\n ('版', 0.09465604762329465),\n ('去年', 0.09103145940670385),\n ('外', 0.08444310231583688)]"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 52
    }
   ],
   "source": [
    "textrank(articles[0])\n",
    "\n",
    "#感觉用word2vec相似度作为权重有点怪怪的"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "[('进入', 1.0),\n ('澎湃', 0.5847296259939706),\n ('拿到', 0.0548973629047256),\n ('不会', 0.0546180254984679),\n ('强调', 0.05460621538170052),\n ('考虑', 0.05459062935145518),\n ('PCB', 0.0545797095417014),\n ('空间', 0.05457955167302171),\n ('30', 0.054579460817448286),\n ('按计划', 0.05457935287802686)]"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 53
    }
   ],
   "source": [
    "textrank(articles[1])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "[('手机', 1.0),\n ('缩水', 0.29252452749916613),\n ('大屏', 0.2602298390761203),\n ('拥有', 0.21514176678840113),\n ('旗舰', 0.20898436679888105),\n ('AMOLED', 0.20599958350122996),\n ('虎', 0.2046873581381779),\n ('可能', 0.20028676493912392),\n ('应该', 0.19264664010165564),\n ('掌握', 0.19159077476197756)]"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 54
    }
   ],
   "source": [
    "textrank(articles[2])\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "# 完成选做一\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-4acd738a",
   "language": "python",
   "display_name": "PyCharm (NLPHomework)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}