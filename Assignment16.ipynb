{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.复习上课内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.理论题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Why we need $\\gamma$ in reinforcement learning ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作用原理：\n",
    "折扣因子通常以符号γ表示，在强化学习中用来调节近远期影响，即agent做决策时考虑多长远，取值范围(0,1]。γ越大agent往前考虑的步数越多，但训练难度也越高；γ越小agent越注重眼前利益，训练难度也越小。我们都希望agent能“深谋远虑”，但过高的折扣因子容易导致算法收敛困难。还以小车导航为例，由于只有到达终点时才有奖励，相比而言惩罚项则多很多，在训练初始阶段负反馈远多于正反馈，一个很高的折扣因子（如0.999）容易使agent过分忌惮前方的“荆棘丛生”，而宁愿待在原地不动；相对而言，一个较低的折扣因子（如0.9）则使agent更加敢于探索环境从而获取抵达终点的成功经验；而一个过低的折扣因子（如0.4），使得稍远一点的反馈都被淹没了，除非离终点很近，agent在大多数情况下根本看不到“光明的未来”，更谈不上为了抵达终点而努力了。\n",
    "\n",
    "选取方法：\n",
    "总之，折扣因子的取值原则是，在算法能够收敛的前提下尽可能大。在实践中，有个经验公式1/(1-γ)，可以用来估计agent做决策时往前考虑的步数。根据对特定任务的分析，合理选择γ值，避免“近视”和“远视”。比如可以根据观察或统计agent到达终点所需的步数分布，选择合适的步数使得agent在该步数内的探索下有一定概率到达终点（正样本），注意这个概率越高训练难度就越小，然后利用经验公式把该步数换算成γ即可。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "2. Please breifly explain what is value function and what is Q function ?\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "状态价值函数：V(s) = E[Ut|St = s],意义为基于t时刻的状态s能获得的未来回报（return)的期望，加入动作选择策略后可以表示为Vπ(s) = Eπ[Ut|St = s](Ut = Rt+1 + γRt+2 + ... + γ^T-t-1 RT)\n",
    "\n",
    "动作价值函数：qπ = Eπ[Ut|St = s，At = a]，意义为基于t时刻的状态s，选择一个action后能够获得的未来回报的期望"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. How temperal difference related to dynamic programming and monte-carlo methods ?"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "一、\n",
    "\n",
    "动态规划是一个相对比较简单些的内容。因为在这部分会假设智能已经知道关于该环境的所有信息，即完全了解马尔科夫决策过程，而不需要和环境互动后才知道。所以智能体知道该环境是如何决定下一状态以及如何决定奖励的。动态规划所要解决的问题就是智能体知道了环境的所有信息后，如何利用这些信息找出最优策略。\n",
    "\n",
    "找到最优策略的方法大致可以表述为：\n",
    "\n",
    "先提出一个策略进行评估\n",
    "再根据评估值提出更好的或者一样好的策略。\n",
    "\n",
    "二、\n",
    "\n",
    "采用蒙特卡洛模拟方法的时候，智能体是不知道环境的动态信息的，需要和环境进行一系列的互动后才了解。\n",
    "\n",
    "预测：状态值&动作值\n",
    "智能体与环境进行一系列互动的过程中，会有一系列的状态，动作和奖励。此处重点探讨阶段性任务，即智能体在时间 T 遇到最终状态时，互动结束。在任何阶段，智能体的目标都是最大化期望积累奖励。\n",
    "\n",
    "一切的问题将从预测开始。在给定一个策略后，智能体如何估算该策略的状态值和动作值？有两种方式：\n",
    "\n",
    "离线策略方法(Off-Policy Method)：用一个策略进行评估，用另一个策略来与环境进行互动。\n",
    "异同策略方法(On-Policy Method)：智能体通过某个策略与环境进行互动，并计算该策略的值函数。\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Please briefly describe what are value iteration and policy iteration ?\n",
    "\n",
    "策略迭代：\n",
    "\n",
    "假设我们有一个3 x 3的棋盘：\n",
    "\n",
    "有一个单元格是超级玛丽,位置是(0, 0)，每回合可以往上、下、左、右四个方向移动\n",
    "有一个单元格是宝藏，超级玛丽找到宝藏则游戏结束，目标是让超级玛丽以最快的速度找到宝藏\n",
    "假设游戏开始时，宝藏的位置一定是(1, 2)\n",
    "\n",
    "\n",
    "初始化：无论超级玛丽在哪个位置，策略默认为向下走\n",
    "\n",
    "策略评估：计算V(s)\n",
    "如果宝藏恰好在正下方，则期望价值等于到达宝藏的距离(-2或者-1）\n",
    "如果宝藏不在正下方，则永远也不可能找到宝藏，期望价值为负无穷\n",
    "\n",
    "策略提升：根据V(s)找到更好的策略\n",
    "如果宝藏恰好在正下方，则策略已经最优，保持不变\n",
    "如果宝藏不在正下方，根据argmax a ∑s',r(r + γV(s')),可以得出最优策略为横向移动一步\n",
    "\n",
    "第一轮迭代：通过上一轮的策略提升，这一轮的策略变成了横向移动或者向下移动（如图所示)\n",
    "\n",
    "策略评估：计算V(s)\n",
    "如果宝藏恰好在正下方，则期望价值等于到达宝藏的距离(-2或者-1）\n",
    "如果宝藏不在正下方，当前策略会选择横向移动，期望价值为-3, -2, -1\n",
    "\n",
    "策略提升：根据V(s)找到更好的策略\n",
    "如果宝藏恰好在正下方，则策略已经最优，保持不变\n",
    "如果宝藏不在正下方，根据argmax a ∑s',r(r + γV(s'))可以得出当前策略已经最优，保持不变"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "价值迭代：\n",
    "\n",
    "还是上一个超级玛丽的例子:\n",
    "\n",
    "马尔科夫决策过程(MDP)设定：\n",
    "\n",
    "状态空间State：超级玛丽当前的坐标\n",
    "\n",
    "决策空间Action: 上、下、左、右四个动作\n",
    "Action对State的影响和回报 P(State', Reward | State, Action)：本文认为该关系是已知的\n",
    "\n",
    "超级玛丽每移动一步，reward = -1\n",
    "\n",
    "超级玛丽得到宝箱，reward = 0并且游戏结束\n",
    "\n",
    "初始化：所有state的价值V(s) = 0\n",
    "\n",
    "第一轮迭代：对于每个state，逐一尝试上、下、左、右四个Action\n",
    "记录Action带来的Reward、以及新状态 V(s')\n",
    "选择最优的Action，更新V(s) = Reward + V(s') = -1 + 0\n",
    "第一轮结束后，所有状态都有V(s) = -1，即从当前位置出发走一步获得Reward=-1\n",
    "\n",
    "第二轮迭代：对于每个state，逐一尝试上、下、左、右四个Action\n",
    "记录Action带来的Reward、以及新状态 V(s')\n",
    "选择最优的Action，更新V(s) = Reward + V(s')\n",
    "对于宝箱周围的State，最优的Action是一步到达宝箱，V(s) = Reward + V(s') = -1 + 0\n",
    "对于其他State，所有的Action都是一样的，V(s) = Reward + V(s') = -1 + -1\n",
    "第二轮结束后，宝箱周围的State的价值保持不变 V(s) = -1，其他State的价值 V(s) = -2\n",
    "\n",
    "第三轮迭代：对于每个state，逐一尝试上、下、左、右四个Action\n",
    "记录Action带来的Reward、以及新状态 V(s')\n",
    "选择最优的Action，更新V(s) = Reward + V(s')\n",
    "对于宝箱周围的State，最优的Action是一步到达宝箱，V(s) = Reward + V(s') = -1 + 0\n",
    "对于宝箱两步距离的State，最优的Action是先一步到达宝箱周边的State，V(s) = Reward + V(s') = -1 + -1\n",
    "对于宝箱三步距离的State，所有Action都是一样的，V(s) = Reward + V(s') = -1 + -2\n",
    "\n",
    "第四轮迭代：对于每个state，逐一尝试上、下、左、右四个Action\n",
    "记录Action带来的Reward、以及新状态 V(s')\n",
    "选择最优的Action，更新V(s) = Reward + V(s')\n",
    "对于宝箱周围的State，最优的Action是一步到达宝箱，V(s) = Reward + V(s') = -1 + 0\n",
    "对于宝箱两步距离的State，最优的Action是先一步到达宝箱周边的State，V(s) = Reward + V(s') = -1 + -1\n",
    "对于宝箱三步距离的State，最优的Action是所有Action都是一样的，V(s) = Reward + V(s') = -1 + -2\n",
    "\n",
    "在第四轮迭代中，所有V(s)更新前后都没有任何变化，价值迭代已经找到了最优策略\n",
    "\n",
    "上面的迭代过程实际上运用了贝尔曼方程 (Bellman Equation)，对每个位置的价值进行更新\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. How can we use deep lerning in reinforcement learning ?\n",
    "\n",
    "Deep Q Network\n",
    "\n",
    "基本思路是，用神经网络建模 Q function，基本公式如下：（ φ 是 state s，θ 代表网络参数）。Loss 为 网络输出值（ Q(φj,aj;θ) ）和目标值（ yj ）之间的平方误差。\n",
    "\n",
    "yj = rj + γMAX a' Q(Φj+1,a',θ）\n",
    "\n",
    "同时，因为训练样本并不满足独立同分布，DQN 引入 Experience Replay 机制从 replay 中随机采样数据以尽量减少样本间的相关性，使得网络更容易训练。另外，DQN 的 target network 和 estimate network 结构一致，经过 C 轮迭代之后更新 target network = estimate network，从而使训练更稳定。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 选做题 （实践）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.table import Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def draw_image(image):\n",
    "    fig,ax = plt.subplots()\n",
    "    ax.set_axis_off()\n",
    "    tb = Table(ax,bbox=[0,0,1,1])\n",
    "\n",
    "    nrows,ncols = image.shape\n",
    "    width,height = 1.0/ncols,1.0/nrows\n",
    "\n",
    "    for (i,j), val in np.ndenumerate(image):\n",
    "        if (i,j) == (0,1):\n",
    "            val = \"A\"\n",
    "        elif (i,j) == (0,3):\n",
    "            val = \"B\"\n",
    "        elif (i,j) == (4,1):\n",
    "            val = \"A'\"\n",
    "        elif (i,j) == (2,3):\n",
    "            val = \"B'\"\n",
    "        tb.add_cell(i,j,width,height,text=val,\n",
    "                    loc='center',facecolor='white')\n",
    "\n",
    "    for i in range(len(image)):\n",
    "        tb.add_cell(i,-1,width,height,text=i+1,loc='right',\n",
    "                    edgecolor='none',facecolor='none')\n",
    "        tb.add_cell(-1,i,width,height/2,text=i+1,loc='center',\n",
    "                    edgecolor='none',facecolor='none')\n",
    "    ax.add_table(tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "WORLD_SIZE=5\n",
    "draw_image(np.zeros((WORLD_SIZE,WORLD_SIZE)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure above shows a rectangular gridworld. The cell of the grid correspond to the state of the environment. At each cell, four actions with equal probability are possible: north, south, east and west, which deterministically cause the agent to move one cell in the respective direction on the grid. Actions that would take the agent off the grid leave its unchanged, but also result in a reward -1. Other actions result in a reward of 0, expect those taht move the agent out of the special states A and B. From state A, all four actions yield a reward of +10 and take the agent to A'. From state B, all actions yield a reward of +5 and take the agent to B'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try to compute the value of each state ? You can choose any algorithms we leanred in the class.\n",
    "Good luck and happy new year. !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}