{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "from operator import itemgetter\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 构造text-rank抽取关键词"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# \"/Users/junjiexie/Documents/NLP学习/nlp文本摘要项目/sqlResult_1558435.csv\"\n",
    "han_filename = r\"C:\\NLP学习备用\\sqlResult_1558435.csv\"\n",
    "data = pd.read_csv(han_filename,encoding=\"GB18030\")\n",
    "articles = data[\"content\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_stopwords():\n",
    "    stopwords = []\n",
    "    with open(r\"/Users/junjiexie/Documents/NLP学习/nlp第九课/停用词表.txt\" ,encoding=\"utf-8\") as f:\n",
    "        line_str = f.readline()\n",
    "        while line_str != \"\":\n",
    "            line_str = line_str.strip()\n",
    "            stopwords.append(line_str)\n",
    "            line_str = f.readline()\n",
    "    return set(stopwords)\n",
    "\n",
    "def token(string):return re.findall('\\w+', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sentences_deal(sentences):\n",
    "    output_list = []\n",
    "    input = \"\".join(token(sentences))\n",
    "    cut_list = \",\".join(jieba.cut(input)).split(\",\")\n",
    "    \n",
    "    stopwords = get_stopwords()\n",
    "    for str in cut_list:\n",
    "        if str in stopwords:\n",
    "            continue\n",
    "        else:\n",
    "            output_list.append(str)\n",
    "    \n",
    "    return output_list\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 定义无向有权图\n",
    "class UndirectWeightedGraph:\n",
    "    d = 0.85\n",
    " \n",
    "    def __init__(self):\n",
    "        self.graph = defaultdict(list)\n",
    "    \n",
    "    #有权无向图的数据结构\n",
    "    def addEdge(self, start, end, weight):\n",
    "        # use a tuple (start, end, weight) instead of a Edge object\n",
    "        self.graph[start].append((start, end, weight))\n",
    "        self.graph[end].append((end, start, weight))\n",
    " \n",
    "    def rank(self):\n",
    "        #记录结点权值\n",
    "        ws = defaultdict(float)\n",
    "        #记录结点出度和\n",
    "        outSum = defaultdict(float)\n",
    "        \n",
    "        # 初始化各个结点的权值\n",
    "        wsdef = 1.0 / (len(self.graph) or 1.0)\n",
    "        \n",
    "        # 统计各个结点的出度的次数之和\n",
    "        for n, out in self.graph.items():\n",
    "            ws[n] = wsdef\n",
    "            outSum[n] = sum((e[2] for e in out), 0.0)\n",
    " \n",
    "        # this line for build stable iteration\n",
    "        sorted_keys = sorted(self.graph.keys())\n",
    "        # 遍历若干次，保证权值收敛，这里写了100次\n",
    "        for x in range(100):  \n",
    "            for n in sorted_keys:\n",
    "                s = 0\n",
    "                # 将这些入度结点贡献后的权值相加\n",
    "                # 贡献率 = 入度结点与结点n的共现次数 / 入度结点的所有出度的次数\n",
    "                for e in self.graph[n]:\n",
    "                    s += e[2] / outSum[e[1]] * ws[e[1]]\n",
    "                # 更新结点n的权值\n",
    "                ws[n] = (1 - self.d) + self.d * s\n",
    " \n",
    "        (min_rank, max_rank) = (sys.float_info[0], sys.float_info[3])\n",
    " \n",
    "        for w in ws.values():\n",
    "            if w < min_rank:\n",
    "                min_rank = w\n",
    "            if w > max_rank:\n",
    "                max_rank = w\n",
    "        \n",
    "        #权值归一化，修正数值分布\n",
    "        for n, w in ws.items():\n",
    "            ws[n] = (w - min_rank / 10.0) / (max_rank - min_rank / 10.0)\n",
    " \n",
    "        return ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(x, y, norm=False):\n",
    "    \"\"\" 计算两个向量x和y的余弦相似度 \"\"\"\n",
    "    assert len(x) == len(y), \"len(x) != len(y)\"\n",
    "    zero_list = [0] * len(x)\n",
    "    if list(x) == zero_list or list(y) == zero_list:\n",
    "        return float(1) if list(x) == list(y) else float(0)\n",
    "\n",
    "    # method 1\n",
    "    res = np.array([[x[i] * y[i], x[i] * x[i], y[i] * y[i]] for i in range(len(x))])\n",
    "    cos = sum(res[:, 0]) / (np.sqrt(sum(res[:, 1])) * np.sqrt(sum(res[:, 2])))\n",
    "\n",
    "    return 0.5 * cos + 0.5 if norm else cos  # 归一化到[0, 1]区间内\n",
    "\n",
    "def word_similarity(model, word1, word2):\n",
    "\n",
    "    #有可能有些词是不在word2vec里的，因此无法计算,给出一个接近零的相似度\n",
    "    try:\n",
    "        word1_vec = model[word1]\n",
    "    except KeyError:\n",
    "        return 0.001\n",
    "    try:  \n",
    "        word2_vec = model[word2]\n",
    "    except KeyError:\n",
    "        return 0.001\n",
    "    \n",
    "    return cosine_similarity(word1_vec, word2_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def textrank(sentences, topK=10, span_num=2):\n",
    "    # 导入word2vec\n",
    "    path = '/Users/junjiexie/OursRepository/text-abstract-extraction/Data/wiki_han_word2vec_300维度.model'\n",
    "    model = Word2Vec.load(path)\n",
    "    # 定义无向有权图\n",
    "    g = UndirectWeightedGraph()\n",
    "    # 定义权重词典\n",
    "    cm = defaultdict(int)\n",
    "    # 文本预处理\n",
    "    words = sentences_deal(sentences)\n",
    "    # 依次遍历每个词\n",
    "    for i, wp in enumerate(words):\n",
    "            # 依次遍历词i 之后窗口范围内的词\n",
    "        for j in range(i + 1, i + span_num):\n",
    "            # 词j 不能超出整个句子\n",
    "            if j >= len(words):\n",
    "                break\n",
    "            #判断这个词组是否已经出现过\n",
    "            if cm[(wp, words[j])] == 0:\n",
    "                cm[(wp, words[j])] = word_similarity(model=model, word1=wp, word2=words[j])\n",
    "            else:\n",
    "                continue\n",
    "    \n",
    "    # jieba中对权重的定义是两词共现次数，这里换成word2vec词向量相似度\n",
    "    for terms, w in cm.items():\n",
    "        g.addEdge(terms[0], terms[1], w)\n",
    "    \n",
    "    # 运行text-rank算法\n",
    "    nodes_rank = g.rank()\n",
    "    \n",
    "    # 根据指标值进行排序\n",
    "    tags = sorted(nodes_rank.items(), key=itemgetter(1), reverse=True)\n",
    " \n",
    "    # 输出topK个词作为关键词\n",
    "    if topK:\n",
    "        return tags[:topK]\n",
    "    else:\n",
    "        return tags\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('发布', 1.0),\n",
       " ('更新', 0.6395818438752795),\n",
       " ('月', 0.2810899844929091),\n",
       " ('本周', 0.18849534996885373),\n",
       " ('含', 0.15672863164471557),\n",
       " ('开发', 0.12142483512437663),\n",
       " ('更新换代', 0.11155062740108165),\n",
       " ('版', 0.09465604762329465),\n",
       " ('去年', 0.09103145940670385),\n",
       " ('外', 0.08444310231583688)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textrank(articles[0])\n",
    "\n",
    "#感觉用word2vec相似度作为权重有点怪怪的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('进入', 1.0),\n",
       " ('澎湃', 0.5847296259939706),\n",
       " ('拿到', 0.0548973629047256),\n",
       " ('不会', 0.0546180254984679),\n",
       " ('强调', 0.05460621538170052),\n",
       " ('考虑', 0.05459062935145518),\n",
       " ('PCB', 0.0545797095417014),\n",
       " ('空间', 0.05457955167302171),\n",
       " ('30', 0.054579460817448286),\n",
       " ('按计划', 0.05457935287802686)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textrank(articles[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/Users/junjiexie/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('手机', 1.0),\n",
       " ('缩水', 0.29252452749916613),\n",
       " ('大屏', 0.2602298390761203),\n",
       " ('拥有', 0.21514176678840113),\n",
       " ('旗舰', 0.20898436679888105),\n",
       " ('AMOLED', 0.20599958350122996),\n",
       " ('虎', 0.2046873581381779),\n",
       " ('可能', 0.20028676493912392),\n",
       " ('应该', 0.19264664010165564),\n",
       " ('掌握', 0.19159077476197756)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textrank(articles[2])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "source": [
    "# 完成选做一,要使用到pyltp，转到Windows平台"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 句子主要成分提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pyltp import Segmentor, Postagger, Parser, NamedEntityRecognizer\n",
    "from gensim.models import word2vec\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pyltp import Segmentor, Postagger, Parser, NamedEntityRecognizer,SentenceSplitter\n",
    "MODELDIR='D:/LTP/ltp_data'\n",
    "# print (\"正在加载LTP模型... ...\")\n",
    "stopwords = [line.strip() for line in open(r\"C:\\NLP学习备用\\停用词表.txt\", 'r',encoding='utf-8').readlines()]\n",
    "segmentor = Segmentor()\n",
    "segmentor.load(os.path.join(MODELDIR, r\"C:\\ltp_data_v3.4.0\\cws.model\"))\n",
    "\n",
    "postagger = Postagger()\n",
    "postagger.load(os.path.join(MODELDIR, r\"C:\\ltp_data_v3.4.0\\pos.model\"))\n",
    "\n",
    "parser = Parser()\n",
    "parser.load(os.path.join(MODELDIR, r\"C:\\ltp_data_v3.4.0\\parser.model\"))\n",
    "\n",
    "recognizer = NamedEntityRecognizer()\n",
    "recognizer.load(os.path.join(MODELDIR, r\"C:\\ltp_data_v3.4.0\\ner.model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#借鉴了github某位大神的code，剥离核心部分\n",
    "def build_parse_child_dict(words, postags, arcs):\n",
    "    \"\"\"\n",
    "    为句子中的每个词语维护一个保存句法依存儿子节点的字典\n",
    "    Args:\n",
    "        words: 分词列表\n",
    "        postags: 词性列表\n",
    "        arcs: 句法依存列表\n",
    "    \"\"\"\n",
    "    child_dict_list = []\n",
    "    for index in range(len(words)):\n",
    "        child_dict = dict()\n",
    "        for arc_index in range(len(arcs)):\n",
    "            if arcs[arc_index].head == index + 1:\n",
    "                if arcs[arc_index].relation in child_dict: #python3删除了has_key方法\n",
    "                    child_dict[arcs[arc_index].relation].append(arc_index)\n",
    "                else:\n",
    "                    child_dict[arcs[arc_index].relation] = []\n",
    "                    child_dict[arcs[arc_index].relation].append(arc_index)\n",
    "        #if child_dict.has_key('SBV'):\n",
    "        #    print words[index],child_dict['SBV']\n",
    "        child_dict_list.append(child_dict)\n",
    "    return child_dict_list\n",
    "\n",
    "def complete_e(words, postags, child_dict_list, word_index):\n",
    "    \"\"\"\n",
    "    完善识别的部分实体\n",
    "    \"\"\"\n",
    "    child_dict = child_dict_list[word_index]\n",
    "    prefix = ''\n",
    "    if 'ATT' in child_dict:\n",
    "        for i in range(len(child_dict['ATT'])):\n",
    "            prefix += complete_e(words, postags, child_dict_list, child_dict['ATT'][i])\n",
    "\n",
    "    postfix = ''\n",
    "    if postags[word_index] == 'v':\n",
    "        if 'VOB' in child_dict:\n",
    "            postfix += complete_e(words, postags, child_dict_list, child_dict['VOB'][0])\n",
    "        if 'SBV' in child_dict:\n",
    "            prefix = complete_e(words, postags, child_dict_list, child_dict['SBV'][0]) + prefix\n",
    "\n",
    "    return prefix + words[word_index] + postfix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fact_triple_extract(sentence):\n",
    "    \"\"\"\n",
    "    对于给定的句子进行事实三元组抽取\n",
    "    Args:\n",
    "        sentence: 要处理的语句\n",
    "    \"\"\"\n",
    "    #print (sentence)\n",
    "    words=[]\n",
    "    cuts = segmentor.segment(sentence)\n",
    "    #print (\"\\t\".join(words))\n",
    "    for word in cuts:\n",
    "        if word not in stopwords:\n",
    "            words.append(word)\n",
    "    postags = postagger.postag(words)\n",
    "    netags = recognizer.recognize(words, postags)\n",
    "    arcs = parser.parse(words, postags)\n",
    "    #print (\"\\t\".join(\"%d:%s\" % (arc.head, arc.relation) for arc in arcs))\n",
    "    \n",
    "    child_dict_list = build_parse_child_dict(words, postags, arcs)\n",
    "    for index in range(len(postags)):\n",
    "        # 抽取以谓词为中心的事实三元组\n",
    "        if postags[index] == 'v':\n",
    "            child_dict = child_dict_list[index]\n",
    "            # 主谓宾\n",
    "            if 'SBV' in child_dict and 'VOB' in child_dict:\n",
    "                e1 = complete_e(words, postags, child_dict_list, child_dict['SBV'][0])\n",
    "                r = words[index]\n",
    "                e2 = complete_e(words, postags, child_dict_list, child_dict['VOB'][0])\n",
    "                return [e1, r, e2]\n",
    "\n",
    "            # 定语后置，动宾关系\n",
    "            if arcs[index].relation == 'ATT':\n",
    "                if 'VOB' in child_dict:\n",
    "                    e1 = complete_e(words, postags, child_dict_list, arcs[index].head - 1)\n",
    "                    r = words[index]\n",
    "                    e2 = complete_e(words, postags, child_dict_list, child_dict['VOB'][0])\n",
    "                    temp_string = r+e2\n",
    "                    if temp_string == e1[:len(temp_string)]:\n",
    "                        e1 = e1[len(temp_string):]\n",
    "                    if temp_string not in e1:\n",
    "                        return print(\"定语后置动宾关系\\t(%s, %s, %s)\\n\" % (e1, r, e2))\n",
    "\n",
    "            # 含有介宾关系的主谓动补关系\n",
    "            if 'SBV' in child_dict and 'CMP' in child_dict:\n",
    "                #e1 = words[child_dict['SBV'][0]]\n",
    "                e1 = complete_e(words, postags, child_dict_list, child_dict['SBV'][0])\n",
    "                cmp_index = child_dict['CMP'][0]\n",
    "                r = words[index] + words[cmp_index]\n",
    "                if 'POB' in child_dict_list[cmp_index]:\n",
    "                    e2 = complete_e(words, postags, child_dict_list, child_dict_list[cmp_index]['POB'][0])\n",
    "                    return [e1, r, e2]\n",
    "\n",
    "\n",
    "        # 尝试抽取命名实体有关的三元组\n",
    "        if netags[index][0] == 'S' or netags[index][0] == 'B':\n",
    "            ni = index\n",
    "            if netags[ni][0] == 'B':\n",
    "                while netags[ni][0] != 'E':\n",
    "                    ni += 1\n",
    "                e1 = ''.join(words[index:ni+1])\n",
    "            else:\n",
    "                e1 = words[ni]\n",
    "            if arcs[ni].relation == 'ATT' and postags[arcs[ni].head-1] == 'n' and netags[arcs[ni].head-1] == 'O':\n",
    "                r = complete_e(words, postags, child_dict_list, arcs[ni].head-1)\n",
    "                if e1 in r:\n",
    "                    r = r[(r.index(e1)+len(e1)):]\n",
    "                if arcs[arcs[ni].head-1].relation == 'ATT' and netags[arcs[arcs[ni].head-1].head-1] != 'O':\n",
    "                    e2 = complete_e(words, postags, child_dict_list, arcs[arcs[ni].head-1].head-1)\n",
    "                    mi = arcs[arcs[ni].head-1].head-1\n",
    "                    li = mi\n",
    "                    if netags[mi][0] == 'B':\n",
    "                        while netags[mi][0] != 'E':\n",
    "                            mi += 1\n",
    "                        e = ''.join(words[li+1:mi+1])\n",
    "                        e2 += e\n",
    "                    if r in e2:\n",
    "                        e2 = e2[(e2.index(r)+len(r)):]\n",
    "                    if r+e2 in sentence:\n",
    "                        return [e1, r, e2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['美国', '遭受', '严重海啸袭击']"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fact_triple_extract(\"2020年美国遭受了严重的海啸袭击\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['小明', '见', '大海']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fact_triple_extract(\"小明有生之年还没见过大海\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 找出是否含有说的意思的句子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 最简单粗暴的方法是，把与说最相近的词都出来，正则匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\NLP学习备用\\wiki_han_word2vec_300维度.model'\n",
    "model = Word2Vec.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_word(topn=100):\n",
    "    similar_by_word = []\n",
    "    for i in model.wv.similar_by_word(\"说\",topn = topn):\n",
    "        similar_by_word.append(i[0])\n",
    "    similar_by_word.append(\"说\")\n",
    "    return set(similar_by_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#多少还是会取决于pyltp的性能\n",
    "def find_contain_say_sentence(sentences):\n",
    "    segmentor = Segmentor()  # 初始化实例\n",
    "    segmentor.load(r\"D:\\MyNLP\\ltp_data_v3.4.0\\cws.model\")  # 加载模型\n",
    "    cut_sentence = \"cut\".join(SentenceSplitter.split(sentences)).split(\"cut\")\n",
    "    similar_word = get_similar_word()\n",
    "    stay_count = []\n",
    "    output_sentence = []\n",
    "    for count,sentence in enumerate(cut_sentence):\n",
    "        sentence = str(sentence)\n",
    "        if len(sentences) == 0:\n",
    "            continue\n",
    "        for word in similar_word:\n",
    "            if sentence.find(word) != -1:\n",
    "                stay_count.append(count)\n",
    "                break;\n",
    "    for i in stay_count:\n",
    "        output_sentence.append(cut_sentence[i])\n",
    "    \n",
    "    return output_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['有人猜测这也是将精力主要用到MIUI 9的研发之中。', '当然，关于MIUI 9的确切信息，我们还是等待官方消息。']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_contain_say_sentence(articles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['至于电池缩水，可能与刘作虎所说，一加手机5要做市面最轻薄大屏旗舰的设定有关。']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_contain_say_sentence(articles[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@深圳交警微博称：昨日清晨交警发现有一女子赤裸上身，行走在南坪快速上，期间还起了轻生年头，一辅警发现后赶紧为其披上黄衣，并一路劝说她。',\n",
       " '南都记者在龙岗大队坂田中队见到了辅警刘青（发现女生的辅警），一位外表高大帅气，说话略带些腼腆的90后青年。',\n",
       " '刘青介绍，6月16日早上7时36分，他正在环城南路附近值勤，接到中队关于一位女子裸身进入机动车可能有危险的警情，随后骑着小铁骑开始沿路寻找，大概花了十多分钟在南坪大道坂田出口往龙岗方向的逆行辅道上发现该女子。',\n",
       " '刘青停好小铁骑，和另外一名巡防员追了上去，发现女子的情绪很低落，话不多，刘青尝试和女子交流，劝说女子离开，可女子并不愿意接受，继续缓慢地往南坪快速路的主干道上走去。',\n",
       " '当女子行进到十字路口中间时，一辆大货车挡住了镜头，但是当女子再次出现镜头时，可以发现女子已经没穿内裤了，全身裸露继续朝着南坪快速方向走去。',\n",
       " '刘青表示，“一开始根本不敢看她，心里挺别扭，感觉很尴尬”，但当刘青跟随女子上了南坪快速路主干道时，女子作出了让人意想不到的举动，她突然靠近护栏要从上面跳下去，刘青赶忙冲上去拉住了女子的手，将其控制住并远离护栏。',\n",
       " '就这样，我被牵着走了大概十多分钟，天突然下起了大暴雨，雨大的连眼睛都睁不开”刘青继续说着，瞬间他们就被雨透了，但女子依然不愿意接受刘青的帮助，就继续冒着大雨往前走。',\n",
       " '大概走了有四十分钟吧，女子突然停下来说“我想回家了”，然后女子也接受了刘青递过来的小黄衣，就出现了深圳微博上的照片，女子披着小黄衣，刘青小心翼翼地在旁边走着的场景。',\n",
       " '才会说',\n",
       " '据警方透露，该女子姓陈，系湖北人，今年44岁，据家属反映其有精神病史。',\n",
       " '其实真爱的到来并不存在年龄的限制',\n",
       " '你们说呢？',\n",
       " '@弓常yan桦：就想问这个小哥哥有女票吗',\n",
       " '去年6月7号上午，淮安市涟水县公安局刑警大队突然接到了一个奇怪的报警电话，一名女子言语不清，声称自己遭到了侵害。']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_contain_say_sentence(articles[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 检测谓语中是否有说的意思"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_splitter(sentence, segmentor=segmentor):\n",
    "    words = segmentor.segment(sentence)  # 分词\n",
    "    words_list = list(words)\n",
    "    return words_list\n",
    "\n",
    "def word_tag(words, postagger=postagger):\n",
    "    postags = postagger.postag(words)  # 词性标注\n",
    "    return postags\n",
    "\n",
    "def word_parse(words, postags, parser=parser):\n",
    "    output = []\n",
    "    arcs = parser.parse(words, postags)  # 句法分析\n",
    "#     print(\"句法分析结果：\")\n",
    "#     print(\"\\t\".join(\"%d:%s\" % (arc.head, arc.relation) for arc in arcs))\n",
    "    return [(arc.head, arc.relation) for arc in arcs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#我的理解是依存句法分析的root节点多数是谓语\n",
    "def find_predicate_contain_say_sentence(sentences):\n",
    "    similar_word = get_similar_word()\n",
    "    stay_count = []\n",
    "    output_sentence = []\n",
    "    segmentor = Segmentor()  # 初始化实例\n",
    "    segmentor.load(r\"D:\\MyNLP\\ltp_data_v3.4.0\\cws.model\")  # 加载模型\n",
    "    cut_sentence = \"cut\".join(SentenceSplitter.split(sentences)).split(\"cut\")\n",
    "    for count,sentence in enumerate(cut_sentence):\n",
    "        if len(sentences) == 0:\n",
    "            continue\n",
    "            \n",
    "        #句法分析\n",
    "        words = word_splitter(sentence)\n",
    "        tag = word_tag(words)\n",
    "        parse = word_parse(words, tag)\n",
    "        for count_parse,element in enumerate(parse):\n",
    "            #检查谓语\n",
    "            if element[1] == \"HED\":\n",
    "                predicate = words[count_parse]\n",
    "                print(predicate)\n",
    "                for word in similar_word:\n",
    "                    if predicate.find(word) != -1:\n",
    "                        stay_count.append(count)\n",
    "                        break;\n",
    "                break;\n",
    "        \n",
    "    for i in stay_count:\n",
    "        output_sentence.append(cut_sentence[i])\n",
    "        \n",
    "    return output_sentence\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "暂停\n",
      "是\n",
      "发布\n",
      "等待\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_predicate_contain_say_sentence(articles[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "强调\n",
      "联手\n",
      "称\n",
      "显示\n",
      "首发\n",
      "是\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['骁龙835作为唯一通过Windows 10桌面平台认证的ARM处理器，高通强调，不会因为只考虑性能而去屏蔽掉小核心。',\n",
       " '报道称，微软已经拿到了一些新的源码，以便Windows 10更好地理解big.little架构。']"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_predicate_contain_say_sentence(articles[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "是\n",
      "说\n",
      "拥有\n",
      "是\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['至于电池缩水，可能与刘作虎所说，一加手机5要做市面最轻薄大屏旗舰的设定有关。']"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_predicate_contain_say_sentence(articles[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
